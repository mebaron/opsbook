{"./":{"url":"./","title":"简介","keywords":"","body":"简介 本书主要介绍了运维相关的知识，包括了云计算、容器、linux知识的学习记录，已经在使用的过程中遇到的一些问题及解决方案，后续会逐步完善文档笔记，希望这些笔记在大家学习的过程能帮到大家。 © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"linux/shell-script-template.html":{"url":"linux/shell-script-template.html","title":"【linux】shell脚本模板","keywords":"","body":"作为一名运维，我们经常会需要编写脚本来完成一些自动化工作，这里提供一个shell脚本的模板 #!/bin/sh ################ Version Info ################## # Create Date: 2021-05-26 # Author: vishon # Mail: nwx_qdlg@163.com # Version: 1.0 # Attention: shell脚本模板 ################################################ # 加载环境变量 # 如果脚本放到crontab中执行，会缺少环境变量，所以需要添加以下3行 . /etc/profile . ~/.bash_profile . /etc/bashrc # 脚本所在目录即脚本名称 script_dir=$( cd \"$( dirname \"$0\" )\" && pwd ) script_name=$(basename ${0}) # 日志目录 log_dir=\"${script_dir}/log\" [ ! -d ${log_dir} ] && { mkdir -p ${log_dir} } errorMsg(){ echo \"USAGE:$0 arg1 arg2 arg3\" exit 2 } doCode() { echo $1 echo $2 echo $3 } main() { if [ $# -ne 3 ];then errorMsg fi doCode \"$1\" \"$2\" \"$3\" } # 需要把隐号加上，不然传入的参数就不能有空格 main \"$@\" © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"linux/cvm-build-openvpn.html":{"url":"linux/cvm-build-openvpn.html","title":"【linux】腾讯云cvm上搭建openvpn","keywords":"","body":"我们在使用共有云的时候，有时候会需要本地电脑访问到云上的vpc机器，但是云上vpc是网络隔离的，如果不加公网ip是无法直接本地访问vpc的，其实这里我们只需要在vpc内有一台机器可以访问公网，然后再这台集群上搭建openvpn，这样本地就可以通过openvpn去直接连接vpc内其他内网机器，不用每台机器都配置公网ip了，下面我们来说下如何在腾讯云的cvm上搭建openvpn。 网络规划 vpc网段：10.0.0.0/16 openvpn分配给客户端的网段：192.168.1.0/24 openvpn服务端ip：10.0.0.13(内网)，106.53.146.250(公网) 安装openvpn # yum install openvpn # wget https://github.com/OpenVPN/easy-rsa/archive/master.zip # unzip master.zip # mv easy-rsa-master/ easy-rsa # mkdir -p /etc/openvpn/ # cp -R easy-rsa/ /etc/openvpn/ # cd /etc/openvpn/ # mkdir client server # ls client easy-rsa server 配置vars文件 # cd /etc/openvpn/easy-rsa/easyrsa3 # cp vars.example vars # vim vars 根据实际修改对应的配置 ....... set_var EASYRSA_REQ_COUNTRY \"CN\" set_var EASYRSA_REQ_PROVINCE \"SZ\" set_var EASYRSA_REQ_CITY \"GD\" set_var EASYRSA_REQ_ORG \"test\" set_var EASYRSA_REQ_EMAIL \"nwx_qdlg@163.com\" set_var EASYRSA_REQ_OU \"test\" ....... 创建server端证书 初始化目录 [root@VM-0-13-centos easyrsa3]# ls easyrsa openssl-easyrsa.cnf vars vars.example x509-types [root@VM-0-13-centos easyrsa3]# ./easyrsa init-pki Note: using Easy-RSA configuration from: /etc/openvpn/easy-rsa/easyrsa3/vars init-pki complete; you may now create a CA or requests. Your newly created PKI dir is: /etc/openvpn/easy-rsa/easyrsa3/pki [root@VM-0-13-centos easyrsa3]# ls easyrsa openssl-easyrsa.cnf pki vars vars.example x509-types 创建CA证书 [root@VM-0-13-centos easyrsa3]# ./easyrsa build-ca Note: using Easy-RSA configuration from: /etc/openvpn/easy-rsa/easyrsa3/vars Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017 Enter New CA Key Passphrase: #输入CA密码，记录下 Re-Enter New CA Key Passphrase: #确认密码 Generating RSA private key, 2048 bit long modulus ..................+++ ............................................+++ e is 65537 (0x10001) You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Common Name (eg: your user, host, or server name) [Easy-RSA CA]:server # ca证书名称 CA creation complete and you may now import and sign cert requests. Your new CA certificate file for publishing is at: /etc/openvpn/easy-rsa/easyrsa3/pki/ca.crt 创建服务端证书 [root@VM-0-13-centos easyrsa3]# ./easyrsa gen-req server nopass Note: using Easy-RSA configuration from: /etc/openvpn/easy-rsa/easyrsa3/vars Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017 Generating a 2048 bit RSA private key ....................+++ ........................+++ writing new private key to '/etc/openvpn/easy-rsa/easyrsa3/pki/easy-rsa-32328.KOVmFR/tmp.kdL0Yx' ----- You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Common Name (eg: your user, host, or server name) [server]:vpc-server #输入服务端名称 Keypair and certificate request completed. Your files are: req: /etc/openvpn/easy-rsa/easyrsa3/pki/reqs/server.req key: /etc/openvpn/easy-rsa/easyrsa3/pki/private/server.key 签约服务端证书 [root@VM-0-13-centos easyrsa3]# ./easyrsa sign server server Note: using Easy-RSA configuration from: /etc/openvpn/easy-rsa/easyrsa3/vars Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017 You are about to sign the following certificate. Please check over the details shown below for accuracy. Note that this request has not been cryptographically verified. Please be sure it came from a trusted source or that you have verified the request checksum with the sender. Request subject, to be signed as a server certificate for 825 days: subject= commonName = vpc-server Type the word 'yes' to continue, or any other input to abort. Confirm request details: yes #输入yes Using configuration from /etc/openvpn/easy-rsa/easyrsa3/pki/easy-rsa-345.HZwt53/tmp.7IIgHU Enter pass phrase for /etc/openvpn/easy-rsa/easyrsa3/pki/private/ca.key: #输入之前配置的CA密码 Check that the request matches the signature Signature ok The Subject's Distinguished Name is as follows commonName :ASN.1 12:'vpc-server' Certificate is to be certified until Jun 29 09:02:24 2023 GMT (825 days) Write out database with 1 new entries Data Base Updated Certificate created at: /etc/openvpn/easy-rsa/easyrsa3/pki/issued/server.crt 创建数据穿越密钥 [root@VM-0-13-centos easyrsa3]# ./easyrsa gen-dh Note: using Easy-RSA configuration from: /etc/openvpn/easy-rsa/easyrsa3/vars Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017 Generating DH parameters, 2048 bit long safe prime, generator 2 This is going to take a long time ....................+..............................................................................................................................................................................................+..........................................................................................................................+..........................................+...................+............................... DH parameters of size 2048 created at /etc/openvpn/easy-rsa/easyrsa3/pki/dh.pem 创建client证书 初始化目录 [root@VM-0-13-centos easyrsa3]# cd /etc/openvpn/client/ [root@VM-0-13-centos client]# cp -R /root/easy-rsa/easyrsa3/ . [root@VM-0-13-centos client]# ll drwxr-xr-x 3 root root 4096 Mar 26 17:07 easyrsa3 [root@VM-0-13-centos client]# cd easyrsa3/ [root@VM-0-13-centos easyrsa3]# ./easyrsa init-pki init-pki complete; you may now create a CA or requests. Your newly created PKI dir is: /etc/openvpn/client/easyrsa3/pki [root@VM-0-13-centos easyrsa3]# ls easyrsa openssl-easyrsa.cnf pki vars.example x509-types 创建客户端CA证书 [root@VM-0-13-centos easyrsa3]# ./easyrsa build-ca Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017 Enter New CA Key Passphrase: #输入ca密码 Re-Enter New CA Key Passphrase: #确认CA密码 Generating RSA private key, 2048 bit long modulus .....................................+++ ...........................................+++ e is 65537 (0x10001) You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Common Name (eg: your user, host, or server name) [Easy-RSA CA]:client-ca #输入ca证书名称 CA creation complete and you may now import and sign cert requests. Your new CA certificate file for publishing is at: /etc/openvpn/client/easyrsa3/pki/ca.crt 创建客户端证书 [root@VM-0-13-centos easyrsa3]# ./easyrsa gen-req client Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017 Generating a 2048 bit RSA private key ........................................................+++ .............................................+++ writing new private key to '/etc/openvpn/client/easyrsa3/pki/easy-rsa-1789.jZxBCq/tmp.1l4buX' Enter PEM pass phrase: #输入客户端CA密码，也是将来登录VPN客户密码！ Verifying - Enter PEM pass phrase: ----- You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Common Name (eg: your user, host, or server name) [client]:niewx #起名字 Keypair and certificate request completed. Your files are: req: /etc/openvpn/client/easyrsa3/pki/reqs/client.req key: /etc/openvpn/client/easyrsa3/pki/private/client.key 导入客户端证书 [root@VM-0-13-centos easyrsa3]# cd /etc/openvpn/easy-rsa/easyrsa3 [root@VM-0-13-centos easyrsa3]# ./easyrsa import-req /etc/openvpn/client/easyrsa3/pki/reqs/client.req client Note: using Easy-RSA configuration from: /etc/openvpn/easy-rsa/easyrsa3/vars Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017 The request has been successfully imported with a short name of: client You may now use this name to perform signing operations on this request. 签约客户端证书 [root@VM-0-13-centos easyrsa3]# cd /etc/openvpn/easy-rsa/easyrsa3 [root@VM-0-13-centos easyrsa3]# ./easyrsa sign client client Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017 You are about to sign the following certificate. Please check over the details shown below for accuracy. Note that this request has not been cryptographically verified. Please be sure it came from a trusted source or that you have verified the request checksum with the sender. Request subject, to be signed as a client certificate for 825 days: subject= commonName = niewx Type the word 'yes' to continue, or any other input to abort. Confirm request details: yes # 输入yes Using configuration from /etc/openvpn/client/easyrsa3/pki/easy-rsa-2777.2aZHdK/tmp.9RSG1Q Enter pass phrase for /etc/openvpn/client/easyrsa3/pki/private/ca.key: #客户端ca密码 Check that the request matches the signature Signature ok The Subject's Distinguished Name is as follows commonName :ASN.1 12:'niewx' Certificate is to be certified until Jun 29 09:16:55 2023 GMT (825 days) Write out database with 1 new entries Data Base Updated Certificate created at: /etc/openvpn/easy-rsa/easyrsa3/pki/issued/client.crt openvpn服务端配置 拷贝服务端证书文件 [root@VM-0-13-centos pki]# cd /etc/openvpn/easy-rsa/easyrsa3/pki [root@VM-0-13-centos pki]# cp ca.crt /etc/openvpn/server/ [root@VM-0-13-centos pki]# cp private/server.key /etc/openvpn/server/ [root@VM-0-13-centos pki]# cp issued/server.crt /etc/openvpn/server/ [root@VM-0-13-centos pki]# cp dh.pem /etc/openvpn/server/ 拷贝客户端证书 [root@VM-0-13-centos pki]# cd /etc/openvpn/client/easyrsa3 [root@VM-0-13-centos pki]# cp /etc/openvpn/easy-rsa/easyrsa3/pki/ca.crt /etc/openvpn/client [root@VM-0-13-centos private]# cp /etc/openvpn/client/easyrsa3/pki/private/client.key /etc/openvpn/client [root@VM-0-13-centos issued]# cp /etc/openvpn/easy-rsa/easyrsa3/pki/issued/client.crt /etc/openvpn/client [root@VM-0-13-centos issued]# cd /etc/openvpn/client/ [root@VM-0-13-centos client]# ls ca.crt client.crt client.key easyrsa3 [root@VM-0-13-centos client]# cd /etc/openvpn/server/ [root@VM-0-13-centos server]# ls ca.crt dh.pem server.crt server.key 配置server.conf [root@VM-0-13-centos openvpn]# cd /etc/openvpn [root@VM-0-13-centos openvpn]# vim server.conf local 0.0.0.0 port 55555 proto tcp dev tun ca /etc/openvpn/server/ca.crt cert /etc/openvpn/server/server.crt key /etc/openvpn/server/server.key # This file should be kept secret dh /etc/openvpn/server/dh.pem server 192.168.1.0 255.255.255.0 ifconfig-pool-persist ipp.txt keepalive 10 120 persist-key persist-tun status openvpn-status.log verb 3 comp-lzo push \"route 10.0.0.0 255.0.0.0\" client-to-client log /var/log/openvpn.log 配置转发参数和iptables规则 [root@VM-0-13-centos openvpn]# sed -i '/net.ipv4.ip_forward/ s/\\(.*= \\).*/\\11/' /etc/sysctl.conf [root@VM-0-13-centos client]# iptables -t nat -A POSTROUTING -s 192.168.1.0/24 -o eth0 -j MASQUERADE [root@VM-0-13-centos client]# iptables -nL -t nat Chain PREROUTING (policy ACCEPT) target prot opt source destination Chain INPUT (policy ACCEPT) target prot opt source destination Chain OUTPUT (policy ACCEPT) target prot opt source destination Chain POSTROUTING (policy ACCEPT) target prot opt source destination MASQUERADE all -- 192.168.1.0/24 0.0.0.0/0 启动oepnven服务端 [root@VM-0-13-centos client]# openvpn /etc/openvpn/server.conf & [1] 5785 [root@VM-0-13-centos client]# ps -ef | grep openvpn root 5785 26254 0 17:37 pts/0 00:00:00 openvpn /etc/openvpn/server.conf 本地机器安装openvpn客户端 可以到https://openvpn.net/community-downloads/下载对应系统客户端安装包 拷贝客户端证书到本地目录 主要/etc/openvpn/client目录下拷贝ca.crt，client.crt，client.key，然后配置下文件client.ovpn，内容如下 client dev tun proto tcp remote 106.53.146.250 55555 resolv-retry infinite nobind persist-key persist-tun ca ca.crt cert client1.crt key client1.key comp-lzo verb 3 运行vpn连接服务端 连接成功后，我们直接内网访问下服务，发现可以直接内网ip访问到prometheus的UI界面，这里说明我们本地电脑成功连接了vpc 自动生成客户端的脚本 [root@VM-0-13-centos client]# cd /etc/openvpn/client [root@VM-0-13-centos client]# cat auto-generate-client.sh # ! /bin/bash set -e OVPN_USER_KEYS_DIR=/etc/openvpn/client/keys EASY_RSA_VERSION=easyrsa3 EASY_RSA_DIR=/etc/openvpn/easy-rsa PKI_DIR=$EASY_RSA_DIR/$EASY_RSA_VERSION/pki for user in \"$@\" do if [ -d \"$OVPN_USER_KEYS_DIR/$user\" ]; then rm -rf $OVPN_USER_KEYS_DIR/$user rm -rf $PKI_DIR/reqs/$user.req rm -rf $PKI_DIR/private/$user.key rm -rf $PKI_DIR/issued/$user.crt sed -i '/'\"$user\"'/d' $PKI_DIR/index.txt #通过index.txt文件查看到证书的情况，首字母为R的证书就是已经被吊销的证书。 exit 0 fi cd $EASY_RSA_DIR/$EASY_RSA_VERSION # 生成客户端 ssl 证书文件 ./easyrsa build-client-full $user nopass # 整理下生成的文件 mkdir -p $OVPN_USER_KEYS_DIR/$user cp $PKI_DIR/ca.crt $OVPN_USER_KEYS_DIR/$user/ # CA 根证书 cp $PKI_DIR/issued/$user.crt $OVPN_USER_KEYS_DIR/$user/ # 客户端证书 cp $PKI_DIR/private/$user.key $OVPN_USER_KEYS_DIR/$user/ # 客户端证书密钥 cp /etc/openvpn/client/sample.ovpn $OVPN_USER_KEYS_DIR/$user/$user.ovpn # 客户端配置文件 sed -i 's/admin/'\"$user\"'/g' $OVPN_USER_KEYS_DIR/$user/$user.ovpn cd $OVPN_USER_KEYS_DIR zip -r $user.zip $user done exit 0 脚本会自动生成客户端证书，执行方式如下，如果需要生成多个用户则在参数加上就行 # sh auto-generate-client.sh test1 test2 ..... 将对应的zip包拷贝给用户，然后再openvpn中指定对应的ovpn文件进行配置下连接即可 © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"linux/vmware-install-centos-set-static-ip-address.html":{"url":"linux/vmware-install-centos-set-static-ip-address.html","title":"【linux】vmware安装centos环境设置静态的ip地址","keywords":"","body":"vmware安装centos环境设置静态的ip地址 有的时候我们为了学习测试，会在自己的笔记本上搭建虚拟机，今天我们来讲下在windows上如何安装centos环境并且设置静态ip，如果是土豪，可以直接到云上购买机器，本篇文章可以忽略。 笔记本主机IP为设置自动获取，不管什么情况下，不受虚拟机影响，只要连接外网就可以正常上网； 只要笔记本主机可以正常访问外网，启动虚拟机中的CentOS 7系统就可以正常访问外网，无需再进行任何设置； 虚拟机设置为固定IP，不管主机在什么网络环境下，是断网环境，还是连接任何网段访问外网的环境下，虚拟机的IP都固定不变，而且使用终端连接，始终不变，正常连接； 虚拟机的固定IP可以按照自己想设置的IP地址网段随意设置，比如我就想设置固定IP为192.168.2.2。 以上4点，网上我没有找到一个帖子可以达到我要求的效果，经过我这段时间研究，经过各种尝试，期间出现各种问题，测试稳定后，总结如下分享给大家，希望对大家有所帮助，少走弯路。 采用方式为NAT模式+固定IP的模式。 配置环境说明：主机为Win10家庭版，虚拟机为VMware Workstation 12 Pro中文版，虚拟机中的Linux系统为CentOS 7 64位。 设置虚拟机的网络连接方式 按照如下图设置，英文版的对照设置即可 配置虚拟机的NAT模式具体地址参数 编辑--虚拟网络编辑器--更改设置 选择VMnet8--取消勾选使用本地DHCP--设置子网IP--网关IP设置（记住此处设置，后面要用到），如下图 说明：修改子网IP设置，实现自由设置固定IP，若你想设置固定IP为192.168.2.2-255，比如192.168.2.2，则子网IP为192.168.2.0；若你想设置固定IP为192.168.1.2-255，比如192.168.1.2，则子网IP为192.168.1.0； 网关IP可以参照如下格式修改：192.168.2.1 配置笔记本主机具体VMnet8本地地址参数 说明：第6步中的IP地址随意设置，但是要保证不能跟你要设置虚拟机的固定IP一样。 修改虚拟机中的CentOS 7系统为固定IP的配置文件 进入centos7命令行界面，修改如下内容 #cd /etc/sysconfig/network-scripts/ #vi ifcfg-eno16777736 说明： #将IPV6…..协议都注释； BOOTPROTO=static #开机协议，有dhcp及static； ONBOOT=yes #设置为开机启动； DNS1=114.114.114.114 #这个是国内的DNS地址，是固定的； IPADDR=192.168.2.2 #你想要设置的固定IP，理论上192.168.2.2-255之间都可以，请自行验证； NETMASK=255.255.255.0 #子网掩码，不需要修改； GATEWAY=192.168.2.1 #网关，这里是你在“2.配置虚拟机的NAT模式具体地址参数”中的（2）选择VMnet8--取消勾选使用本地DHCP--设置子网IP--网关IP设置。 重启网络服务 service network restart 检验配置是否成功 查看修改后的固定IP为192.168.2.2，配置正确 ifconfig 测试虚拟机中的CentOS 7系统是否能连外网，有数据返回，说明可以连接外网 ping www.baidu.com 测试本机是否能ping通虚拟机的固定IP 有数据返回，说明可以使用终端工具正常连接 鼠标放到开始菜单右键，选择命令提示符（管理员），打开命令操作界面： ping 192.168.2.2 远程终端连接 若连接失败是因为CentOS 7的防火墙端口没有打开，比如开启80，3306端口，最后一定要重启防火墙 #查看防火墙状态 systemctl status firewalld #开启80端口 firewall-cmd --zone=public --add-port=80/tcp --permanent #开启3306端口 firewall-cmd --zone=public --add-port=3306/tcp --permanent #重启防火墙： firewall-cmd --reload 连接成功 © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"linux/Use-awk-to-extract-field-information.html":{"url":"linux/Use-awk-to-extract-field-information.html","title":"【linux】使用awk处理字段信息","keywords":"","body":"日常工作中有的时候我们需要对我们的文本进行处理，比如获取某几列的内容，再列之间加入分割符，这里不得不用到awk这个工具了，awk是一种处理文本文件的语言，是一个强大的文本分析工具，下面我们来讲讲如何使用它 我们以下面这个region.txt文本来作为示例 [root@VM-0-3-centos ~]# cat region.txt 地域 名称 简写（不推荐） 是否全量 ap-beijing 华北地区(北京) bj 是 ap-chengdu 西南地区(成都) cd 是 ap-chongqing 西南地区(重庆) cq 是 ap-guangzhou 华南地区(广州) gz 是 获取文本的某几列 获取文本的地域和简写 [root@VM-0-3-centos ~]# cat region.txt | awk -F \" \" '{print $1,$3}' 地域 简写（不推荐） ap-beijing bj ap-chengdu cd ap-chongqing cq ap-guangzhou gz 输入文本指定分隔符 输入文本字段之间用|||分割 [root@VM-0-3-centos ~]# cat region.txt | awk -F \" \" -v OFS='|||' '{print $1,$3}' 地域|||简写（不推荐） ap-beijing|||bj ap-chengdu|||cd ap-chongqing|||cq ap-guangzhou|||gz 参考文档 awk还有很多其他用法，具体可以参考文档 https://www.runoob.com/linux/linux-comm-awk.html © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"linux/Installation-and-use-of-atop-monitoring-tool-in-Linux-system.html":{"url":"linux/Installation-and-use-of-atop-monitoring-tool-in-Linux-system.html","title":"【linux】Linux系统atop监控工具的安装和使用","keywords":"","body":"Linux系统atop监控工具的安装和使用 atop就是一款用于监控Linux系统资源与进程的工具，它以一定的频率记录系统的运行状态，所采集的数据包含系统CPU、内存、磁盘、网络的资源使用情况和进程运行情况，并能以日志文件的方式保存在磁盘中，服务器出现问题后，可获取相应的atop日志文件进行分析。 atop的安装 Centos: yum -y install atop Ubuntu: apt -y install atop atop的启停 systemctl start atop systemctl restart atop systemctl stop atop systemctl status atop atop的配置文件 atop的启动配置文件是/etc/sysconfig/atop [root@vm-0-3-centos atop]# cat /etc/sysconfig/atop LOGOPTS=\"\" LOGINTERVAL=600 LOGGENERATIONS=28 LOGPATH=/var/log/atop LOGINTERVAL=600 表示将默认的600s监控周期修改为30s。建议修改为30s，您可结合实际情况进行修改。 LOGGENERATIONS=28 表示将默认的日志保留时间28天修改为7天。为避免 atop 长时间运行占用太多磁盘空间，建议修改为7天，您可结合实际情况进行修改。 LOGPATH=/var/log/atop 表示日志文件的存储路径，可以用默认的。 # sed -i \"s/LOGINTERVAL=600/LOGINTERVAL=30/\" /etc/sysconfig/atop # sed -i \"s/LOGGENERATIONS=28/LOGGENERATIONS=7/\" /etc/sysconfig/atop atop日志分析 这里当linux出现资源使用过高，想查看下当时是什么进程导致的，可以从日志里分析，用atop -r atop_xxxxx分析日志文件 [root@vm-0-3-centos ~]# cd /var/log/atop/ [root@vm-0-3-centos atop]# ll total 131380 -rw-r--r-- 1 root root 134526756 Dec 28 23:48 atop_20211228 [root@vm-0-3-centos atop]# atop -r atop_20211228 atop常用命令 您可在打开日志文件后，使用以下命令筛选所需数据： c：按照进程的CPU使用率降序筛选。 m：按照进程的内存使用率降序筛选。 d：按照进程的磁盘使用率降序筛选。 a：按照进程资源综合使用率进行降序筛选。 n：按照进程的网络使用率进行降序筛选（使用此命令需安装额外的内核模块，默认不支持）。 t：跳转到下一个监控采集点。 T：跳转到上一个监控采集点。 b：指定时间点，格式为 YYYYMMDDhhmm。 监控字段主要参数说明如下： ATOP 行：主机名、信息采样日期和时间点。 PRC 行：进程整体运行情况。 sys 及 user：CPU 被用于处理进程时，进程在内核态及用户态所占 CPU 的时间比例。 #proc：进程总数。 #zombie：僵死进程的数量。 #exit：Atop 采样周期期间退出的进程数量。 CPU 行：CPU 整体（即多核 CPU 作为一个整体 CPU 资源）的使用情况。CPU 行的各字段数值相加结果为 N00%，N 为 CPU 核数。 sys 及 user：CPU 被用于处理进程时，进程在内核态及用户态所占 CPU 的时间比例。 irq：CPU 被用于处理中断的时间比例。 idle：CPU 处在完全空闲状态的时间比例。 wait：CPU 处在“进程等待磁盘 IO 导致 CPU 空闲”状态的时间比例。 CPL 行：CPU 负载情况。 avg1、avg5 和 avg15：过去1分钟、5分钟和15分钟内运行队列中的平均进程数量。 csw：指示上下文交换次数。 intr：指示中断发生次数。 MEM 行：内存的使用情况。 tot：物理内存总量。 cache ：用于页缓存的内存大小。 buff：用于文件缓存的内存大小。 slab：系统内核占用的内存大小。 SWP 行：交换空间的使用情况。 tot：交换区总量。 free：空闲交换空间大小。 PAG 行：虚拟内存分页情况 swin 及 swout：换入和换出内存页数。 DSK 行：磁盘使用情况，每一个磁盘设备对应一列。如果有 sdb 设备，那么增加一行 DSK 信息。 sda：磁盘设备标识。 busy：磁盘忙时比例。 read 及 write：读、写请求数量。 NET 行：多列 NET 展示了网络状况，包括传输层（TCP 和 UDP）、IP 层以及各活动的网口信息。 xxxxxi：各层或活动网口收包数目。 xxxxxo：各层或活动网口发包数目。 © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"linux/Basic-knowledge-of-iptables.html":{"url":"linux/Basic-knowledge-of-iptables.html","title":"【linux】iptables基础知识","keywords":"","body":"iptables的表和链说明 iptables处理动作 此处列出一些常用的动作，之后的文章会对它们进行详细的示例与总结： ACCEPT：允许数据包通过。 DROP：直接丢弃数据包，不给任何回应信息，这时候客户端会感觉自己的请求泥牛入海了，过了超时时间才会有反应。 REJECT：拒绝数据包通过，必要时会给数据发送端一个响应的信息，客户端刚请求就会收到拒绝的信息。 SNAT：源地址转换，解决内网用户用同一个公网地址上网的问题。 MASQUERADE：是SNAT的一种特殊形式，适用于动态的、临时会变的ip上。 DNAT：目标地址转换。 REDIRECT：在本机做端口映射。 LOG：在/var/log/messages文件中记录日志信息，然后将数据包传递给下一条规则，也就是说除了记录以外不对数据包做任何其他操作，仍然让下一条规则去匹配。 MIRROR: 镜像数据包，也就是将来源 IP 与目的地 IP 对调后，将数据包送回，进行完此处理动作后，将会中断过滤程序 QUEUE: 中断过滤程序，将数据包放入队列，交给其它程序处理。透过自行开发的处理程序，可以进行其它应用，例如：计算联机费.......等 RETURN: 结束在目前规则链中的过滤程序，返回主规则链继续过滤，如果把自订规则链看成是一个子程序，那么这个动作，就相当提早结束子程序并返回到主程序中 MARK: 将数据包标上某个代号，以便提供作为后续过滤的条件判断依据，进行完此处理动作后，将会继续比对其它规则 REJECT REJECT动作的常用选项为–reject-with 使用–reject-with选项，可以设置提示信息，当对方被拒绝时，会提示对方为什么被拒绝。 可用值如下 icmp-net-unreachable icmp-host-unreachable icmp-port-unreachable icmp-proto-unreachable icmp-net-prohibited icmp-host-pro-hibited icmp-admin-prohibited 当不设置任何值时，默认值为icmp-port-unreachable。 SNAT 配置SNAT，可以隐藏网内主机的IP地址，也可以共享公网IP，访问互联网，如果只是共享IP的话，只配置如下SNAT规则即可。 iptables -t nat -A POSTROUTING -s 10.1.0.0/16 -j SNAT --to-source 公网IP MASQUERADE 动态的SNAT操作，如下命令表示将10.1网段的报文的源IP修改为eth0网卡中可用的地址 iptables -t nat -A POSTROUTING -s 10.1.0.0/16 -o eth0 -j MASQUERADE REDIRECT 在本机进行目标端口映射时可以使用REDIRECT动作。 iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-ports 8080 iptables查询参数说明 -L: 查看列表 -t: 查看指定表，默认是filter表 -v: 显示详情信息 -n: 表示不对IP地址进行名称反解，直接显示IP地址 --line-numbers: 显示规则编号 -x: 选项表示显示计数器的精确值 常用命令： iptables -t 表名 -L iptables -t 表名 -L 链名 iptables -t 表名 -v -L iptables -t 表名 -v -x -L iptables规则配置参数说明 -F: 选项为flush之意，即冲刷指定的链，即删除指定链中的所有规则 -I: 指明将”规则”插入至哪个链中，-I表示insert，即插入的意思，所以-I INPUT表示将规则插入于INPUT链中，即添加规则之意，-I表示在链的首部插入规则 -s: 指明”匹配条件”中的”源地址”，即如果报文的源地址属于-s对应的地址，那么报文则满足匹配条件，-s为source之意，表示源地址。 -j: 指明当”匹配条件”被满足时，所对应的动作 -A: 为append之意，所以，-A INPUT则表示在INPUT链中追加规则，-A表示在链的尾部追加规则 -D: 选项表示删除指定链中的某条规则 -R: 选项表示修改指定的链 -P: 修改链的默认策略 -I INPUT 2表示在INPUT链中新增规则，新增的规则的编号为2，不指定编号，默认是在最上面。 -D INPUT 3表示删除INPUT链中的第3条规则 -R 选项修改某个规则时，必须指定规则对应的原本的匹配条件（如果有多个匹配条件，都需要指定）。 -P FORWARD DROP表示将表中FORWRD链的默认策略改为DROP 规则保存和还原 将规则重定向到/etc/sysconfig/iptables文件中即可 iptables-save > /etc/sysconfig/iptables 将/etc/sysconfig/iptables中的规则重新载入为当前的iptables规则 iptables-restore 规则详细参数配置 -p: 指定需要匹配的报文的协议类型 -s: 可以匹配报文的源地址，指定源地址时，一次指定多个，用”逗号”隔开即可，！-s表示取反 -d: 指定”目标地址”作为匹配条件，！-d表示取反 -i: 指定网卡名 -o: 选项用于匹配报文将从哪个网卡流出 --dport: 指定目标端口 --sport: 指定源端口 指定目的和源端口需要引用模块 iptables -t filter -I INPUT -s 192.168.0.1 -p tcp -m tcp --dport -j REJECT 我们就使用了扩展匹配条件--dport，指定了匹配报文的目标端口，如果外来报文的目标端口为本机的22号端口（ssh默认端口），则拒绝之，而在使用--dport之前，我们使用-m选项，指定了对应的扩展模块为tcp，也就是说，如果想要使用--dport这个扩展匹配条件，则必须依靠某个扩展模块完成，上例中，这个扩展模块就是tcp扩展模块，最终，我们使用的是tcp扩展模块中的dport扩展匹配条件。 当使用-p选项指定了报文的协议时，如果在没有使用-m指定对应的扩展模块名称的情况下，使用了扩展匹配条件， iptables默认会调用与-p选项对应的协议名称相同的模块。 指定目的或者源端口范围： --dport 22:25 表示目标端口为22到25之间的所有端口 --dport 22: 表示匹配0号到22号之间的所有端口 --dport :22 表示匹配80号端口以及其以后的所有端口（直到65535） iptables模块 tcp扩展模块 常用的扩展匹配条件如下： -p tcp -m tcp --sport 用于匹配tcp协议报文的源端口，可以使用冒号指定一个连续的端口范围 -p tcp -m tcp --dport 用于匹配tcp协议报文的目标端口，可以使用冒号指定一个连续的端口范围 #示例如下 iptables -t filter -I OUTPUT -d 192.168.1.146 -p tcp -m tcp --sport 22 -j REJECT iptables -t filter -I INPUT -s 192.168.1.146 -p tcp -m tcp --dport 22:25 -j REJECT iptables -t filter -I INPUT -s 192.168.1.146 -p tcp -m tcp --dport :22 -j REJECT iptables -t filter -I INPUT -s 192.168.1.146 -p tcp -m tcp --dport 80: -j REJECT iptables -t filter -I OUTPUT -d 192.168.1.146 -p tcp -m tcp ! --sport 22 -j ACCEPT multiport扩展模块 常用的扩展匹配条件如下： -p tcp -m multiport --sports 用于匹配报文的源端口，可以指定离散的多个端口号,端口之间用”逗号”隔开 -p udp -m multiport --dports 用于匹配报文的目标端口，可以指定离散的多个端口号，端口之间用”逗号”隔开 #示例如下 iptables -t filter -I OUTPUT -d 192.168.1.146 -p udp -m multiport --sports 137,138 -j REJECT iptables -t filter -I INPUT -s 192.168.1.146 -p tcp -m multiport --dports 22,80 -j REJECT iptables -t filter -I INPUT -s 192.168.1.146 -p tcp -m multiport ! --dports 22,80 -j REJECT iptables -t filter -I INPUT -s 192.168.1.146 -p tcp -m multiport --dports 80:88 -j REJECT iptables -t filter -I INPUT -s 192.168.1.146 -p tcp -m multiport --dports 22,80:88 -j REJECT iprange模块 包含的扩展匹配条件如下 --src-range：指定连续的源地址范围 --dst-range：指定连续的目标地址范围 #示例 iptables -t filter -I INPUT -m iprange --src-range 192.168.1.127-192.168.1.146 -j DROP iptables -t filter -I OUTPUT -m iprange --dst-range 192.168.1.127-192.168.1.146 -j DROP iptables -t filter -I INPUT -m iprange ! --src-range 192.168.1.127-192.168.1.146 -j DROP string模块 常用扩展匹配条件如下 --algo：指定对应的匹配算法，可用算法为bm、kmp，此选项为必需选项。 --string：指定需要匹配的字符串 #示例 iptables -t filter -I INPUT -p tcp --sport 80 -m string --algo bm --string \"OOXX\" -j REJECT iptables -t filter -I INPUT -p tcp --sport 80 -m string --algo bm --string \"OOXX\" -j REJECT time模块 常用扩展匹配条件如下 --timestart：用于指定时间范围的开始时间，不可取反 --timestop：用于指定时间范围的结束时间，不可取反 --weekdays：用于指定”星期几”，可取反 --monthdays：用于指定”几号”，可取反 --datestart：用于指定日期范围的开始日期，不可取反 --datestop：用于指定日期范围的结束时间，不可取反 iptables -t filter -I OUTPUT -p tcp --dport 80 -m time --timestart 09:00:00 --timestop 19:00:00 -j REJECT iptables -t filter -I OUTPUT -p tcp --dport 443 -m time --timestart 09:00:00 --timestop 19:00:00 -j REJECT iptables -t filter -I OUTPUT -p tcp --dport 80 -m time --weekdays 6,7 -j REJECT iptables -t filter -I OUTPUT -p tcp --dport 80 -m time --monthdays 22,23 -j REJECT iptables -t filter -I OUTPUT -p tcp --dport 80 -m time ! --monthdays 22,23 -j REJECT iptables -t filter -I OUTPUT -p tcp --dport 80 -m time --timestart 09:00:00 --timestop 18:00:00 --weekdays 6,7 -j REJECT iptables -t filter -I OUTPUT -p tcp --dport 80 -m time --weekdays 5 --monthdays 22,23,24,25,26,27,28 -j REJECT iptables -t filter -I OUTPUT -p tcp --dport 80 -m time --datestart 2017-12-24 --datestop 2017-12-27 -j REJECT connlimit 模块 常用的扩展匹配条件如下 --connlimit-above：单独使用此选项时，表示限制每个IP的链接数量。 --connlimit-mask：此选项不能单独使用，在使用--connlimit-above选项时，配合此选项，则可以针对”某类IP段内的一定数量的IP”进行连接数量的限制，如果不明白可以参考上文的详细解释。 示例 iptables -I INPUT -p tcp --dport 22 -m connlimit --connlimit-above 2 -j REJECT iptables -I INPUT -p tcp --dport 22 -m connlimit --connlimit-above 20 --connlimit-mask 24 -j REJECT iptables -I INPUT -p tcp --dport 22 -m connlimit --connlimit-above 10 --connlimit-mask 27 -j REJECT limit模块 常用的扩展匹配条件如下 --limit-burst：类比”令牌桶”算法，此选项用于指定令牌桶中令牌的最大数量，上文中已经详细的描述了”令牌桶”的概念，方便回顾。 --limit：类比”令牌桶”算法，此选项用于指定令牌桶中生成新令牌的频率，可用时间单位有second、minute 、hour、day。 示例 iptables -t filter -I INPUT -p icmp -m limit --limit-burst 3 --limit 10/minute -j ACCEPT iptables -t filter -A INPUT -p icmp -j REJECT 自定义链 自定义链参数说明: -N: -N IN_WEB表示创建一个自定义链，自定义链的名称为IN_WEB -E: 选项可以修改自定义链名 -X: 选项可以删除自定义链，删除满足条件:1.自定义链没有被任何默认链引用，即自定义链的引用计数为0; 2.自定义链中没有任何规则，即自定义链为空。 # 创建自定义链 iptables -t filter -N IN_WEB # 往自定义链添加规则 iptables -t filter -I IN_WEB -s 192.168.1.1 -j REJECT # 引用自定义链 iptables -t filter -I INPUT -p tcp --dport 80 -j IN_WEB # 重命名自定义链 iptables -E IN_WEB WEB # 删除自定义链 iptables -X WEB iptables作为防火墙的前提 #使用如下命令查看当前主机是否已经开启了核心转发，0表示为开启，1表示已开启 cat /proc/sys/net/ipv4/ip_forward #使用如下两种方法均可临时开启核心转发，立即生效，但是重启网络配置后会失效。 方法一：echo 1 > /proc/sys/net/ipv4/ip_forward 方法二：sysctl -w net.ipv4.ip_forward=1 #使用如下方法开启核心转发功能，重启网络服务后永久生效。 配置/etc/sysctl.conf文件（centos7中配置/usr/lib/sysctl.d/00-system.conf文件），在配置文件中将 net.ipv4.ip_forward设置为1 iptables白名单 先添加一条默认拒绝的规则，然后再为需要放行的报文设置规则 iptables -A FORWARD -j REJECT iptables -I FORWARD -s 10.1.0.0/16 -p tcp --dport 80 -j ACCEPT iptables -I FORWARD -d 10.1.0.0/16 -p tcp --sport 80 -j ACCEPT iptables -I FORWARD -s 10.1.0.0/16 -p tcp --dport 22 -j ACCEPT iptables -I FORWARD -d 10.1.0.0/16 -p tcp --sport 22 -j ACCEPT 注意事项 规则的顺序，相同服务的规则，更严格的规则应该放在前面。 当规则中有多个匹配条件时，条件之间默认存在”与”的关系。 更容易被匹配到的规则放置在前面。 IPTABLES所在主机作为网络防火 墙时，在配置规则时，应着重考虑方向性，双向都要考虑，从外到内，从内到外。 在配置IPTABLES白名单时，往往会将链的默认策略设置为ACCEPT，通过在链的最后设置REJECT规则实现白名单机制，而不是将链的默认策略设置为DROP，如果将链的默认策略设置为DROP，当链中的规则被清空时，管理员的请求也将会被DROP掉。 © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"database/mysql-devops-note.html":{"url":"database/mysql-devops-note.html","title":"【datebase】mysql运维笔记","keywords":"","body":"Mysql运维知识 这里总结下mysql数据库常用的运维操作 登录MySQL 登录MySQL的命令是mysql， mysql 的使用语法如下： mysql [-u username] [-h host] [-p[password]] [dbname] username 与 password 分别是 MySQL 的用户名与密码，mysql的初始管理帐号是root，没有密码，注意：这个root用户不是Linux的系统用户。MySQL默认用户是root，由于初始没有密码，第一次进时只需键入mysql即可。 [root@test1 local]# mysql Welcome to the MySQL monitor.　Commands end with ; or \\g. Your MySQL connection id is 1 to server version: 4.0.16-standard Type 'help;' or '\\h' for help. Type '\\c' to clear the buffer. mysql> 出现了“mysql>”提示符，恭喜你，安装成功！ 增加了密码后的登录格式如下： mysql -u root -p Enter password: (输入密码) 其中-u后跟的是用户名，-p要求输入密码，回车后在输入密码处输入密码。 注意：这个mysql文件在/usr/bin目录下，与后面讲的启动文件/etc/init.d/mysql不是一个文件。 MySQL的几个重要目录 MySQL安装完成后不象SQL Server默认安装在一个目录，它的数据库文件、配置文件和命令文件分别在不同的目录，了解这些目录非常重要，尤其对于Linux的初学者，因为Linux本身的目录结构就比较复杂，如果搞不清楚MySQL的安装目录那就无从谈起深入学习。 下面就介绍一下这几个目录。 数据库目录 /var/lib/mysql/ 配置文件 /usr/share/mysql（mysql.server命令及配置文件） 相关命令 /usr/bin(mysqladmin mysqldump等命令) 启动脚本 /etc/rc.d/init.d/（启动脚本文件mysql的目录） 修改登录密码 MySQL默认没有密码，安装完毕增加密码的重要性是不言而喻的。 命令 usr/bin/mysqladmin -u root password ‘new-password’ 格式：mysqladmin -u用户名 -p旧密码 password 新密码 例子 例1：给root加个密码123456。 键入以下命令 ： [root@test1 local]# /usr/bin/mysqladmin -u root password 123456 注：因为开始时root没有密码，所以-p旧密码一项就可以省略了。 测试是否修改成功 不用密码登录 [root@test1 local]# mysql ERROR 1045: Access denied for user: ‘root@localhost’ (Using password: NO) 显示错误，说明密码已经修改。 用修改后的密码登录 [root@test1 local]# mysql -u root -p Enter password: (输入修改后的密码123456) Welcome to the MySQL monitor.　Commands end with ; or \\g. Your MySQL connection id is 4 to server version: 4.0.16-standard Type 'help;' or '\\h' for help. Type '\\c' to clear the buffer. mysql> 成功！ 这是通过mysqladmin命令修改口令，也可通过修改库来更改口令。 启动与停止 启动 MySQL安装完成后启动文件mysql在/etc/init.d目录下，在需要启动时运行下面命令即可。 [root@test1 init.d]# /etc/init.d/mysql start 停止 /usr/bin/mysqladmin -u root -p shutdown 自动启动 1）察看mysql是否在自动启动列表中 [root@test1 local]#　/sbin/chkconfig –list 2）把MySQL添加到你系统的启动服务组里面去 [root@test1 local]#　/sbin/chkconfig　– add　mysql 3）把MySQL从启动服务组里面删除。 [root@test1 local]#　/sbin/chkconfig　– del　mysql 更改MySQL目录 MySQL默认的数据文件存储目录为/var/lib/mysql。假如要把目录移到/home/data下需要进行下面几步 1.home目录下建立data目录 cd /home mkdir data 2.把MySQL服务进程停掉 mysqladmin -u root -p shutdown 3.把/var/lib/mysql整个目录移到/home/data mv /var/lib/mysql　/home/data/ 这样就把MySQL的数据文件移动到了/home/data/mysql下 4.找到my.cnf配置文件 如果/etc/目录下没有my.cnf配置文件，请到/usr/share/mysql/下找到*.cnf文件，拷贝其中一个到/etc/并改名为my.cnf)中。命令如下： [root@test1 mysql]# cp /usr/share/mysql/my-medium.cnf　/etc/my.cnf 5.编辑MySQL的配置文件/etc/my.cnf 为保证MySQL能够正常工作，需要指明mysql.sock文件的产生位置。 修改socket=/var/lib/mysql/mysql.sock一行中等号右边的值为：/home/mysql/mysql.sock 。操作如下： vi　 my.cnf　　　 (用vi工具编辑my.cnf文件，找到下列数据修改之) # The MySQL server [mysqld] port　　　= 3306 #socket　 = /var/lib/mysql/mysql.sock（原内容，为了更稳妥用“#”注释此行） socket　 = /home/data/mysql/mysql.sock　　　（加上此行） 6.修改MySQL启动脚本/etc/rc.d/init.d/mysql 最后，需要修改MySQL启动脚本/etc/rc.d/init.d/mysql，把其中datadir=/var/lib/mysql一行中，等号右边的路径改成你现在的实际存放路径：home/data/mysql。 [root@test1 etc]# vi　/etc/rc.d/init.d/mysql #datadir=/var/lib/mysql　　　　（注释此行） datadir=/home/data/mysql　　 （加上此行） 7.重新启动MySQL服务 /etc/rc.d/init.d/mysql　start 或用reboot命令重启Linux，如果工作正常移动就成功了，否则对照前面的7步再检查一下。 MySQL的常用操作 注意：MySQL中每个命令后都要以分号；结尾。 显示数据库 mysql> show databases; +----------+ | Database | +----------+ | mysql　　| | test　　 | +----------+ rows in set (0.04 sec) Mysql刚安装完有两个数据库：mysql和test。mysql库非常重要，它里面有MySQL的系统信息，我们改密码和新增用户，实际上就是用这个库中的相关表进行操作。 显示数据库中的表 mysql> use mysql; （打开库，对每个库进行操作就要打开此库，类似于foxpro ） Database changed mysql> show tables; +-----------------+ | Tables_in_mysql | +-----------------+ | columns_priv　　| | db　　　　　　　| | func　　　　　　| | host　　　　　　| | tables_priv　　 | | user　　　　　　| +-----------------+ 6 rows in set (0.01 sec) 显示数据表的结构 describe 表名; 显示表中的记录 select * from 表名; 例如：显示mysql库中user表中的纪录。所有能对MySQL用户操作的用户都在此表中。 Select * from user; 建库 create database 库名; 例如：创建一个名字位aaa的库 mysql> create databases aaa; 建表 use 库名； create table 表名 (字段设定列表)； 例如：在刚创建的aaa库中建立表name,表中有id(序号，自动增长)，xm（姓名）,xb（性别）,csny（出身年月）四个字段 use aaa; mysql> create table name (id int(3) auto_increment not null primary key, xm char(8),xb char(2),csny date); 可以用describe命令察看刚建立的表结构。 mysql> describe name; +-------+---------+------+-----+---------+----------------+ | Field | Type | Null | Key | Default | Extra　　　　　| +-------+---------+------+-----+---------+----------------+ | id　　| int(3)　|　　　| PRI | NULL　　| auto_increment | | xm　　| char(8) | YES　|　　 | NULL　　|　　　　　　　　| | xb　　| char(2) | YES　|　　 | NULL　　|　　　　　　　　| | csny　| date　　| YES　|　　 | NULL　　|　　　　　　　　| +-------+---------+------+-----+---------+----------------+ 增加记录 例如：增加几条相关纪录。 mysql> insert into name values(”,’张三’,’男’,’1971-10-01′); mysql> insert into name values(”,’白云’,’女’,’1972-05-20′); 可用select命令来验证结果。 mysql> select * from name; +—-+——+——+————+ | id | xm　 | xb　 | csny　　　 | +—-+——+——+————+ |　1 | 张三 | 男　 | 1971-10-01 | |　2 | 白云 | 女　 | 1972-05-20 | +—-+——+——+————+ 修改纪录 例如：将张三的出生年月改为1971-01-10 mysql> update name set csny=’1971-01-10′ where xm=’张三’; 删除纪录 例如：删除张三的纪录。 mysql> delete from name where xm=’张三’; 删库和删表 drop database 库名; drop table 表名； 增加MySQL用户 格式：grant select on 数据库.* to 用户名@登录主机 identified by “密码” 1、增加一个用户user_1密码为123，让他可以在任何主机上登录，并对所有数据库有查询、插入、修改、删除的权限。首先用以root用户连入MySQL，然后键入以下命令： mysql> grant select,insert,update,delete on *.* to user_1@”%” Identified by “123″; 例1增加的用户是十分危险的，如果知道了user_1的密码，那么他就可以在网上的任何一台电脑上登录你的MySQL数据库并对你的数据为所欲为了，解决办法见例2。 2、增加一个用户user_2密码为123,让此用户只可以在localhost上登录，并可以对数据库aaa进行查询、插入、修改、删除的操作（localhost指本地主机，即MySQL数据库所在的那台主机），这样用户即使用知道user_2的密码，他也无法从网上直接访问数据库，只能通过MYSQL主机来操作aaa库。 mysql>grant select,insert,update,delete on aaa.* to user_2@localhost identified by “123″; 用新增的用户如果登录不了MySQL，在登录时用如下命令： mysql -u user_1 -p　-h 192.168.113.50　（-h后跟的是要登录主机的ip地址） mysql的数据导入导出 mysql数据库执行导入导出命令在可执行的mysql目录下执行 从数据库导出数据库或表文件： mysqldump -u用戶名 -p密码 -d 数据库名 表名 > 脚本名; 导出整个数据库结构和数据 mysqldump -h localhost -uroot -p123456 database > e:\\dump.sql 导出单个数据表结构和数据 mysqldump -h localhost -uroot -p123456 database table > e:\\dump.sql 导出整个数据库结构（不包含数据） mysqldump -h localhost -uroot -p123456 -d database > e:\\dump.sql 导出单个数据表结构（不包含数据） mysqldump -h localhost -uroot -p123456 -d database table > e:\\dump.sql 导入数据库或表到数据库（数据库要先建好） 方法1：mysql -h localhost -uroot -p123456 -d database table mysql中用户的权限分配 用户管理 mysql>use mysql; 查看 mysql> select host,user,password from user ; 创建 mysql> create user zx_root IDENTIFIED by 'xxxxx'; //identified by 会将纯文本密码加密作为散列值存储 修改 mysql>rename user feng to newuser；//mysql 5之后可以使用，之前需要使用update 更新user表 删除 mysql>drop user newuser; //mysql5之前删除用户时必须先使用revoke 删除用户权限，然后删除用户，mysql5之后drop 命令可以删除用户的同时删除用户的相关权限 更改密码 mysql> set password for zx_root =password('xxxxxx'); mysql> update mysql.user set password=password('xxxx') where user='otheruser' 查看用户权限 mysql> show grants for zx_root; 赋予权限 mysql> grant select on dmc_db.* to zx_root; 回收权限 mysql> revoke select on dmc_db.* from zx_root; //如果权限不存在会报错 上面的命令也可使用多个权限同时赋予和回收，权限之间使用逗号分隔 mysql> grant select，update，delete ，insert on dmc_db.* to zx_root; 如果想立即看到结果使用 flush privileges ; 命令更新 设置权限时必须给出一下信息 要授予的权限 被授予访问权限的数据库或表 用户名 grant和revoke可以在几个层次上控制访问权限 整个服务器，使用 grant ALL 和revoke ALL 整个数据库，使用on database.* 特点表，使用on database.table 特定的列 特定的存储过程 user表中host列的值的意义 % 匹配所有主机 localhost localhost不会被解析成IP地址，直接通过UNIXsocket连接 127.0.0.1 会通过TCP/IP协议连接，并且只能在本机访问； ::1 ::1就是兼容支持ipv6的，表示同ipv4的127.0.0.1 grant 普通数据用户，查询、插入、更新、删除 数据库中所有表数据的权利。 grant select on testdb.* to common_user@’%’ grant insert on testdb.* to common_user@’%’ grant update on testdb.* to common_user@’%’ grant delete on testdb.* to common_user@’%’ 或者，用一条 MySQL 命令来替代： grant select, insert, update, delete on testdb.* to common_user@’%’ 9>.grant 数据库开发人员，创建表、索引、视图、存储过程、函数。。。等权限。 grant 创建、修改、删除 MySQL 数据表结构权限。 grant create on testdb.* to developer@’192.168.0.%’; grant alter on testdb.* to developer@’192.168.0.%’; grant drop on testdb.* to developer@’192.168.0.%’; grant 操作 MySQL 外键权限。 grant references on testdb.* to developer@’192.168.0.%’; grant 操作 MySQL 临时表权限。 grant create temporary tables on testdb.* to developer@’192.168.0.%’; grant 操作 MySQL 索引权限。 grant index on testdb.* to developer@’192.168.0.%’; grant 操作 MySQL 视图、查看视图源代码 权限。 grant create view on testdb.* to developer@’192.168.0.%’; grant show view on testdb.* to developer@’192.168.0.%’; grant 操作 MySQL 存储过程、函数 权限。 grant create routine on testdb.* to developer@’192.168.0.%’; -- now, can show procedure status grant alter routine on testdb.* to developer@’192.168.0.%’; -- now, you can drop a procedure grant execute on testdb.* to developer@’192.168.0.%’; 10>.grant 普通 DBA 管理某个 MySQL 数据库的权限。 grant all privileges on testdb to dba@’localhost’ 其中，关键字 “privileges” 可以省略。 11>.grant 高级 DBA 管理 MySQL 中所有数据库的权限。 grant all on *.* to dba@’localhost’ 12>.MySQL grant 权限，分别可以作用在多个层次上。 1. grant 作用在整个 MySQL 服务器上： grant select on *.* to dba@localhost; -- dba 可以查询 MySQL 中所有数据库中的表。 grant all on *.* to dba@localhost; -- dba 可以管理 MySQL 中的所有数据库 2. grant 作用在单个数据库上： grant select on testdb.* to dba@localhost; -- dba 可以查询 testdb 中的表。 3. grant 作用在单个数据表上： grant select, insert, update, delete on testdb.orders to dba@localhost; 4. grant 作用在表中的列上： grant select(id, se, rank) on testdb.apache_log to dba@localhost; 5. grant 作用在存储过程、函数上： grant execute on procedure testdb.pr_add to ’dba’@’localhost’ grant execute on function testdb.fn_add to ’dba’@’localhost’ 注意：修改完权限以后 一定要刷新服务，或者重启服务，刷新服务用：FLUSH PRIVILEGES。 mysql主从复制和读写分离的搭建 Mysql的主从复制的搭建，大家可以采用在linux，windows，docker，dockr-compose来搭建mysql 本次采用方式为docker-composer来搭建多个mysql服务端 目录结构如下 docker-compose.yml文件内容 version: '2' services: mysql-master: build: context: ./ dockerfile: master/Dockerfile environment: - \"MYSQL_ROOT_PASSWORD=root\" - \"MYSQL_DATABASE=replicas_db\" links: - mysql-slave ports: - \"33065:3306\" restart: always hostname: mysql-master mysql-slave: build: context: ./ dockerfile: slave/Dockerfile environment: - \"MYSQL_ROOT_PASSWORD=root\" - \"MYSQL_DATABASE=replicas_db\" ports: - \"33066:3306\" restart: always hostname: mysql-slave Master的dockerfile和my.cnf Dockerfile内容 FROM mysql:5.7 MAINTAINER harrison ADD ./master/my.cnf /etc/mysql/my.cnf my.cnf内容 [mysqld] ## 设置server_id，一般设置为IP，注意要唯一 server_id=100 ## 复制过滤：也就是指定哪个数据库不用同步（mysql库一般不同步） binlog-ignore-db=mysql ## 开启二进制日志功能，可以随便取，最好有含义（关键就是这里了） log-bin=replicas-mysql-bin ## 为每个session分配的内存，在事务过程中用来存储二进制日志的缓存 binlog_cache_size=1M ## 主从复制的格式（mixed,statement,row，默认格式是statement） binlog_format=mixed ## 二进制日志自动删除/过期的天数。默认值为0，表示不自动删除。 expire_logs_days=7 ## 跳过主从复制中遇到的所有错误或指定类型的错误，避免slave端复制中断。 ## 如：1062错误是指一些主键重复，1032错误是因为主从数据库数据不一致 slave_skip_errors=1062 Slave的dockerfile和my.cnf Dockerfile内容 FROM mysql:5.7 MAINTAINER harrison ADD ./slave/my.cnf /etc/mysql/my.cnf my.cnf内容 [mysqld] ## 设置server_id，一般设置为IP，注意要唯一 server_id=101 ## 复制过滤：也就是指定哪个数据库不用同步（mysql库一般不同步） binlog-ignore-db=mysql ## 开启二进制日志功能，以备Slave作为其它Slave的Master时使用 log-bin=replicas-mysql-slave1-bin ## 为每个session 分配的内存，在事务过程中用来存储二进制日志的缓存 binlog_cache_size=1M ## 主从复制的格式（mixed,statement,row，默认格式是statement） binlog_format=mixed ## 二进制日志自动删除/过期的天数。默认值为0，表示不自动删除。 expire_logs_days=7 ## 跳过主从复制中遇到的所有错误或指定类型的错误，避免slave端复制中断。 ## 如：1062错误是指一些主键重复，1032错误是因为主从数据库数据不一致 slave_skip_errors=1062 ## relay_log配置中继日志 relay_log=replicas-mysql-relay-bin ## log_slave_updates表示slave将复制事件写进自己的二进制日志 log_slave_updates=1 ## 防止改变数据(除了特殊的线程) read_only=1 进入 docker 目录，运行 docker-compose 启动命令。 $ docker-compose up -d 检查从库的起始状态 $ show master status; 从数据库处于 未同步复制状态。 检查主库的状态 $ show master status; 记录 主数据库 binary-log的文件名称和数据同步起始位置。 File: replicas-mysql-bin.000003 Position: 154 从库配置主库信息 在 从数据库上运行主数据库的相关配置sql进行主从关联 CHANGE MASTER TO MASTER_HOST='mysql-master', MASTER_USER='root', MASTER_PASSWORD='root', MASTER_LOG_FILE='replicas-mysql-bin.000003', MASTER_LOG_POS=154; 重新启动 slave 服务 $ stop slave $ start slave 进一步检查从数据库的状态信息，两者已经进行数据同步关联。 master修改密码如何同步slave 所以，更新密码后，只需要： change master to master_user='replication user', master_password='new passwd'; mysql慢查询的原因 没有索引或者没有用到索引(这是查询慢最常见问题，是程序设计的缺陷) I/O吞吐量小，形成了瓶颈效应。 没有创建计算列导致查询不优化。 内存不足 网络速度慢 查询出的数据量过大(可以采用多次查询，其他的方法降低数据量) 锁或者死锁(这也是查询慢最常见的问题，是程序设计的缺陷) sp_lock,sp_who,活动的用户查看,原因是读写竞争资源。 返回了不必要的行和列 查询语句不好，没有优化 © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"database/mongodb-devops-note.html":{"url":"database/mongodb-devops-note.html","title":"【datebase】mongodb运维笔记","keywords":"","body":"Mongdb运维知识 mongodb集群的搭建 系统环境 Centos7.5、MongoDB4.0.2、关闭防火墙。 Ip 路由服务端口 配置服务端口 分片1端口 分片2端口 分片3端口 192.168.30.30 27017 27018 27001 27002 27003 192.168.30.31 27017 27018 27001 27002 27003 192.168.30.32 27017 27018 27001 27002 27003 三台机器的配置服务(27018)形成复制集，分片1、2、3也在各机器都部署一个实例，它们之间形成复制集，客户端直接连接3个路由服务与之交互，配置服务和分片服务对客户端是透明的。 服务器的安装及配置(3台服务器执行相同操作) 下载解压MongoDB 到MongoDB官网下载：https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-4.0.6.tgz 解压到/home/mongodb，设置环境变量: echo 'export PATH=$PATH:/home/mongodb/bin' >> /etc/profile 保存后执行： source /etc/profile 创建路由、配置、分片等的相关目录与文件 启动配置文件存放的文件夹：mkdir -p /home/mongodb/conf 配置服务数据存放目录：mkdir -p /home/mongodb/data/config 分片1服务数据存放目录：mkdir -p /home/mongodb/data/shard1 分片2服务数据存放目录：mkdir -p /home/mongodb/data/shard2 分片3服务数据存放目录：mkdir -p /home/mongodb/data/shard3 配置服务日志存放文件：touch /home/mongodb/log/config.log 路由服务日志存放文件：touch /home/mongodb/log/mongos.log 分片1服务日志存放文件：touch /home/mongodb/log/shard1.log 分片2服务日志存放文件：touch /home/mongodb/log/shard2.log 分片3服务日志存放文件：touch /home/mongodb/log/shard3.log mkdir -p /home/mongodb/conf mkdir -p /home/mongodb/data/config mkdir -p /home/mongodb/data/shard1 mkdir -p /home/mongodb/data/shard2 mkdir -p /home/mongodb/data/shard3 mkdir -p /home/mongodb/log touch /home/mongodb/log/config.log touch /home/mongodb/log/mongos.log touch /home/mongodb/log/shard1.log touch /home/mongodb/log/shard2.log touch /home/mongodb/log/shard3.log 配置服务器部署(3台服务器执行相同操作) 在/home/mongodb/conf目录创建config.conf [root@master conf]# cat config.conf dbpath=/home/mongodb/data/config logpath=/home/mongodb/log/config.log port=27018 logappend=true fork=true maxConns=5000 #复制集名称 replSet=configs #置参数为true configsvr=true #允许任意机器连接 bind_ip=0.0.0.0 配置复制集 scp config.conf root@192.168.30.31:/home/mongodb/conf scp config.conf root@192.168.30.32:/home/mongodb/conf 分别启动三台服务器的配置服务： mongod -f /home/mongodb/conf/config.conf 连接mongo,只需在任意一台机器执行即可： mongo --host 10.211.55.3 --port 27018 切换数据库： use admin 初始化复制集： rs.initiate({_id:\"configs\",members:[{_id:0,host:\"192.168.30.30:27018\"},{_id:1,host:\"192.168.30.31:27018\"}, {_id:2,host:\"192.168.30.32:27018\"}]}) 其中_id:\"configs\"的configs是上面config.conf配置文件里的复制集名称，把三台服务器的配置服务组成复制集。 查看状态： rs.status() 等几十秒左右，执行上面的命令查看状态，三台机器的配置服务就已形成复制集，其中1台为PRIMARY，其他2台为SECONDARY。 分片服务部署(3台服务器执行相同操作) 在/home/mongodb/conf目录创建shard1.conf、shard2.conf、shard3.conf，内容如下： [root@master conf]# cat shard1.conf dbpath=/home/mongodb/data/shard1 #其他2个分片对应修改为shard2、shard3文件夹 logpath=/home/mongodb/log/shard1.log #其他2个分片对应修改为shard2.log、shard3.log port=27001 #其他2个分片对应修改为27002、27003 logappend=true fork=true maxConns=5000 storageEngine=mmapv1 shardsvr=true replSet=shard1 #其他2个分片对应修改为shard2、shard3 bind_ip=0.0.0.0 端口分别是27001、27002、27003，分别对应shard1.conf、shard2.conf、shard3.conf。 还有数据存放目录、日志文件这几个地方都需要对应修改。 在3台机器的相同端口形成一个分片的复制集，由于3台机器都需要这3个文件，所以根据这9个配置文件分别启动分片服务： scp shard* root@192.168.30.30:/home/mongodb/conf scp shard* root@192.168.30.31:/home/mongodb/conf scp shard* root@192.168.30.32:/home/mongodb/conf mongod --smallfiles -f /home/mongodb/conf/shard1.conf mongod --smallfiles -f /home/mongodb/conf/shard2.conf mongod --smallfiles -f /home/mongodb/conf/shard3.conf2 将分片配置为复制集连接mongo，只需在任意一台机器执行即可： mongo --host 192.168.30.30 --port 27001 //这里以shard1为例，其他两个分片则再需对应连接到27002、27003的端口进行操作即可 切换数据库： use admin 初始化复制集： rs.initiate({_id:\"shard1\",members:[{_id:0,host:\"192.168.30.30:27001\"},{_id:1,host:\"192.168.30.31:27001\"},{_id:2,host:\"192.168.30.32:27001\"}]}) 以上是基于分片1来操作，同理，其他2个分片也要连到各自的端口来执行一遍上述的操作，让3个分片各自形成1主2从的复制集，注意端口及仲裁节点的问题即可，操作完成后3个分片都启动完成，并完成复制集模式。 mongo --host 192.168.30.30 --port 27002 use admin rs.initiate({_id:\"shard2\",members:[{_id:0,host:\"192.168.30.30:27002\"},{_id:1,host:\"192.168.30.31:27002\"},{_id:2,host:\"192.168.30.32:27002\"}]}) mongo --host 192.168.30.30 --port 27003 use admin rs.initiate({_id:\"shard3\",members:[{_id:0,host:\"192.168.30.30:27003\"},{_id:1,host:\"192.168.30.31:27003\"},{_id:2,host:\"192.168.30.32:27003\"}]}) 路由服务部署(3台服务器执行相同操作) 创建mongos.conf 在/home/mongodb/conf目录创建mongos.conf，内容如下： [root@master conf]# cat mongos.conf logpath=/home/mongodb/log/mongos.log logappend = true port = 27017 fork = true configdb = configs/192.168.30.30:27018,192.168.30.31:27018,192.168.30.32:27018 maxConns=20000 bind_ip=0.0.0.0 scp mongos.conf root@192.168.30.31:/home/mongodb/conf scp mongos.conf root@192.168.30.32:/home/mongodb/conf 启动mongos 分别在三台服务器启动： mongos -f /home/mongodb/conf/mongos.conf 启动分片功能 连接mongo： mongo --host 10.211.55.3 --port 27017 切换数据库： use admin 添加分片，只需在一台机器执行即可： sh.addShard(\"shard1/192.168.30:27001,192.168.30.31:27001,192.168.30.32:27001\") sh.addShard(\"shard2/192.168.30:27001,192.168.30.31:27001,192.168.30.32:27001\") sh.addShard(\"shard3/192.168.30:27001,192.168.30.31:27001,192.168.30.32:27001\") 查看集群状态： sh.status() 实现分片功能 设置分片chunk大小 use config db.setting.save({\"_id\":\"chunksize\",\"value\":1}) # 设置块大小为1M是方便实验，不然需要插入海量数据 模拟写入数据 use calon for(i=1;i创建索引对表进行分片 db.user.createIndex({\"id\":1}) # 以\"id\"作为索引 sh.shardCollection(calon.user\",{\"id\":1}) # 根据\"id\"对user表进行分片 sh.status() # 查看分片情况 到此，MongoDB分布式集群就搭建完毕。 docker中运行mongodb 镜像下载 执行 docker search mongo 命令和docker pull mongo 运行mongo镜像 docker run \\ --name mongodb_server \\ -p 27017:27017 \\ -v /mysoft/mongodb/configdb:/data/configdb/ \\ -v /mysoft/mongodb/db/:/data/db/ \\ -d mongo --auth 采用admin用户进入mongodb docker exec -it a7e5d4e4ca69 mongo admin 创建admin管理员账户 db.createUser({ user: 'admin', pwd: 'admin123456', roles: [ { role: \"userAdminAnyDatabase\", db: \"admin\" } ] }); 以 admin 用户身份进入mongo docker exec -it a7e5d4e4ca69 mongo admin 对 admin 用户 进行身份认证 db.auth(\"admin\",\"admin123456\"); 创建 用户、密码和数据库 db.createUser({ user: 'swen', pwd: 'swen123456', roles: [ { role: \"readWrite\", db: \"app\" } ] }); 以 admin 用户身份进入mongo docker exec -it a7e5d4e4ca69 mongo admin 对 swen 进行身份认证 db.auth(\"swen\",\"swen123456\"); 切换数据库 use app 添加数据 db.test.save({name:\"zhangsan\"}); 查看数据库 show dbs 数据库集合（类似于表）操作命令 show collections db.createCollection(\"mycol\", { capped : true, autoIndexId : true, size : 6142800, max : 10000 } ) db.mycol2.insert({\"name\" : \"菜鸟教程\"}) db.mycol2.drop() 数据库文档（类似于一行一行数据）操作命令 db.col.insert({title: 'MongoDB 教程', description: 'MongoDB 是一个 Nosql 数据库', by: '菜鸟教程', url: 'http://www.runoob.com', tags: ['mongodb', 'database', 'NoSQL'], likes: 100 }) db.col.find() document=({title: 'MongoDB 教程', description: 'MongoDB 是一个 Nosql 数据库', by: '菜鸟教程', url: 'http://www.runoob.com', tags: ['mongodb', 'database', 'NoSQL'], likes: 100 }); db.col.insert(document) var document = db.collection.insertOne({\"a\": 3}) document { \"acknowledged\" : true, \"insertedId\" : ObjectId(\"571a218011a82a1d94c02333\") } db.col.update({'title':'MongoDB 教程'},{$set:{'title':'MongoDB'}}) db.col.find().pretty() db.col.save({ \"_id\" : ObjectId(\"56064f89ade2f21f36b03136\"), \"title\" : \"MongoDB\", \"description\" : \"MongoDB 是一个 Nosql 数据库\", \"by\" : \"Runoob\", \"url\" : \"http://www.runoob.com\", \"tags\" : [ \"mongodb\", \"NoSQL\" ], \"likes\" : 110 }) -- pretty() 方法以格式化的方式来显示所有文档。 db.col.remove({'title':'MongoDB 教程'}) db.col.remove(DELETION_CRITERIA,1) db.repairDatabase() db.inventory.deleteMany({ \"likes\" : 110 }) db.col.find({\"likes\": {$gt:50}, $or: [{\"by\": \"菜鸟教程\"},{\"title\": \"MongoDB 教程\"}]}).pretty() mongodb的导入导出 导出工具mongoexport Mongodb中的mongoexport工具可以把一个collection导出成JSON格式或CSV格式的文件。可以通过参数指定导出的数据项，也可以根据指定的条件导出数据。 可通过 mongoexport --help 命令查看具体使用方法 参数说明： -h:指明数据库宿主机的IP -u:指明数据库的用户名 -p:指明数据库的密码 -d:指明数据库的名字 -c:指明collection的名字 -f:指明要导出那些列 -o:指明到要导出的文件名 -q:指明导出数据的过滤条件 示例： 导出goods数据库下students集合的数据 执行图中命令 bin目录下生成students.dat文件，内容如下(也可在命令行中执行 type students.dat 查看) 从上面的结果可以看出，我们在导出数据时没有显示指定导出样式 ，默认导出了JSON格式的数据。实际情况下常常需要导出csv格式的数据，命令如下 mongoexport -d goods -c students --csv -f classid,name,age -o students_csv.dat 参数详解： -d:指明使用的库，本例中为goods -c:指明要导出的集合，本例中为students -o:指明要导出的文件名，本例中为students_csv.dat -csv：指明要导出为csv格式 -f：指明需要导出classid、name、age这3列的数据 导入工具mongoimport Mongodb中的mongoimport工具可以把一个特定格式文件中的内容导入到指定的collection中。该工具可以导入JSON格式数据，也可以导入CSV格式数据。 可通过 mongoimport --help 命令查看具体使用方法 参数说明： -h:指明数据库宿主机的IP -u:指明数据库的用户名 -p:指明数据库的密码 -d:指明数据库的名字 -c:指明collection的名字 -f:指明要导入那些列 示例 先删除students集合数据，验证 db.students.remove({}); db.students.find(); 导入之前导出的students.dat文件 上面演示的是导入JSON格式的文件中的内容，如果要导入CSV格式文件中的内容，则需要通过--type参数指定导入格式 mongoimport -d goods -c students --type csv --headerline --file students_csv.dat 参数详解 -d:指明数据库名，本例中为goods -c:指明collection名，本例中为students -type:指明要导入的文件格式 -headerline:指明第一行是列名，不需要导入 -file：指明要导入的文件 students_csv.dat：导入的文件名 © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"database/redis-devops-note.html":{"url":"database/redis-devops-note.html","title":"【datebase】redis运维笔记","keywords":"","body":"redis cluster安装 下载并解压 cd /root/software wget http://download.redis.io/releases/redis-3.2.4.tar.gz tar -zxvf redis-3.2.4.tar.gz 编译安装 cd redis-3.2.4 make && make install 将redis-trib.rb复制到/usr/local/bin目录下 cd src cp redis-trib.rb /usr/local/bin/ 创建 Redis 节点 首先在 192.168.31.245 机器上 /root/software/redis-3.2.4 目录下创建 redis_cluster 目录； mkdir redis_cluster 在 redis_cluster 目录下，创建名为7000、7001、7002的目录，并将 redis.conf 拷贝到这三个目录中 mkdir 7000 7001 7002 cp redis.conf redis_cluster/7000 cp redis.conf redis_cluster/7001 cp redis.conf redis_cluster/7002 分别修改这三个配置文件，修改如下内容 port 7000 //端口7000,7002,7003 bind 本机ip //默认ip为127.0.0.1 需要改为其他节点机器可访问的ip 否则创建集群时无法访问对应的端口，无法创建集群 daemonize yes //redis后台运行 pidfile /var/run/redis_7000.pid //pidfile文件对应7000,7001,7002 cluster-enabled yes //开启集群 把注释#去掉 cluster-config-file nodes_7000.conf //集群的配置 配置文件首次启动自动生成 7000,7001,7002 cluster-node-timeout 15000 //请求超时 默认15秒，可自行设置 appendonly yes //aof日志开启 有需要就开启，它会每次写操作都记录一条日志　 masterauth abc //注意:如果对集群设置密码,需做以下改动,否则不能设置密码。 requirepass abc //requirepass和masterauth都需要设置，并且每个节点的密码需要一致，否则发生主从切换时，就会遇到授权问题，可以模拟并观察日志 接着在另外一台机器上（192.168.31.210），的操作重复以上三步，只是把目录改为7003、7004、7005，对应的配置文件也按照这个规则修改即可 启动各个节点 第一台机器上执行 redis-server redis_cluster/7000/redis.conf redis-server redis_cluster/7001/redis.conf redis-server redis_cluster/7002/redis.conf 另外一台机器上执行 redis-server redis_cluster/7003/redis.conf redis-server redis_cluster/7004/redis.conf redis-server redis_cluster/7005/redis.conf 检查 redis 启动情况 ps -ef | grep redis root 61020 1 0 02:14 ? 00:00:01 redis-server 127.0.0.1:7000 [cluster] root 61024 1 0 02:14 ? 00:00:01 redis-server 127.0.0.1:7001 [cluster] root 61029 1 0 02:14 ? 00:00:01 redis-server 127.0.0.1:7002 [cluster] netstat -tnlp | grep redis tcp 0 0 127.0.0.1:17000 0.0.0.0:* LISTEN 61020/redis-server tcp 0 0 127.0.0.1:17001 0.0.0.0:* LISTEN 61024/redis-server tcp 0 0 127.0.0.1:17002 0.0.0.0:* LISTEN 61029/redis-server tcp 0 0 127.0.0.1:7000 0.0.0.0:* LISTEN 61020/redis-server tcp 0 0 127.0.0.1:7001 0.0.0.0:* LISTEN 61024/redis-server tcp 0 0 127.0.0.1:7002 0.0.0.0:* LISTEN 61029/redis-server ##另外一台机器 ps -ef | grep redis root 9957 1 0 02:32 ? 00:00:01 redis-server 127.0.0.1:7003 [cluster] root 9964 1 0 02:32 ? 00:00:01 redis-server 127.0.0.1:7004 [cluster] root 9971 1 0 02:32 ? 00:00:01 redis-server 127.0.0.1:7005 [cluster] root 10065 4744 0 02:38 pts/0 00:00:00 grep --color=auto redis netstat -tlnp | grep redis tcp 0 0 127.0.0.1:17003 0.0.0.0:* LISTEN 9957/redis-server 1 tcp 0 0 127.0.0.1:17004 0.0.0.0:* LISTEN 9964/redis-server 1 tcp 0 0 127.0.0.1:17005 0.0.0.0:* LISTEN 9971/redis-server 1 tcp 0 0 127.0.0.1:7003 0.0.0.0:* LISTEN 9957/redis-server 1 tcp 0 0 127.0.0.1:7004 0.0.0.0:* LISTEN 9964/redis-server 1 tcp 0 0 127.0.0.1:7005 0.0.0.0:* LISTEN 9971/redis-server 1 创建集群 Redis 官方提供了 redis-trib.rb 这个工具，就在解压目录的 src 目录中，第三步中已将它复制到 /usr/local/bin 目录中，可以直接在命令行中使用了。使用下面这个命令即可完成安装。 redis-trib.rb create --replicas 1 192.168.31.245:7000 192.168.31.245:7001 192.168.31.245:7002 192.168.31.210:7003 192.168.31.210:7004 192.168.31.210:7005 其中，前三个 ip:port 为第一台机器的节点，剩下三个为第二台机器。 如果运行报下面错误， 是因为这个工具是用 ruby 实现的，所以需要安装 ruby。安装命令如下： yum -y install ruby ruby-devel rubygems rpm-build gem install redis 之后再运行 redis-trib.rb 命令，会出现如下提示： 输入 yes 即可，然后出现如下内容，说明安装成功。 　　 集群验证 在第一台机器上连接集群的7002端口的节点，在另外一台连接7005节点，连接方式为 redis-cli -h 192.168.31.245 -c -p 7002 ,加参数 -C 可连接到集群，因为上面 redis.conf 将 bind 改为了ip地址，所以 -h 参数不可以省略。 在7005节点执行命令 set hello world ，执行结果如下： 然后在另外一台7002端口，查看 key 为 hello 的内容， get hello ，执行结果如下： 说明集群运作正常。 简单说一下原理 redis cluster在设计的时候，就考虑到了去中心化，去中间件，也就是说，集群中的每个节点都是平等的关系，都是对等的，每个节点都保存各自的数据和整个集群的状态。每个节点都和其他所有节点连接，而且这些连接保持活跃，这样就保证了我们只需要连接集群中的任意一个节点，就可以获取到其他节点的数据。 Redis 集群没有并使用传统的一致性哈希来分配数据，而是采用另外一种叫做哈希槽 (hash slot)的方式来分配的。redis cluster 默认分配了 16384 个slot，当我们set一个key 时，会用CRC16算法来取模得到所属的slot，然后将这个key 分到哈希槽区间的节点上，具体算法就是：CRC16(key) % 16384。所以我们在测试的时候看到set 和 get 的时候，直接跳转到了7000端口的节点。 Redis 集群会把数据存在一个 master 节点，然后在这个 master 和其对应的salve 之间进行数据同步。当读取数据时，也根据一致性哈希算法到对应的 master 节点获取数据。只有当一个master 挂掉之后，才会启动一个对应的 salve 节点，充当 master 。 需要注意的是：必须要3个或以上的主节点，否则在创建集群时会失败，并且当存活的主节点数小于总节点数的一半时，整个集群就无法提供服务了。 Docker搭建Redis集群 集群规划 ip 节点名称 角色 192.168.30.30 master sentinel 192.168.30.31 slave1 sentinel 192.168.30.32 slave2 sentinel redis集群搭建 在30上创建目录 /root/redis/conf /root/redis/data /root/redis/log 用来存放容器中的配置文件，持久化数据和日志 修改master节点redis.conf,从网上下载一份拷贝到/root/redis/conf修改，修改如下： A.注释掉IP绑定 #bind 127.0.0.1 B.端口设置为 port 6379 C.pidfile 为 pidfile /var/run/redis_6379.pid D.logfile \"/log/redis.log\" E.保护模式为no:protected-mode no Slave1修改如下 A.注释掉IP绑定 #bind 127.0.0.1 B.端口设置为 port 6380 C.pidfile 为 pidfile /var/run/redis_6380.pid D.logfile \"/log/redis.log\" E.保护模式为no:protected-mode no F.设置主服务器 replicaof 192.168.30.30 6379 Slave2修改如下 A.注释掉IP绑定 #bind 127.0.0.1 B.端口设置为 port 6381 C.pidfile 为 pidfile /var/run/redis_6381.pid D.logfile \"/log/redis.log\" E.保护模式为no:protected-mode no F.设置主服务器 replicaof 192.168.30.30 6379 在三台机器上分别执行容器运行命令 docker run -p 6379:6379 -v /root/redis/conf/redis.conf:/usr/local/etc/redis/redis.conf -v /root/redis/data:/data -v /root/redis/log:/log -d --name redis-master --restart=always --network=host --privileged=true 106.54.126.251:5000/redis:5.0.4 redis-server /usr/local/etc/redis/redis.conf --appendonly yes docker run -p 6380:6380 -v /root/redis/conf/redis.conf:/usr/local/etc/redis/redis.conf -v /root/redis/data:/data -v /root/redis/log:/log -d --name redis-slave1 --restart=always --network=host --privileged=true 106.54.126.251:5000/redis:5.0.4 redis-server /usr/local/etc/redis/redis.conf --appendonly yes docker run -p 6381:6381 -v /root/redis/conf/redis.conf:/usr/local/etc/redis/redis.conf -v /root/redis/data:/data -v /root/redis/log:/log -d --name redis-slave2 --restart=always --network=host --privileged=true 106.54.126.251:5000/redis:5.0.4 redis-server /usr/local/etc/redis/redis.conf --appendonly yes sentinel集群搭建 修改master节点sentinel.conf,从网上下载一份拷贝到/root/redis/conf修改，修改如下： A.注释掉IP绑定 #bind 127.0.0.1 B.端口设置为 port 26379 C.修改监控主机ip：sentinel monitor mymaster 192.168.30.30 6379 2 D.logfile \"/log/sentinel.log\" E.注释保护模式为:protected-mode no Slave1修改如下 A.注释掉IP绑定 #bind 127.0.0.1 B.端口设置为 port 26380 C.修改监控主机ip：sentinel monitor mymaster 192.168.30.30 6379 2 D.logfile \"/log/sentinel.log\" E.注释保护模式为:protected-mode no Slave2修改如下 A.注释掉IP绑定 #bind 127.0.0.1 B.端口设置为 port 26381 C.修改监控主机ip：sentinel monitor mymaster 192.168.30.30 6379 2 D.logfile \"/log/sentinel.log\" E.注释保护模式为:protected-mode no 在三台机器上分别执行容器运行命令 docker run -it --name sentinel-01 -p 26379:26379 -v /root/redis/conf/sentinel.conf:/usr/local/etc/redis/sentinel.conf -v /root/redis/log:/log -d --restart=always --network=host --privileged 106.54.126.251:5000/redis:5.0.4 redis-sentinel /usr/local/etc/redis/sentinel.conf docker run -it --name sentinel-02 -p 26380:26380 -v /root/redis/conf/sentinel.conf:/usr/local/etc/redis/sentinel.conf -v /root/redis/log:/log -d --restart=always --network=host --privileged 106.54.126.251:5000/redis:5.0.4 redis-sentinel /usr/local/etc/redis/sentinel.conf docker run -it --name sentinel-03 -p 26381:26381 -v /root/redis/conf/sentinel.conf:/usr/local/etc/redis/sentinel.conf -v /root/redis/log:/log -d --restart=always --network=host --privileged 106.54.126.251:5000/redis:5.0.4 redis-sentinel /usr/local/etc/redis/sentinel.conf 到此，集群搭建完毕，可以停掉master,看会不会更换master节点 Redis日志的格式 In the log files the various log levels are represented as follows: . debug - verbose * notice # warning The log output for Redis 2.x will look something like this: [pid] date loglevel message For instance: [4018] 14 Nov 07:01:22.119 * Background saving terminated with success The possible values for role are as follows: X sentinel C RDB/AOF writing child S slave M master © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"database/elasticsearch-devops-note.html":{"url":"database/elasticsearch-devops-note.html","title":"【datebase】elasticsearch运维笔记","keywords":"","body":"docker-compose搭建Elasticsearch集群 在主节点创建一个目录es,并创建docker-compose.yaml，主节点yaml文件如下 version: '2' services: elasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch:6.6.2 privileged: true environment: - cluster.name=docker-cluster - xpack.security.enabled=false - bootstrap.memory_lock=true - node.master=true - node.data=true # store data on master - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" #根据机器实际来分配 - \"discovery.zen.ping.unicast.hosts=192.168.30.30\" # put your master_ip here! make sure `telnet MASTER_IP 9300` is oK - \"transport.host=0.0.0.0\" - \"network.host=0.0.0.0\" - node.name=esmaster ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 cap_add: - IPC_LOCK volumes: - /data/elasticsearch-service/data:/usr/share/elasticsearch/data ports: - 9208:9200 - 9308:9300 network_mode: \"host\" 执行命令docker-compose up -d启动es，一般会报错max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]，可以通过sudo sysctl -w vm.max_map_count=262144设置下值。 在node节点上同样操作创建es目录，创建docker-compose.yaml，node节点yaml文件如下 version: '2.2' services: elasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch:6.6.2 privileged: true environment: - cluster.name=docker-cluster - xpack.security.enabled=false - bootstrap.memory_lock=true - node.master=false - node.data=true - \"ES_JAVA_OPTS=-Xms12g -Xmx12g\" - \"discovery.zen.ping.unicast.hosts=192.168.30.30\" # put your master_ip here! make sure `telnet MASTER_IP 9300` is oK ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 cap_add: - IPC_LOCK volumes: - /data/elasticsearch-service/data:/usr/share/elasticsearch/data ports: - 9208:9200 - 9308:9300 network_mode: \"host\" 执行命令docker-compose up -d启动es，一般会报错max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]，可以通过sudo sysctl -w vm.max_map_count=262144设置下值 helm部署es集群到k8s上 # helm repo add elastic https://helm.elastic.co # helm install --name elasticsearch elastic/elasticsearch elasticsearch命令操作大全 命令格式 curl -X :/// ：REST风格的语法谓词 :节点ip :节点端口号，默认9200 :索引名 :索引类型 :操作对象的ID号 访问带鉴权的es curl -u 'elastic:Je2pW2SOa' http://192.168.30.32:9200/ 查看es信息 curl http://192.168.30.32:9200/ 查看集群状态 curl http://192.168.30.32:9200/_cat/nodes 查看所有索引 curl http://192.168.30.32:9200/_cat/indices 创建索引nwx curl -XPUT '192.168.30.32:9200/nwx?pretty' 查看customer索引状态: curl -XGET http://192.168.30.32:9200/_cat/indices/customer/?pretty 往索引customer中插入数据，类型为external，id为1 curl -XPUT '192.168.30.32:9200/customer/external/1?pretty' -d ' { \"name\": \"nie wei xing\" }' 往索引customer中插入数据，类型为aaa，id为1 curl -XPUT '192.168.30.32:9200/customer/aaa/1?pretty' -d ' { \"name\": \"nie wei xing\" }' 往索引customer中批量插入数据 curl -XPOST '192.168.30.32:9200/customer/external/_bulk?pretty' -d ' {\"index\":{\"_id\":\"3\"}} {\"name\": \"John Doe\" } {\"index\":{\"_id\":\"4\"}} {\"name\": \"Jane Doe\" } ' 导入数据集 curl -H 'Content-Type: application/x-ndjson' -XPOST '192.168.30.32:9200/bank/account/_bulk?pretty' --data-binary @accounts.json curl -H 'Content-Type: application/x-ndjson' -XPOST '192.168.30.32:9200/shakespeare/doc/_bulk?pretty' --data-binary @shakespeare_6.0.json curl -H 'Content-Type: application/x-ndjson' -XPOST '192.168.30.32:9200/_bulk?pretty' --data-binary @logs.jsonl 查询插入的数据 curl -XGET '192.168.30.32:9200/customer/external/1?pretty' curl -XGET '192.168.30.32:9200/customer/aaa/1?pretty' 查询某个索引的所有数据 curl '192.168.30.32:9200/bank/_search?q=*&pretty' curl '192.168.30.32:9200/customer/_search?q=*&pretty' curl '192.168.30.32:9200/shakespeare/_search?q=*&pretty' curl -XPOST '192.168.30.32:9200/bank/_search?pretty' -d ' { \"query\": { \"match_all\": {} } }' 匹配所有数据，但只返回1个 curl -XPOST '192.168.30.32:9200/bank/_search?pretty' -d ' { \"query\": { \"match_all\": {} }, \"size\": 1 }' 注意：如果siez不指定，则默认返回10条数据。 返回从11到20的数据。（索引下标从0开始） curl -XPOST '192.168.30.32:9200/bank/_search?pretty' -d ' { \"query\": { \"match_all\": {} }, \"from\" : 10, \"size\" : 10 }' 匹配所有数据返回前10条 匹配所有的索引中的数据，按照balance字段降序排序，并且返回前10条 curl -XPOST '192.168.30.32:9200/bank/_search?pretty' -d ' { \"query\": { \"match_all\": {} }, \"sort\": { \"balance\": { \"order\": \"desc\" } } }' 下面例子展示如何返回两个字段（account_number balance） curl -XPOST '192.168.30.32:9200/bank/_search?pretty' -d ' { \"query\": { \"match_all\": {} }, \"_source\": [\"account_number\", \"balance\"] }' 返回account_number 为20 的数据 curl -XPOST '192.168.30.32:9200/bank/_search?pretty' -d ' { 　\"query\": { \"match\": { \"account_number\": 20 } } }' 返回address中包含mill的所有数据 curl -XPOST '192.168.30.32:9200/bank/_search?pretty' -d ' { 　\"query\": { \"match\": { \"address\": \"mill\" } } }' 　 返回地址中包含mill或者lane的所有数据 curl -XPOST '192.168.30.32:9200/bank/_search?pretty' -d ' { \"query\": { \"match\": { \"address\": \"mill lane\" } } }' 这个例子是多匹配（match_phrase短语匹配） 返回地址中包含短语 “mill lane”的所有数据： curl -XPOST '192.168.30.32:9200/bank/_search?pretty' -d ' { \"query\": { \"match_phrase\": { \"address\": \"mill lane\" } } }' must布尔查询 布尔查询允许我们将多个简单的查询组合成一个更复杂的布尔逻辑查询。 这个例子将两个查询组合，返回地址中含有mill和lane的所有记录数据： curl -XPOST '192.168.30.32:9200/bank/_search?pretty' -d ' { \"query\": { \"bool\": { \"must\": [ { \"match\": { \"address\": \"mill\" } }, { \"match\": { \"address\": \"lane\" } } ] } } }' Should布尔查询 上述例子中，must表示所有查询必须都为真才被认为匹配。 相反, 这个例子组合两个查询，返回地址中含有mill或者lane的所有记录数据： curl -XPOST '192.168.30.32:9200/bank/_search?pretty' -d ' { \"query\": { \"bool\": { \"should\": [ { \"match\": { \"address\": \"mill\" } }, { \"match\": { \"address\": \"lane\" } } ] } } }' must_not布尔查询 curl -XPOST '192.168.30.32:9200/bank/_search?pretty' -d ' { \"query\": { \"bool\": { \"must_not\": [ { \"match\": { \"address\": \"mill\" } }, { \"match\": { \"address\": \"lane\" } } ] } } }' 多级逻辑查询 上述例子中,must_not表示查询列表中没有为真的（也就是全为假）时则认为匹配。 我们可以组合must、should、must_not来实现更加复杂的多级逻辑查询。 下面这个例子返回年龄大于40岁、不居住在ID的所有数据： curl -XPOST '192.168.30.32:9200/bank/_search?pretty' -d ' { \"query\": { \"bool\": { \"must\": [ { \"match\": { \"age\": \"40\" } } ], \"must_not\": [ { \"match\": { \"state\": \"ID\" } } ] } } }' 过滤filter(查询条件设置) 下面这个例子使用了布尔查询返回balance在20000到30000之间的所有数据。 curl -XPOST '192.168.30.32:9200/bank/_search?pretty' -d ' { \"query\": { \"bool\": { \"must\": { \"match_all\": {} }, \"filter\": { \"range\": { \"balance\": { \"gte\": 20000, \"lte\": 30000 } } } } } }' 聚合 Aggregations 下面这个例子： 将所有的数据按照state分组（group），然后按照分组记录数从大到小排序，返回前十条（默认）： curl -XPOST '192.168.30.32:9200/bank/_search?pretty' -d ' { \"size\": 0, \"aggs\": { \"group_by_state\": { \"terms\": { \"field\": \"state.keyword\" } } } }' 下面这个实例按照state分组，降序排序，返回balance的平均值： curl -XPOST '192.168.30.32:9200/bank/_search?pretty' -d ' { \"size\": 0, \"aggs\": { \"group_by_state\": { \"terms\": { \"field\": \"state.keyword\" }, \"aggs\": { \"average_balance\": { \"avg\": { \"field\": \"balance.keyword\" } } } } } }' 带条件查询 curl -XPOST '192.168.30.32:9200/bank/_search?pretty' -d ' { \"query\": { \"match\": { \"account_number\": 20 } } }' 索引数据的修改 curl -XPUT '192.168.30.32:9200/customer/external/1?pretty' -d ' { \"name\": \"Jane Doe\" }' 索引数据的更新 curl -XPOST '192.168.30.32:9200/customer/external/1/_update?pretty' -d ' { \"doc\": {\"name\": \"Jane Doe1\"} }' 索引数据的删除 curl -XDELETE '192.168.30.32:9200/customer/external/2?pretty' 索引的删除 curl -XDELETE '192.168.30.32:9200/nwx?pretty' 更新id为1的内容并删除id为2的 curl -XPOST '192.168.30.32:9200/customer/external/_bulk?pretty' -d ' {\"update\":{\"_id\":\"1\"}} {\"doc\": { \"name\": \"John Doe becomes Jane Doe\" } } {\"delete\":{\"_id\":\"2\"}}' 设置全局的慢日志级别 curl -XPUT '192.168.30.32:9200/_cluster/settings/?pretty' -H 'Content-Type: application/json' -d '{ \"transient\": { \"logger.index.search.slowlog\":\"DEBUG\", \"logger.index.indexing.slowlog\":\"DEBUG\" } }' 设置customer索引的慢日志超时时间 curl -XPUT '192.168.30.32:9200/customer/_settings/?pretty' -H 'Content-Type: application/json' -d '{ \"index.search.slowlog.threshold.query.debug\" : \"1ms\", \"index.search.slowlog.threshold.fetch.debug\": \"1ms\", \"index.indexing.slowlog.threshold.index.debug\": \"1ms\" }' curl -XPUT '192.168.30.32:9200/people/_settings/?pretty' -H 'Content-Type: application/json' -d '{ \"index.search.slowlog.threshold.query.debug\" : \"1ms\", \"index.search.slowlog.threshold.fetch.debug\": \"1ms\", \"index.indexing.slowlog.threshold.index.debug\": \"1ms\" }' curl -XPUT '192.168.30.32:9200/bank/_settings/?pretty' -H 'Content-Type: application/json' -d '{ \"index.search.slowlog.threshold.query.debug\" : \"1ms\", \"index.search.slowlog.threshold.fetch.debug\": \"1ms\", \"index.indexing.slowlog.threshold.index.debug\": \"1ms\" }' © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"docker/dockerfile-study.html":{"url":"docker/dockerfile-study.html","title":"【docker】dockerfile学习笔记","keywords":"","body":"DockerFile的例子 FROM centos MAINTAINER nobody \"xx@qq.com\" RUN mkdir -p /opt/jdk/ RUN mkdir -p /opt/tomcat/ ADD jdk1.7.0_79 /opt/jdk/ ADD tomcat /opt/tomcat/ ENV CATALINA_HOME /opt/tomcat ENV JAVA_HOME /opt/jdk EXPOSE 8080 VOLUME /opt/tomcat/data ENV PATH $PATH:$JAVA_HOME/bin CMD [\"/opt/tomcat/bin/catalina.sh\",\"run\"] DockerFile的语法解析 Dockerfile的基本指令有十三个，分别是：FROM、MAINTAINER、RUN、CMD、EXPOSE、ENV、ADD、COPY、ENTRYPOINT、VOLUME、USER、WORKDIR、ONBUILD。下面对这些指令的用法一一说明。 FROM 用法：FROM 说明：第一个指令必须是FROM了，其指定一个构建镜像的基础源镜像，如果本地没有就会从公共库中拉取，没有指定镜像的标签会使用默认的latest标签，可以出现多次，如果需要在一个Dockerfile中构建多个镜像。 MAINTAINER 用法：MAINTAINER 说明：描述镜像的创建者，名称和邮箱 RUN 用法：RUN \"command\" \"param1\" \"param2\" 说明：RUN命令是一个常用的命令，RUN命令可以执行多次，执行完成之后会成为一个新的镜像，这里也是指镜像的分层构建。一句RUN就是一层，也相当于一个版本。这就是之前说的缓存的原理。我们知道docker是镜像层是只读的，所以你如果第一句安装了软件，用完在后面一句删除是不可能的。所以这种情况要在一句RUN命令中完成，可以通过&符号连接多个RUN语句。RUN后面的必须是双引号不能是单引号（没引号貌似也不要紧），command是不会调用shell的，所以也不会继承相应变量，要查看输入RUN \"sh\" \"-c\" \"echo\" \"$HOME\"，而不是RUN \"echo\" \"$HOME\"。 CMD 用法：CMD command param1 param2 说明：CMD在Dockerfile中只能出现一次，有多个，只有最后一个会有效。其作用是在启动容器的时候提供一个默认的命令项。如果用户执行docker run的时候提供了命令项，就会覆盖掉这个命令。没提供就会使用构建时的命令。 EXPOSE 用法：EXPOSE [...] 说明：告诉Docker服务器容器对外映射的容器端口号，在docker run -p的时候生效。 ENV 用法：EVN 只能设置一个 　　　EVN =允许一次设置多个 说明：设置容器的环境变量，可以让其后面的RUN命令使用，容器运行的时候这个变量也会保留。 ADD 用法：ADD 说明：复制本机文件或目录或远程文件，添加到指定的容器目录，支持GO的正则模糊匹配。路径是绝对路径，不存在会自动创建。如果源是一个目录，只会复制目录下的内容，目录本身不会复制。ADD命令会将复制的压缩文件夹自动解压，这也是与COPY命令最大的不同。 COPY 用法：COPY 说明：COPY除了不能自动解压，也不能复制网络文件。其它功能和ADD相同。 ENTRYPOINT(推荐使用这个作为启动命令) 用法：ENTRYPOINT \"command\" \"param1\" \"param2\" 说明：这个命令和CMD命令一样，唯一的区别是不能被docker run命令的执行命令覆盖，如果要覆盖需要带上选项--entrypoint，如果有多个选项，只有最后一个会生效。 VOLUME 用法：VOLUME [\"path\"] 说明：在主机上创建一个挂载，挂载到容器的指定路径。docker run -v命令也能完成这个操作，而且更强大。这个命令不能指定宿主机的需要挂载到容器的文件夹路径。但docker run -v可以，而且其还可以挂载数据容器。 USER 用法：USER daemon 说明：指定运行容器时的用户名或UID，后续的RUN、CMD、ENTRYPOINT也会使用指定的用户运行命令。 WORKDIR 用法:WORKDIR path 说明：为RUN、CMD、ENTRYPOINT指令配置工作目录。可以使用多个WORKDIR指令，后续参数如果是相对路径，则会基于之前的命令指定的路径。如：WORKDIR /home　　WORKDIR test 。最终的路径就是/home/test。path路径也可以是环境变量，比如有环境变量HOME=/home，WORKDIR $HOME/test也就是/home/test。 ONBUILD 用法：ONBUILD [INSTRUCTION] 说明：配置当前所创建的镜像作为其它新创建镜像的基础镜像时，所执行的操作指令。意思就是，这个镜像创建后，如果其它镜像以这个镜像为基础，会先执行这个镜像的ONBUILD命令。 ARG 语法：ARG [=] 设置变量命令，ARG命令定义了一个变量，在docker build创建镜像的时候，使用 --build-arg =来指定参数 如果用户在build镜像时指定了一个参数没有定义在Dockerfile种，那么将有一个Warning © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"docker/docker-compose-study.html":{"url":"docker/docker-compose-study.html","title":"【docker】docker-compose学习笔记","keywords":"","body":"Docker Compose 是 Docker 官方编排（Orchestration）项目之一，负责快速在集群中部署分布式应用。 安装方式 二进制安装 $ sudo curl -L https://github.com/docker/compose/releases/download/1.17.1/docker-compose-`uname -s`-`uname -m` > /usr/local/bin/docker-compose $ sudo chmod +x /usr/local/bin/docker-compose bash 补全命令 $ curl -L https://raw.githubusercontent.com/docker/compose/1.8.0/contrib/completion/bash/docker-compose > /etc/bash_completion.d/docker-compose 删除docker-compose sudo rm /usr/local/bin/docker-compose pip方式安装 安装python-pip yum -y install python-pip 安装docker-compose pip install docker-compose 待安装完成后，执行查询版本的命令，即可安装docker-compose docker-compose version docker-compose.yml文件的编写 version: '3' services: web: build: . ports: - \"5000:5000\" redis: image: \"redis:alpine\" 注意每个服务都必须通过 image 指令指定镜像或 build 指令（需要 Dockerfile）等来自动构建生成镜像。 如果使用build指令，在Dockerfile中设置的选项(例如： CMD , EXPOSE , VOLUME , ENV等)将会自动被获取，无需在docker-compose.yml中再次设置。 build 指定 Dockerfile 所在文件夹的路径（可以是绝对路径，或者相对 docker-compose.yml 文件的路径）。Compose将会利用它自动构建这个镜像，然后使用这个镜像 其他具体参数可以参考官方文档Docker Compose的Compose 模板文件部分 docker-compose命令 Options: -f, --file FILE Specify an alternate compose file (default: docker-compose.yml) -p, --project-name NAME Specify an alternate project name (default: directory name) --verbose Show more output --log-level LEVEL Set log level (DEBUG, INFO, WARNING, ERROR, CRITICAL) --no-ansi Do not print ANSI control characters -v, --version Print version and exit -H, --host HOST Daemon socket to connect to --tls Use TLS; implied by --tlsverify --tlscacert CA_PATH Trust certs signed only by this CA --tlscert CLIENT_CERT_PATH Path to TLS certificate file --tlskey TLS_KEY_PATH Path to TLS key file --tlsverify Use TLS and verify the remote --skip-hostname-check Don't check the daemon's hostname against the name specified in the client certificate --project-directory PATH Specify an alternate working directory (default: the path of the Compose file) --compatibility If set, Compose will attempt to convert deploy keys in v3 files to their non-Swarm equivalent Commands: build Build or rebuild services bundle Generate a Docker bundle from the Compose file config Validate and view the Compose file create Create services down Stop and remove containers, networks, images, and volumes events Receive real time events from containers exec Execute a command in a running container help Get help on a command images List images kill Kill containers logs View output from containers pause Pause services port Print the public port for a port binding ps List containers pull Pull service images push Push service images restart Restart services rm Remove stopped containers run Run a one-off command scale Set number of containers for a service start Start services stop Stop services top Display the running processes unpause Unpause services up Create and start containers version Show the Docker-Compose version information © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"docker/Installation-and-use-of-containerd.html":{"url":"docker/Installation-and-use-of-containerd.html","title":"【containerd】containerd的安装和使用","keywords":"","body":"containerd的安装和使用 Containerd是一个工业标准的容器运行时，重点是它简洁，健壮，便携，在Linux和window上可以作为一个守护进程运行，它可以管理主机系统上容器的完整的生命周期：镜像传输和存储，容器的执行和监控，低级别的存储和网络。 containerd和docker不同，containerd重点是继承在大规模的系统中，例如kubernetes，而不是面向开发者，让开发者使用，更多的是容器运行时的概念，承载容器运行。 containerd的架构图如下： 安装containerd 安装containerd这里写了一个脚本来快速部署，脚本会部署containerd和crictl命令行工具到机器上，crictl是CRI兼容的容器运行时命令行接口，可以用来操作containerd的镜像和容器等。 运行下面脚本需要填写2个参数，第一个是crictl的版本，第二个参数填写containerd版本，注意传入的版本号需要去掉开头的v。 crictl版本号获取地址: https://github.com/kubernetes-sigs/cri-tools/tags containerd版本号获取地址: https://github.com/containerd/containerd/tags crictl 默认连接到 unix:///var/run/dockershim.sock。 对于其他的运行时，你可以用多种不同的方法设置端点： 通过设置参数 --runtime-endpoint 和 --image-endpoint 通过设置环境变量 CONTAINER_RUNTIME_ENDPOINT 和 IMAGE_SERVICE_ENDPOINT 通过在配置文件中设置端点 --config=/etc/crictl.yaml 你还可以在连接到服务器并启用或禁用调试时指定超时值，方法是在配置文件中指定 timeout 或 debug 值，或者使用 --timeout 和 --debug 命令行参数。 install-containerd.sh内容如下 #!/bin/bash crictl_version=$1 contained_version=$2 if [ $# = 0 ];then echo \"Run 'sh install-containerd.sh --h' for more information on a command.\" fi if [[ $1 = \"--h\" ]];then echo \"Please enter the first parameter enters the crictl version, the second parameter enters the containerd version Usage: sh install-containerd.sh [crictl_version] [contained_version]\" fi main(){ # download crictl client wget https://github.com/kubernetes-sigs/cri-tools/releases/download/v${crictl_version}/crictl-v${crictl_version}-linux-amd64.tar.gz tar -C /usr/local/bin -xf crictl-v${crictl_version}-linux-amd64.tar.gz rm -rf crictl-v${crictl_version}-linux-amd64.tar.gz # download containerd pkg wget https://github.com/containerd/containerd/releases/download/v${contained_version}/containerd-${contained_version}-linux-amd64.tar.gz tar -C /usr/local -xf containerd-${contained_version}-linux-amd64.tar.gz rm -rf containerd-${contained_version}-linux-amd64.tar.gz #crictl start config cat /etc/crictl.yaml runtime-endpoint: unix:///run/containerd/containerd.sock image-endpoint: unix:///run/containerd/containerd.sock timeout: 10 debug: false EOF # create contained config mkdir -p /etc/containerd cd /usr/local/bin/ ./containerd config default > /etc/containerd/config.toml # systemd manager containerd cat /lib/systemd/system/containerd.service [Unit] Description=containerd container runtime Documentation=https://containerd.io After=network.target [Service] ExecStartPre=/sbin/modprobe overlay ExecStart=/usr/local/bin/containerd Delegate=yes KillMode=process LimitNOFILE=1048576 # Having non-zero Limit*s causes performance problems due to accounting overhead # in the kernel. We recommend using cgroups to do container-local accounting. LimitNPROC=infinity LimitCORE=infinity [Install] WantedBy=multi-user.target EOF # Start containerd and set it to start automatically sudo systemctl daemon-reload sudo systemctl enable containerd.service sudo systemctl start containerd.service sudo systemctl status containerd.service } if [ $# = 2 ];then main fi 检查containerd的运行状态，如果提示running则说明安装成功 [root@VM-0-13-centos ~]# systemctl status containerd.service ● containerd.service - containerd container runtime Loaded: loaded (/usr/lib/systemd/system/containerd.service; enabled; vendor preset: disabled) Active: active (running) since Thu 2021-08-19 09:46:45 CST; 2 weeks 0 days ago Docs: https://containerd.io Main PID: 828192 (containerd) CGroup: /system.slice/containerd.service ................... crictl常用的命令 [root@VM-0-13-centos ~]# crictl -h NAME: crictl - client for CRI USAGE: crictl [global options] command [command options] [arguments...] VERSION: v1.22.0 COMMANDS: attach Attach to a running container create Create a new container exec Run a command in a running container version Display runtime version information images, image, img List images inspect Display the status of one or more containers inspecti Return the status of one or more images imagefsinfo Return image filesystem info inspectp Display the status of one or more pods logs Fetch the logs of a container port-forward Forward local port to a pod ps List containers pull Pull an image from a registry run Run a new container inside a sandbox runp Run a new pod rm Remove one or more containers rmi Remove one or more images rmp Remove one or more pods pods List pods start Start one or more created containers info Display information of the container runtime stop Stop one or more running containers stopp Stop one or more running pods update Update one or more running containers config Get and set crictl client configuration options stats List container(s) resource usage statistics completion Output shell completion code help, h Shows a list of commands or help for one command crictl是没有构建命令的，如果想构建镜像可以用docker或者用buildah工具，buildah可以参考文档https://github.com/containers/buildah containerd也有自带客户端工具，叫ctr，执行ctr命令时需要带上--namespace http://k8s.io，建议还是安装crictl工具操作。 crictl配置私有镜像仓库 crictl是没有login命令的，如果需要拉取私有镜像仓库的镜像，需要在containerd的配置文件/etc/containerd/config.toml中配置私有镜像仓库的登录信息 [plugins] [plugins.cri.registry.mirrors.\"ccr.ccs.tencentyun.com\"] endpoint = [\"https://ccr.ccs.tencentyun.com\"] [plugins.cri.registry.configs.\"ccr.ccs.tencentyun.com\".auth] username = \"xxxxx\" password = \"xxxxx\" 配置好之后在重启containerd即可拉取私有镜像 # systemctl restart containerd.service # crictl pull ccr.ccs.tencentyun.com/xxx/xxx:v1 配置镜像加速 有的时候为了能够加速镜像的拉取，需要配置镜像加速的代理，可以在/etc/containerd/config.toml配置镜像加速 [plugins] [plugins.cri.registry.mirrors.\"docker.io\"] endpoint = [\"https://mirror.ccs.tencentyun.com\",\"https://xxxx.mirror.aliyuncs.com\"] 配置好之后在重启containerd即可生效 # systemctl restart containerd.service © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"docker/Docker-limits-the-disk-space-that-containers-can-occupy.html":{"url":"docker/Docker-limits-the-disk-space-that-containers-can-occupy.html","title":"【docker】docker限制容器可占用的磁盘空间","keywords":"","body":"docker限制容器可占用的磁盘空间 Docker容器默认启动的虚拟机，会占用宿主机的资源（CPU、内存、硬盘），例如默认Docker基于Overlay2驱动方式，容器硬盘的rootfs根分区空间是整个宿主机的空间大小。 可以指定默认容器的大小（在启动容器的时候指定），可以在docker配置文件指定Docker容器rootfs容量大小。 如果这里需要设置容器可用磁盘空间大小，需要保证节点的文件系统是xfs 具体配置如下，修改/etc/docker/daemon.json文件，设置容器的磁盘空间大小为20G。 { \"data-root\": \"/data/docker\", \"storage-driver\": \"overlay2\", \"storage-opts\": [ \"overlay2.override_kernel_check=true\", \"overlay2.size=20G\" ] } © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"docker/Getting-started-with-buildkit.html":{"url":"docker/Getting-started-with-buildkit.html","title":"【docker】buildkit入门使用","keywords":"","body":"buildkit入门使用 最近因为K8s抛弃Docker了,所以就只装了个containerd,这样就需要一个单独的镜像构建工具了,就用了buildkit,这也是Docker公司扶持的,他们公司的人出来搞的开源工具,官网在 https://github.com/moby/buildkit 架构 服务端为buildkitd,负责和runc或containerd后端连接干活,目前只支持这两个后端 客户端为buildctl,负责解析镜像构建文件Dockerfile,并向服务端发出构建指令,所以客户端可以和服务端不在一台机器上,也不需要root权限之类 服务端默认使用runc后端,但是建议使用containerd后端,这样构建出的镜像就会存在containerd的buildkit名字空间下 buildkit安装部署 登录你的机器，我这里机器是k8s的节点，节点安装的runtime是containerd，然后从github上下载安装包 # wget https://github.com/moby/buildkit/releases/download/v0.9.3/buildkit-v0.9.3.linux-amd64.tar.gz # tar -xvf buildkit-v0.9.3.linux-amd64.tar.gz 将安装包解压后，我们来启动buildkitd服务，使用--oci-worker=false --containerd-worker=true参数,可以让buildkitd服务使用containerd后端 buildkitd --oci-worker=false --containerd-worker=true & 采用buildctl构建本地镜像 这里我们写一个简单的dockerfile来构建镜像 [root@nwx-gr-node1 ~]# cat Dockerfile FROM centos:7 RUN yum install openssh-server openssh-clients tree nmap dos2unix lrzsz nc lsof wget tcpdump htop iftop iotop sysstat nethogs bind-u 然后执行下面命令来构建镜像 buildctl build \\ --frontend=dockerfile.v0 \\ --local context=. \\ --local dockerfile=. \\ --output type=image,name=ccr.ccs.tencentyun.com/v_cjweichen/nwx-reg:buildctl frontend可以使用网关做前端,未做其他尝试,这里直接使用dockerfile.0 --local context 指向当前目录,这是Dockerfile执行构建时的路径上下文,比如在从目录中拷贝文件到镜像里 --local dockerfile指向当前目录,表示Dockerfile在此目录 --output 的 name 表示构建的镜像名称 构建完成后镜像会存在本地containerd的buildkit名字空间下 推送镜像到远程仓库 这里我们先查看下刚构建好的镜像，可以用containerd的命令行工具ctr，类似已docker命令 [root@nwx-gr-node1 ~]# ctr -n buildkit i ls REF TYPE DIGEST SIZE PLATFORMS LABELS ccr.ccs.tencentyun.com/v_cjweichen/nwx-reg:buildctl application/vnd.docker.distribution.manifest.v2+json sha256:82c93337b755c5b1cb7ce7370aba1293ce0ba89d87782083ea2c54a7a05f73c9 146.2 MiB linux/amd64 - 然后执行下面命令进行镜像推送 [root@nwx-gr-node1 ~]# ctr -n buildkit i push -u xxxx:xxxx ccr.ccs.tencentyun.com/v_cjweichen/nwx-reg:buildctl manifest-sha256:82c93337b755c5b1cb7ce7370aba1293ce0ba89d87782083ea2c54a7a05f73c9: done |++++++++++++++++++++++++++++++++++++++| layer-sha256:bfadf5a699433130807948d95bf0f130370b0fc1187e13dde4f5eb889fa2906b: done |++++++++++++++++++++++++++++++++++++++| config-sha256:e863cf7ae13f8eae21f9aea93c3d5d4ffbac9ca6222edf7c561081b889a9d38d: done |++++++++++++++++++++++++++++++++++++++| layer-sha256:2d473b07cdd5f0912cd6f1a703352c82b512407db6b05b43f2553732b55df3bc: done |++++++++++++++++++++++++++++++++++++++| elapsed: 4.8 s total: 146.2 (30.3 MiB/s) 构建自动推送镜像到镜像仓库 上面我们是先构建好镜像，然后再推送镜像，其实buildkit支持构建完自动推送镜像。 首先配置下镜像仓库的登录配置文件，registry的帐号密码配置在 ~/.docker/config.json 文件中 , 这里沿用了Docker的配置,虽然我们并没有装Docker { \"auths\": { \"docker.io\": { \"auth\": \"base64(username:password)\" } } } 登录镜像的仓库的配置文件设置好之后，在执行命令构建并推送镜像，其实就是在后面加上push=true的参数即可 buildctl build \\ --frontend=dockerfile.v0 \\ --local context=. \\ --local dockerfile=. \\ --output type=image,name=ccr.ccs.tencentyun.com/v_cjweichen/nwx-reg:buildctl,push=true © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"k8s/k8s-common-commands.html":{"url":"k8s/k8s-common-commands.html","title":"【k8s】k8s常用命令总结","keywords":"","body":"k8s常用命令 获取hostNetwork网络模式的deployment kubectl get deployment --all-namespaces -o jsonpath='{range .items[?(@.spec.template.spec.hostNetwork==true)]} {\"namespace: \"} {.metadata.namespace}{\" name:\"} {.metadata.name} {\"\\n\"} {end}' 获取特权模式的deployment kubectl get deployment --all-namespaces -o jsonpath='{range .items[?(@.spec.template.spec.containers[*].securityContext.privileged==true)]} {\"namespace: \"}{.metadata.namespace}{\"name:\"} {.metadata.name} {\"\\n\"} {end}' 获取控制进程可以获得超出其父进程的特权的deployment kubectl get deployment --all-namespaces -o jsonpath='{range .items[?(@.spec.template.spec.containers[*].securityContext.allowPrivilegeEscalation==true)]} {\"namespace: \"}{.metadata.namespace}{\"name:\"} {.metadata.name} {\"\\n\"} {end}' 获取所有deployment的容器名称和namespace kubectl get deployment --all-namespaces -o jsonpath='{range .items[?(@.spec.template.spec.containers[*])]} {\"namespace: \"}{.metadata.namespace}{\"name:\"} {.metadata.name} {\"\\n\"} {end}' 获取所有deployment的容器端口 kubectl get deployment --all-namespaces -o jsonpath='{range .items[*]} {\"namespace: \"}{.metadata.namespace}{\" name:\"} {.metadata.name} {\" \"}{.spec.template.spec.containers[*].ports} {\"\\n\"} {end}' 获取配置了hostport的deployment kubectl get deployment --all-namespaces -o jsonpath='{range .items[*]} {\"namespace: \"}{.metadata.namespace}{\" name:\"} {.metadata.name} {\" \"}{.spec.template.spec.containers[*].ports} {\"\\n\"} {end}' |grep map | grep hostPort | awk '{print $1 $2 \" \"$3 $4}' 获取配置了capabilities属性的deployment kubectl get deployment --all-namespaces -o jsonpath='{range .items[*]} {\"namespace: \"}{.metadata.namespace}{\" name:\"} {.metadata.name} {\" \"}{.spec.template.spec.containers[*].securityContext} {\"\\n\"} {end}' |grep capabilities | awk '{print $1 $2 \" \"$3 $4}' 获取所有配置了hostNetwork的DaemonSet kubectl get ds --all-namespaces -o jsonpath='{range .items[?(@.spec.template.spec.hostNetwork==true)]} {\"namespace: \"} {.metadata.namespace}{\" name:\"} {.metadata.name} {\"\\n\"} {end}' 获取所有配置了hostIPC模式的DaemonSet kubectl get ds --all-namespaces -o jsonpath='{range .items[?(@.spec.template.spec.hostIPC==true)]} {\"namespace: \"} {.metadata.namespace}{\" name:\"} {.metadata.name} {\"\\n\"} {end}' 获取所有配置了hostPID模式的DaemonSet kubectl get ds --all-namespaces -o jsonpath='{range .items[?(@.spec.template.spec.hostPID==true)]} {\"namespace: \"} {.metadata.namespace}{\" name:\"} {.metadata.name} {\"\\n\"} {end}' 获取所有配置了allowPrivilegeEscalation模式的DaemonSet kubectl get ds --all-namespaces -o jsonpath='{range .items[?(@.spec.template.spec.containers[*].securityContext.allowPrivilegeEscalation==true)]} {\"namespace: \"}{.metadata.namespace}{\"name:\"} {.metadata.name} {\"\\n\"} {end}' 获取配置了hostport的DaemonSet kubectl get ds --all-namespaces -o jsonpath='{range .items[*]} {\"namespace: \"}{.metadata.namespace}{\" name:\"} {.metadata.name} {\" \"}{.spec.template.spec.containers[*].ports} {\"\\n\"} {end}' |grep map | grep hostPort | awk '{print $1 $2 \" \"$3 $4}' 获取所有pod的ip和所在node的ip kubectl get pods --all-namespaces -o=jsonpath='{range .items[*]}[nodeip:{.status.hostIP}, podip:{.status.podIP}]{\"\\n\"}{end}' © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"k8s/k8s-force-delete-terminating-ns.html":{"url":"k8s/k8s-force-delete-terminating-ns.html","title":"【k8s】强制删除Terminating状态ns","keywords":"","body":"强制删除Terminating状态ns kubectl get ns 查看处于Terminating的ns [root@VM_1_4_centos ~]# kubectl get ns | grep testns testns Terminating 21d 将处于Terminating的ns的描述文件保存下来 [root@VM_1_4_centos ~]# kubectl get ns testns -o json > tmp.json [root@VM_1_4_centos ~]# cat tmp.json { \"apiVersion\": \"v1\", \"kind\": \"Namespace\", \"metadata\": { \"creationTimestamp\": \"2020-10-13T14:28:07Z\", \"name\": \"testns\", \"resourceVersion\": \"13782744400\", \"selfLink\": \"/api/v1/namespaces/testns\", \"uid\": \"9ff63d71-a4a1-43bc-89e3-78bf29788844\" }, \"spec\": { \"finalizers\": [ \"kubernetes\" ] }, \"status\": { \"phase\": \"Terminating\" } } 本地启动kube proxy kubectl proxy --port=8081 新开窗口执行删除操作 curl -k -H \"Content-Type: application/json\" -X PUT --data-binary @tmp.json http://127.0.0.1:8081/api/v1/namespaces/testns/finalize 如果上面方法无法删除namespace，可以通过如下方法看下namespace是不是还有什么资源没有清理 若命名空间依然无法删除，则查询命名空间哪些资源 kubectl api-resources --verbs=list --namespaced -o name | xargs -n 1 kubectl get --show-kind --ignore-not-found -n 然后删除这些资源： $ kubectl -n p-4q9rv delete projectalertgroup.management.cattle.io/projectalert-workload-alert --grace-period=0 --force 若 Pod 还是无法删除，可以在 Pod 中添加补丁： kubectl -n p-4q9rv patch projectalertgroup.management.cattle.io/projectalert-workload-alert -p '{\"metadata\":{\"finalizers\":[]}}' --type='merge' 添加补丁后，强制删除： kubectl -n p-4q9rv delete projectalertrule.management.cattle.io/memory-close-to-resource-limited --grace-period=0 --force 然后执行下面命令删除namespace kubectl patch namespace -p '{\"metadata\":{\"finalizers\":[]}}' --type='merge' kubectl delete namespace cattle-system --grace-period=0 --force 其实也可以直接将修改对应ns生成json文件 [root@master-1 ~]# vim tmp.json 删除spec字段后，执行以下curl命令，使用kube-apiserver的8081端口，执行删除操作 #注意修改@XXX.json ，修改 namespaces/XXX/finalize ,其中XXX 表示你要删除的命名空间名称 [root@master-1 ~]# curl -k -H \"Content-Type: application/json\" -X PUT --data-binary @tmp.json http://127.0.0.1:8081/api/v1/namespaces/mysql/finalize 用下面命令清理也可以 $ kubectl get ns delete-me -o json | jq '.spec.finalizers=[]' > ns-without-finalizers.json cat ns-without-finalizers.json $ kubectl proxy & $ PID=$! $ curl -X PUT http://localhost:8001/api/v1/namespaces/delete-me/finalize -H \"Content-Type: application/json\" --data-binary @ns-without-finalizers.json $ kill $PID © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"k8s/userd-filebeat-as-sidecar-collect-log.html":{"url":"k8s/userd-filebeat-as-sidecar-collect-log.html","title":"【k8s】k8s中filebeat作为sidecar采集容器日志","keywords":"","body":"使用k8s的时候，会遇到一种情况就是需要单独采集下某个服务的日志进行过滤分析，这时候我们可以单独给服务部署一个filebeat的sidecar来采集过滤日志，下面我们来讲下如何部署 这里我们举例收集nginx服务的容器日志，直接部署下面的yaml文件即可 apiVersion: apps/v1 kind: Deployment metadata: name: nginx namespace: log spec: replicas: 2 selector: matchLabels: project: www app: nginx template: metadata: labels: project: www app: nginx spec: imagePullSecrets: - name: qcloudregistrykey containers: - name: nginx image: nginx ports: - containerPort: 80 name: web protocol: TCP resources: requests: cpu: 0.5 memory: 256Mi limits: cpu: 1 memory: 1Gi volumeMounts: - name: nginx-logs mountPath: /var/log/nginx - name: filebeat image: elastic/filebeat:7.3.1 args: [ \"-c\", \"/etc/filebeat.yml\", \"-e\", ] resources: limits: memory: 500Mi requests: cpu: 100m memory: 100Mi securityContext: runAsUser: 0 volumeMounts: - name: filebeat-config mountPath: /etc/filebeat.yml subPath: filebeat.yml - name: nginx-logs mountPath: /var/log/nginx volumes: - name: nginx-logs emptyDir: {} - name: filebeat-config configMap: name: filebeat-nginx-config --- apiVersion: v1 kind: ConfigMap metadata: name: filebeat-nginx-config namespace: log data: filebeat.yml: |- filebeat.inputs: - type: log paths: - /var/log/nginx/access.log # tags: [\"access\"] fields: app: www type: nginx-access fields_under_root: true setup.ilm.enabled: false setup.template.name: \"nginx-access\" setup.template.pattern: \"nginx-access-*\" output.elasticsearch: hosts: ['elasticsearch-master.log:9200'] index: \"nginx-access-%{+yyyy.MM.dd}\" 这里我们对上面的yaml进行说明下，我们将filebeat和nginx的/var/log/nginx目录挂载到emptyDir下，这样filebeat就可以直接读取到了nginx容器的日志目录，然后配置了一下filebeat的配置项，里面配置了日志采集的路径，以及输出的地址和索引名称，这里是直接收集到es里，当然你也可以投递到logstash和kafka logstash和kafka的配置参考如下 output.logstash: #logstash输出模块 enabled: true #启用模块 hosts: [\"localhost:5044\"] #logstash地址 worker: 1 #每个logstash的worker数？？？？？，默认1 compression_level: 3 #压缩级别，默认3 loadbalance: true #负载均衡开关，在不同的logstash间负载 pipelining: 0 #在处理新的批量期间，异步发送至logstash的批量次数？？？？？ index: 'filebeat' #可选配置，索引名称，默认为filebeat proxy_url: socks5://user:password@socks5-server:2233 #socks5代理服务器地址 proxy_use_local_resolver: false #使用代理时是否使用本地解析，默认false output.kafka: #kafka输出模块 output.redis: #redis输出模块 enabled: true #启用模块 hosts: [\"localhost:6379\"] #redis地址，地址为一个列表，如果loadbalance开启，则负载到里表中的服务器，当一个redis服务器不可达，事件将被分发到可到达的redis服务器 port: 6379 #redis端口，如果hosts内未包含端口信息，默认6379 key: filebeat #事件发布到redis的list或channel，默认filebeat password: #redis密码，默认无 db: 0 #redis的db值，默认0 datatype: list #发布事件使用的redis数据类型，如果为list，使用RPUSH命令（生产消费模式）。如果为channel，使用PUBLISH命令{发布订阅模式}。默认为list worker: 1 #为每个redis服务器启动的工作进程数，会根据负载均衡配置递增 loadbalance: true #负载均衡，默认开启 timeout: 5s #redis连接超时时间，默认5s max_retries: 3 #filebeat会忽略此设置，并一直重试到全部发送为止，其他beat设置为0即忽略，默认3次 bulk_max_size: 2048 ##对一个redis请求或管道批量的最大事件数，默认2048 proxy_url: #socks5代理地址，必须使用socks5:// proxy_use_local_resolver: false #使用代理时是否使用本地解析，默认false 因为我的es以及接到kibana里面了，这里我们试试在集群内访问下nginx服务的svc，在kibana是否能检索到日志 [root@VM-0-3-centos block]# for i in {1..100}; do curl http://172.16.90.29/; done 访问后，这里刷新下kibana，可以收到刚访问日志，我们访问了100次，这里也刚好收集100条 © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"k8s/k8s-generate-kubeonfig.html":{"url":"k8s/k8s-generate-kubeonfig.html","title":"【k8s】k8s中生成自定义用户kubeconfig","keywords":"","body":"这里我们讲一下如何用客户端证书和ca证书生成一份自定义用户的kubeconfig 进入节点上/etc/kubernetes/目录，我们发现节点有下面这几个文件，client的证书和秘钥是从kubelet-kubeconfig这个文件中提取出来的，提取方式参考文档https://cloud.tencent.com/developer/article/1814668，cluster-ca.crt是每个节点默认有的。 [root@VM-0-3-centos kubernetes]# ll total 56 -rw-r--r-- 1 root root 1135 Apr 17 21:55 client-cert.pem -rw-r--r-- 1 root root 1679 Apr 17 21:55 client-key.pem -rw-r--r-- 1 root root 1025 Nov 26 2020 cluster-ca.crt -rw-r--r-- 1 root root 5483 Nov 26 2020 kubelet-kubeconfig 下面我们执行下面命令来生成一份niewx.kubeconfig的kubeconfig，这里我们是指定了kubeconfig的名称，会生成在当前目录下，如果你不想指定名称，去掉--kubeconfig=niewx.kubeconfig这个，kubeconfig会默认生成在$HOME/.kube/config # 配置kubernetes集群参数 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/cluster-ca.crt \\ --embed-certs=true \\ --server=https://cls-xxxxx.ccs.tencent-cloud.com \\ --kubeconfig=niewx.kubeconfig # 配置客户端认证参数 kubectl config set-credentials niewx \\ --client-certificate=/etc/kubernetes/client-cert.pem \\ --embed-certs=true \\ --client-key=/etc/kubernetes/client-key.pem \\ --kubeconfig=niewx.kubeconfig # 设置上下文参数 kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=niewx \\ --kubeconfig=niewx.kubeconfig # 设置默认上下文 kubectl config --kubeconfig=niewx.kubeconfig use-context kubernetes 执行完上述命令就发现会生成一个user为niewx的kubeconfig文件，然后我们可以指定这个kubeconfig来访问集群，设置上下文非必须设置，不设置直接指定kubeconfg访问即可，或者将niewx.kubeconfig拷贝到$HOME/.kube/config这个文件，进行访问。 [root@VM-0-3-centos kubernetes]# ll | grep niewx -rw------- 1 root root 5477 Jun 2 19:10 niewx.kubeconfig [root@VM-0-3-centos kubernetes]# cat niewx.kubeconfig apiVersion: v1 clusters: - cluster: certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJd01UR server: https://cls-xxxxxx.ccs.tencent-cloud.com name: kubernetes contexts: - context: cluster: kubernetes user: niewx name: kubernetes current-context: \"\" kind: Config preferences: {} users: - name: niewx user: client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURHVENDQWdHZ0F3SUJBZ0lJS2NXVHpIY0ZudFl3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6Q client-key-data: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcFFJQkFBS0NBUUVBd3NGWHdtNjlFVkY1WW1DNGx5bXVocFR2cGt6bCsxd0dTdWxGSnJqU0VpSTlWSTV6Ck9Zclh1UkM2VmtTTnRVa [root@VM-0-3-centos kubernetes]# kubectl --kubeconfig=niewx.kubeconfig get node NAME STATUS ROLES AGE VERSION 10.0.0.10 Ready 60d v1.18.4-tke.8 10.0.0.157 Ready 147d v1.18.4-tke.6 10.0.0.2 Ready 55d v1.18.4-tke.8 10.0.0.3 Ready 189d v1.18.4-tke.8 © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"k8s/kubecm-manages-k8s-clusters.html":{"url":"k8s/kubecm-manages-k8s-clusters.html","title":"【k8s】kubecm管理多k8s集群","keywords":"","body":"kubecm是一个k8s集群管理工具，可以合并多个kubeconfig文件，切换集群等 安装kubecm curl -Lo kubecm.tar.gz https://github.com/sunny0826/kubecm/releases/download/v0.15.3/kubecm_0.15.3_Linux_x86_64.tar.gz tar -zxvf kubecm.tar.gz kubecm cd kubecm sudo mv kubecm /usr/local/bin/ 设置自动补全 $ source > ~/.bashrc $ source ~/.bashrc kubecm的使用 执行kubecm，如果显示下面内容说明安装成功 [root@VM-0-13-centos ~]# kubecm Manage your kubeconfig more easily. ██ ██ ██ ██ ██████ ███████ ██████ ███ ███ ██ ██ ██ ██ ██ ██ ██ ██ ████ ████ █████ ██ ██ ██████ █████ ██ ██ ████ ██ ██ ██ ██ ██ ██ ██ ██ ██ ██ ██ ██ ██ ██ ██████ ██████ ███████ ██████ ██ ██ Tips Find more information at: https://kubecm.cloud Usage: kubecm [command] Available Commands: add Add KubeConfig to $HOME/.kube/config alias Generate alias for all contexts clear Clear lapsed context, cluster and user completion Generates bash/zsh completion scripts create Create new KubeConfig(experiment) delete Delete the specified context from the kubeconfig help Help about any command list List KubeConfig merge Merge the KubeConfig files in the specified directory namespace Switch or change namespace interactively rename Rename the contexts of kubeconfig switch Switch Kube Context interactively version Print version info Flags: --config string path of kubeconfig (default \"/root/.kube/config\") -h, --help help for kubecm Use \"kubecm [command] --help\" for more information about a command. 添加集群 这里准备了3个集群的kubeconfig文件 [root@VM-0-13-centos .kube]# ll | grep config -rw-r--r-- 1 root root 1823 Jun 2 14:25 eks.config -rw-r--r-- 1 root root 5545 Jun 2 15:12 test.config -rw-r--r-- 1 root root 5541 Jun 2 14:54 tke.config 这里我们先创建一个config文件，用来生成合并后的kubeconfig [root@VM-0-13-centos .kube]# touch config [root@VM-0-13-centos .kube]# kubecm add -f tke.config Add Context: tke 👻 True 「tke.config」 write successful! +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ | CURRENT | NAME | CLUSTER | USER | SERVER | Namespace | +============+=========+=======================+====================+===================================+==============+ | * | tke | cluster-k4m2g9mf44 | user-k4m2g9mf44 | https://cls-xxxxxxxx.ccs.tence | default | | | | | | nt-cloud.com | | +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ [root@VM-0-13-centos .kube]# kubecm add -f eks.config Add Context: eks 👻 True 「eks.config」 write successful! +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ | CURRENT | NAME | CLUSTER | USER | SERVER | Namespace | +============+=========+=======================+====================+===================================+==============+ | | eks | cluster-6t8847hhfb | user-6t8847hhfb | https://xx.xx.xx.xx:443/ | default | +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ | * | tke | cluster-k4m2g9mf44 | user-k4m2g9mf44 | https://cls-xxxxxxxx.ccs.tence | default | | | | | | nt-cloud.com | | +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ [root@VM-0-13-centos .kube]# kubecm add -f test.config Add Context: test 👻 True 「test.config」 write successful! +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ | CURRENT | NAME | CLUSTER | USER | SERVER | Namespace | +============+=========+=======================+====================+===================================+==============+ | | eks | cluster-6t8847hhfb | user-6t8847hhfb | https://xx.xx.xx.xx:443/ | default | +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ | | test | cluster-9f86dg8h88 | user-9f86dg8h88 | https://cls-xxxxxxxx.ccs.tence | default | | | | | | nt-cloud.com | | +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ | * | tke | cluster-k4m2g9mf44 | user-k4m2g9mf44 | https://cls-xxxxxxxx.ccs.tence | default | | | | | | nt-cloud.com | | +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ kubecm查看集群 [root@VM-0-13-centos .kube]# kubecm ls +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ | CURRENT | NAME | CLUSTER | USER | SERVER | Namespace | +============+=========+=======================+====================+===================================+==============+ | | eks | cluster-6t8847hhfb | user-6t8847hhfb | https://xx.xx.xx.xx:443/ xxx| default | +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ | * | tke | cluster-k4m2g9mf44 | user-k4m2g9mf44 | https://cls-xxxxxxxx.ccs.tence | default | | | | | | nt-cloud.com | | +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ Cluster check succeeded! Kubernetes version v1.18.4-tke.6 Kubernetes master is running at https://cls-xxxxx.ccs.tencent-cloud.com [Summary] Namespace: 63 Node: 4 Pod: 155 kubecm删除集群 [root@VM-0-13-centos .kube]# kubecm delete test Context Delete:「test」 「/root/.kube/config」 write successful! +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ | CURRENT | NAME | CLUSTER | USER | SERVER | Namespace | +============+=========+=======================+====================+===================================+==============+ | | eks | cluster-6t8847hhfb | user-6t8847hhfb | https://xx.xx.xx.xx:443/ | default | +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ | * | tke | cluster-k4m2g9mf44 | user-k4m2g9mf44 | https://cls-xxxx.ccs.tence | default | | | | | | nt-cloud.com | | +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ 这里我们删除了test集群 kubecm切换集群 [root@VM-0-13-centos .kube]# kubectl get node NAME STATUS ROLES AGE VERSION 10.0.0.10 Ready 61d v1.18.4-tke.8 10.0.0.157 Ready 147d v1.18.4-tke.6 10.0.0.2 Ready 56d v1.18.4-tke.8 10.0.0.3 Ready 189d v1.18.4-tke.8 [root@VM-0-13-centos .kube]# kubecm switch eks 「/root/.kube/config」 write successful! +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ | CURRENT | NAME | CLUSTER | USER | SERVER | Namespace | +============+=========+=======================+====================+===================================+==============+ | * | eks | cluster-6t8847hhfb | user-6t8847hhfb | https://xx.xx.xx.xx:443/ | default | +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ | | tke | cluster-k4m2g9mf44 | user-k4m2g9mf44 | https://cls-xxxxxxxx.ccs.tence | default | | | | | | nt-cloud.com | | +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ Switched to context 「eks」 [root@VM-0-13-centos .kube]# kubectl get node NAME STATUS ROLES AGE VERSION eklet-subnet-ktam6hp8 Ready 56d v2.4.4-dirty 一开始我们默认操作集群是tke，现在切换到eks © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"k8s/getting-started-with-kustomize-actually.html":{"url":"k8s/getting-started-with-kustomize-actually.html","title":"【k8s】kustomize入门实践","keywords":"","body":"kustomize是kubernetes原生的配置管理，以无模板方式来定制应用的配置。kustomize使用k8s原生概念帮助创建并复用资源配置(YAML)，允许用户以一个应用描述文件（YAML 文件）为基础（Base YAML），然后通过Overlay的方式生成最终部署应用所需的描述文件。 这里简单了解下几个概念 overlay overlay 是一个 kustomization, 它修改(并因此依赖于)另外一个kustomization. overlay中的kustomization指的是一些其它的kustomization, 称为其 base. 没有 base, overlay 无法使用，并且一个 overlay 可以用作 另一个 overlay 的 base(基础)。简而言之，overlay 声明了与 base 之间的差异。通过 overlay 来维护基于 base 的不同 variants(变体)，例如开发、QA 和生产环境的不同variants，其实overlay就是不同版本的工作空间，依赖于base工作空间。 variant variant 是在集群中将 overlay 应用于 base 的结果。例如开发和生产环境都修改了一些共同 base 以创建不同的 variant。这些 variant 使用相同的总体资源，并与简单的方式变化，例如 deployment 的副本数、ConfigMap使用的数据源等。简而言之，variant 是含有同一组 base 的不同 kustomization，其实variant就是某一个版本环境的所有资源文件。 resource 在kustomize的上下文中，resource 是描述 k8s API 对象的 YAML 或 JSON 文件的相对路径。即是指向一个声明了 kubernetes API对象的YAML文件 patch 修改文件的一般说明。文件路径，指向一个声明了 kubernetes API patch 的 YAML 文件 kustomize安装 curl -s \"https://raw.githubusercontent.com/\\ kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\" | bash 安装成功后，执行kustomize可以查看帮助指导 [root@VM-0-13-centos mesh]# kustomize Manages declarative configuration of Kubernetes. See https://sigs.k8s.io/kustomize Usage: kustomize [command] Available Commands: build Print configuration per contents of kustomization.yaml cfg Commands for reading and writing configuration. completion Generate shell completion script create Create a new kustomization in the current directory edit Edits a kustomization file fn Commands for running functions against configuration. help Help about any command version Prints the kustomize version Flags: -h, --help help for kustomize --stack-trace print a stack-trace on error Additional help topics: kustomize docs-fn [Alpha] Documentation for developing and invoking Configuration Functions. kustomize docs-fn-spec [Alpha] Documentation for Configuration Functions Specification. kustomize docs-io-annotations [Alpha] Documentation for annotations used by io. kustomize docs-merge [Alpha] Documentation for merging Resources (2-way merge). kustomize docs-merge3 [Alpha] Documentation for merging Resources (3-way merge). kustomize tutorials-command-basics [Alpha] Tutorials for using basic config commands. kustomize tutorials-function-basics [Alpha] Tutorials for using functions. Use \"kustomize [command] --help\" for more information about a command. kustomize部署helloword kustomize的demo示例可以参考链接https://github.com/kubernetes-sigs/kustomize/tree/master/examples，下面我们以helloworld为例进行示范下 创建base 首先我们创建一个一个helloworld的工作空间，在/tmp下创建一个临时目录 DEMO_HOME=$(mktemp -d) 如果我们需要用到overlay，则需要创建base工作空间，让集群的资源放在base下 BASE=$DEMO_HOME/base mkdir -p $BASE curl -s -o \"$BASE/#1.yaml\" \"https://raw.githubusercontent.com\\ /kubernetes-sigs/kustomize\\ /master/examples/helloWorld\\ /{configMap,deployment,kustomization,service}.yaml\" 这样我们就将基础的yaml文件放到了base下 [root@VM-0-13-centos base]# tree $DEMO_HOME /tmp/tmp.w5Ic40K11n └── base ├── configMap.yaml ├── deployment.yaml ├── kustomization.yaml └── service.yaml 如果你想部署这些资源，可以用kubectl命令部署 kubectl apply -f $DEMO_HOME/base 我们可以预览下base的资源，会将base下的yaml内容打印在标准输出 [root@VM-0-13-centos base]# kustomize build $BASE apiVersion: v1 data: altGreeting: Good Morning! enableRisky: \"false\" kind: ConfigMap metadata: labels: app: hello name: the-map --- apiVersion: v1 kind: Service metadata: labels: app: hello name: the-service spec: ports: - port: 8666 protocol: TCP targetPort: 8080 selector: app: hello deployment: hello type: LoadBalancer --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: hello name: the-deployment spec: replicas: 3 selector: matchLabels: app: hello deployment: hello template: metadata: labels: app: hello deployment: hello spec: containers: - command: - /hello - --port=8080 - --enableRiskyFeature=$(ENABLE_RISKY) env: - name: ALT_GREETING valueFrom: configMapKeyRef: key: altGreeting name: the-map - name: ENABLE_RISKY valueFrom: configMapKeyRef: key: enableRisky name: the-map image: monopole/hello:1 name: the-container ports: - containerPort: 8080 当然我们也可以订制base下的，下面我们订制下app的label [root@VM-0-13-centos base]# sed -i.bak 's/app: hello/app: my-hello/' \\ > $BASE/kustomization.yaml [root@VM-0-13-centos base]# ll total 20 -rw-r--r-- 1 root root 117 Jun 4 12:10 configMap.yaml -rw-r--r-- 1 root root 750 Jun 4 12:10 deployment.yaml -rw-r--r-- 1 root root 266 Jun 4 12:37 kustomization.yaml -rw-r--r-- 1 root root 263 Jun 4 12:10 kustomization.yaml.bak -rw-r--r-- 1 root root 183 Jun 4 12:10 service.yaml [root@VM-0-13-centos base]# kustomize build $BASE | grep -C 3 app: kind: ConfigMap metadata: labels: app: my-hello name: the-map --- apiVersion: v1 kind: Service metadata: labels: app: my-hello name: the-service spec: ports: -- protocol: TCP targetPort: 8080 selector: app: my-hello deployment: hello type: LoadBalancer --- -- kind: Deployment metadata: labels: app: my-hello name: the-deployment spec: replicas: 3 selector: matchLabels: app: my-hello deployment: hello template: metadata: labels: app: my-hello deployment: hello spec: containers: 下面我们来部署多个Overlays来对应多个helloword订制版本 OVERLAYS=$DEMO_HOME/overlays mkdir -p $OVERLAYS/staging mkdir -p $OVERLAYS/production 创建staging Overlays 在staging目录中创建一个kustomization 文件，用来定义一个新的名称前缀和一些不同的 labels 。 cat $OVERLAYS/staging/kustomization.yaml namePrefix: staging- commonLabels: variant: staging org: acmeCorporation commonAnnotations: note: Hello, I am staging! resources: - ../../base patchesStrategicMerge: - map.yaml EOF 新增一个自定义的 configMap 将问候消息从 Good Morning! 改为 Have a pineapple! 。 同时，将 risky 标记设置为 true 。 cat $OVERLAYS/staging/map.yaml apiVersion: v1 kind: ConfigMap metadata: name: the-map data: altGreeting: \"Have a pineapple!\" enableRisky: \"true\" EOF 创建production Overlays 在 production 目录中创建一个 kustomization 文件，用来定义一个新的名称前缀和 labels 。 cat $OVERLAYS/production/kustomization.yaml namePrefix: production- commonLabels: variant: production org: acmeCorporation commonAnnotations: note: Hello, I am production! resources: - ../../base patchesStrategicMerge: - deployment.yaml EOF Production Patch，因为生产环境需要处理更多的流量，新建一个production patch来增加副本数。 cat $OVERLAYS/production/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: the-deployment spec: replicas: 5 EOF 比较overlays DEMO_HOME 现在包含： base目录：对拉取到的源配置进行了简单定制 overlays目录：包含在集群中创建不同 staging 和 production variants 的 kustomizations 和 patches 。 查看目录结构和差异： [root@VM-0-13-centos base]# tree $DEMO_HOME /tmp/tmp.w5Ic40K11n ├── base │ ├── configMap.yaml │ ├── deployment.yaml │ ├── kustomization.yaml │ ├── kustomization.yaml.bak │ └── service.yaml └── overlays ├── production │ ├── deployment.yaml │ └── kustomization.yaml └── staging ├── kustomization.yaml └── map.yaml 直接比较 staging 和 production 输出的不同： [root@VM-0-13-centos base]# diff \\ > more 3,4c3,4 altGreeting: Good Morning! > enableRisky: \"false\" 8c8 note: Hello, I am production! 12,13c12,13 variant: production > name: production-the-map 19c19 note: Hello, I am production! 23,24c23,24 variant: production > name: production-the-service 34c34 variant: production 41c41 note: Hello, I am production! 45,46c45,46 variant: production > name: production-the-deployment 48c48 replicas: 5 54c54 variant: production 58c58 note: Hello, I am production! 63c63 variant: production 75c75 部署不同的overlys 输出不同 overlys 的配置： [root@VM-0-13-centos base]# kustomize build $OVERLAYS/staging apiVersion: v1 data: altGreeting: Have a pineapple! enableRisky: \"true\" kind: ConfigMap metadata: annotations: note: Hello, I am staging! labels: app: my-hello org: acmeCorporation variant: staging name: staging-the-map --- apiVersion: v1 kind: Service metadata: annotations: note: Hello, I am staging! labels: app: my-hello org: acmeCorporation variant: staging name: staging-the-service spec: ports: - port: 8666 protocol: TCP targetPort: 8080 selector: app: my-hello deployment: hello org: acmeCorporation variant: staging type: LoadBalancer --- apiVersion: apps/v1 kind: Deployment metadata: annotations: note: Hello, I am staging! labels: app: my-hello org: acmeCorporation variant: staging name: staging-the-deployment spec: replicas: 3 selector: matchLabels: app: my-hello deployment: hello org: acmeCorporation variant: staging template: metadata: annotations: note: Hello, I am staging! labels: app: my-hello deployment: hello org: acmeCorporation variant: staging spec: containers: - command: - /hello - --port=8080 - --enableRiskyFeature=$(ENABLE_RISKY) env: - name: ALT_GREETING valueFrom: configMapKeyRef: key: altGreeting name: staging-the-map - name: ENABLE_RISKY valueFrom: configMapKeyRef: key: enableRisky name: staging-the-map image: monopole/hello:1 name: the-container ports: - containerPort: 8080 [root@VM-0-13-centos base]# kustomize build $OVERLAYS/production apiVersion: v1 data: altGreeting: Good Morning! enableRisky: \"false\" kind: ConfigMap metadata: annotations: note: Hello, I am production! labels: app: my-hello org: acmeCorporation variant: production name: production-the-map --- apiVersion: v1 kind: Service metadata: annotations: note: Hello, I am production! labels: app: my-hello org: acmeCorporation variant: production name: production-the-service spec: ports: - port: 8666 protocol: TCP targetPort: 8080 selector: app: my-hello deployment: hello org: acmeCorporation variant: production type: LoadBalancer --- apiVersion: apps/v1 kind: Deployment metadata: annotations: note: Hello, I am production! labels: app: my-hello org: acmeCorporation variant: production name: production-the-deployment spec: replicas: 5 selector: matchLabels: app: my-hello deployment: hello org: acmeCorporation variant: production template: metadata: annotations: note: Hello, I am production! labels: app: my-hello deployment: hello org: acmeCorporation variant: production spec: containers: - command: - /hello - --port=8080 - --enableRiskyFeature=$(ENABLE_RISKY) env: - name: ALT_GREETING valueFrom: configMapKeyRef: key: altGreeting name: production-the-map - name: ENABLE_RISKY valueFrom: configMapKeyRef: key: enableRisky name: production-the-map image: monopole/hello:1 name: the-container ports: - containerPort: 8080 将上述命令传递给kubectl进行部署 kustomize build $OVERLAYS/staging |\\ kubectl apply -f - kustomize build $OVERLAYS/production |\\ kubectl apply -f - 也可直接使用kubectl部署，但是需要注意的是kubectl版本需要在v1.14.0以上 kubectl apply -k $OVERLAYS/staging kubectl apply -k $OVERLAYS/production © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"k8s/Use-scripts-to-perform-health-detection-on-pods.html":{"url":"k8s/Use-scripts-to-perform-health-detection-on-pods.html","title":"【k8s】用脚本对pod进行健康探测","keywords":"","body":"Kubernetes 支持对容器进行周期性探测，并根据探测结果判断容器的健康状态，执行额外的操作。 健康检查类别 健康检查分为以下类别： 容器存活检查：用于检测容器是否存活，类似于执行 ps 命令检查进程是否存在。如果容器的存活检查失败，集群会对该容器执行重启操作。如果容器的存活检查成功，则不执行任何操作。 容器就绪检查：用于检测容器是否准备好开始处理用户请求。例如，程序的启动时间较长时，需要加载磁盘数据或者要依赖外部的某个模块启动完成才能提供服务。此时，可通过容器就绪检查方式检查程序进程，确认程序是否启动完成。如果容器的就绪检查失败，集群会屏蔽请求访问该容器。如果容器的就绪检查成功，则会开放对该容器的访问。 健康检查方式 TCP 端口探测 TCP 端口探测的原理如下： 对于提供 TCP 通信服务的容器，集群周期性地对该容器建立 TCP 连接。如果连接成功，证明探测成功，否则探测失败。选择 TCP 端口探测方式，必须指定容器监听的端口。 例如，一个 redis 容器，它的服务端口是6379。我们对该容器配置了 TCP 端口探测，并指定探测端口为6379，那么集群会周期性地对该容器的6379端口发起 TCP 连接。如果连接成功，证明检查成功，否则检查失败。 HTTP 请求探测 HTTP 请求探测是针对于提供 HTTP/HTTPS 服务的容器，并集群周期性地对该容器发起 HTTP/HTTPS GET 请求。如果 HTTP/HTTPS response 返回码属于200 - 399范围，证明探测成功，否则探测失败。使用 HTTP 请求探测必须指定容器监听的端口和 HTTP/HTTPS 的请求路径。 例如，提供 HTTP 服务的容器，服务端口为 80，HTTP 检查路径为 /health-check，那么集群会周期性地对容器发起GET http://containerIP:80/health-check 请求。 执行命令检查 执行命令检查是一种强大的检查方式，该方式要求用户指定一个容器内的可执行命令，集群会周期性地在容器内执行该命令。如果命令的返回结果是0，检查成功，否则检查失败。 对于 TCP 端口探测 和 HTTP 请求探测，都可以通过执行命令检查的方式来替代： 对于 TCP 端口探测，可以写一个程序对容器的端口进行 connect。如果 connect 成功，脚本返回0，否则返回-1。 对于 HTTP 请求探测，可以写一个脚本来对容器进行 wget 并检查 response 的返回码。例如，wget http://127.0.0.1:80/health-check。如果返回码在200 - 399的范围，脚本返回0，否则返回 -1。 脚本进行健康探测 其实很多时候，我们对业务容器进行健康探测可能无法用简单的一条命令或者探测端口是否可达就判断业务是否正常，而是需要用较复杂的脚本的来实现，这样我们就需要来判断脚本的返回值了，很多人会直接在脚本输出0或者-1来进行判断，但是发现健康检查不生效。 其实这里不能用echo来输出对应的返回值，因为你用echo输出值，实际上执行脚本还是成功的，探针是获取的echo $?的值，这个值还是0，那么用脚本进行健康探测，需要怎么来写呢？ #!/bin/sh num=$1 if [ $num = 1 ]; then echo \"ok\" else echo \"fail\" exit 1 fi 这里我们如果想探测失败需要用exit 1，这样执行脚本获取的返回值就是1，探针就会认为这个是失败的。 下面我们把脚本放到镜像里面测试下，然后配置下探针，看下是否会生效，这里可以用这个镜像测试ccr.ccs.tencentyun.com/nwx_registry/health-check:latest 下面我们看下我们的deployment对应的健康检查配置 apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: health-check qcloud-app: health-check name: health-check namespace: tke-test spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: health-check qcloud-app: health-check strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: labels: k8s-app: health-check qcloud-app: health-check spec: containers: - image: ccr.ccs.tencentyun.com/nwx_registry/health-check:latest imagePullPolicy: Always livenessProbe: exec: command: - sh - /tmp/test.sh - \"2\" failureThreshold: 3 initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 name: health-check resources: limits: cpu: 500m memory: 1Gi requests: cpu: 250m memory: 256Mi securityContext: privileged: false terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 根据我们配置的存活探针，这里检查脚本会返回1，也就是失败，连续探测3次失败后就会重启容器，我们验证下是不是这样的 Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 42s default-scheduler Successfully assigned tke-test/health-check-65b4d88575-sg7p9 to 10.0.0.157 Normal Started 40s kubelet, 10.0.0.157 Started container health-check Warning Unhealthy 2s (x3 over 22s) kubelet, 10.0.0.157 Liveness probe failed: fail Normal Killing 2s kubelet, 10.0.0.157 Container health-check failed liveness probe, will be restarted Normal Pulling 0s (x2 over 40s) kubelet, 10.0.0.157 Pulling image \"ccr.ccs.tencentyun.com/nwx_registry/health-check:latest\" Normal Pulled 0s (x2 over 40s) kubelet, 10.0.0.157 Successfully pulled image \"ccr.ccs.tencentyun.com/nwx_registry/health-check:latest\" Normal Created 0s (x2 over 40s) kubelet, 10.0.0.157 Created container health-check 从事件日志，可以发现，这里返回1，探针就认为是失败的，所以这里连续3次就重启容器。 综上所述，用探针来检查容器状态，如果用脚本进行判断，当失败时候，用exit 1来返回脚本返回结果。 © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"k8s/Batch-delete-ns-in-terminating-state-in-the-cluster.html":{"url":"k8s/Batch-delete-ns-in-terminating-state-in-the-cluster.html","title":"【k8s】批量删除集群内terminating状态的ns","keywords":"","body":"批量删除k8s集群terminating状态的ns 有的时候删除ns会卡主，处于terminating状态，这里为什么会卡主可以参考这个文档https://cloud.tencent.com/developer/article/1802531 如果集群存在多个terminating状态的ns，一个个删除比较麻烦，这里提供下一个简单的小脚本batch-delete-terminating-ns.sh来删除下，脚本内容如下 #!/bin/bash set -euxo pipefail if [ -f \"/etc/redhat-release\" ]; then yum install jq -y fi if [ -f \"/etc/lsb-release\" ]; then apt-get install jq -y fi kubectl proxy & PID=$! for i in `kubectl get ns | grep Terminating | awk -F \" \" '{print $1}'`; do kubectl get ns $i -o json | jq '.spec.finalizers=[]' > $i.json curl -X PUT http://localhost:8001/api/v1/namespaces/$i/finalize -H \"Content-Type: application/json\" --data-binary @${i}.json rm -rf $i.json done kill $PID © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"k8s/Modify-the-resources-configuration-of-the-container-under-the-namespace.html":{"url":"k8s/Modify-the-resources-configuration-of-the-container-under-the-namespace.html","title":"【k8s】批量修改命名空间下容器的resources配置","keywords":"","body":"修改命名空间下容器的resources配置 统一给单个命名空间下的容器配置resources，可以用下面脚本，具体的cpu和limit值设置，可以修改脚本，脚本内容如下： #!/bin/sh ns=$1 if [ $# = 0 ];then echo \"Run 'sh modify-workload-resorces.sh --h' for more information on a command.\" fi if [[ $1 = \"--h\" ]];then echo \"Enter the namespace to be repaired Usage: sh modify-workload-resorces.sh [namespace]\" fi main(){ for i in `kubectl get deploy,sts -n $ns |awk -F ' ' '{print $1}' | grep -v NAME`; do for j in `kubectl get $i -n $ns -o=jsonpath='{.spec.template.spec.containers[*].name}'`; do kubectl set resources $i -c=$j -n $ns --requests=cpu=50m,memory=512Mi --limits=cpu=500m,memory=1000Mi done done } if [ $# = 1 ];then main fi © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"k8s/Alpine-mirror-cannot-access-redis-with-telnet.html":{"url":"k8s/Alpine-mirror-cannot-access-redis-with-telnet.html","title":"【k8s】alpine镜像无法用telnet访问redis","keywords":"","body":"alpine镜像无法用telnet访问redis 问题现象 k8s中如果你启动的镜像操作系统是alpine，容器内无法用telnet连接redis。 测试过程 redis访问地址172.16.32.220，端口6379 分别用alpine和centos通过telnet测试连接redis 在alpine用redis-cli连接redis，排除alpine网络问题无法访问redis 解决方案 从上面的测试结果可以看出，alpine是能正常访问连接redis，然后centos也能telnet访问redis，只有在alpine无法telnet访问redis。 这个问题的原因，当前也没有具体的原因，怀疑是不同操作系统内核包不一样导致的。 这里的解决方案就是尽量避免在alpine通过telnet去测试redis是否正常或者网络是否通。 © vishon all right reserved，powered by GitbookUpdated at 2024-01-09 10:05:20 "},"k8s/coredns-startup-error-8080-port-address-already-in-use.html":{"url":"k8s/coredns-startup-error-8080-port-address-already-in-use.html","title":"【k8s】coredns服务pod启动报错listen tcp :8080: bind: address already in use","keywords":"","body":"coredns服务pod启动报错listen tcp :8080: bind: address already in use 问题现象 业务pod无法通过域名访问外网，在pod里面解析域名也失败，解析域名失败，应该是coredns服务出现了异常，检查coredns的pod，发现coredns的pod一直在重启，kubectl logs查看pod日志报错listen tcp :8080: bind: address already in use。 排查思路 从pod的启动日志报错来看，是端口被占用了，查看了下coredns的yaml，不是hostnetwork模式，正常容器里面怎么会出现监听端口被占用呢。怀疑是coredns的配置哪里有问题，检查了下coredns的configmaq配置，发现有在corefile里面自定义上游的dns配置。 a.com { forword . 1.1.1.1 2.2.2.2 health } 从上面自定义的coredns配置看，有加上health这个插件，那么这个插件需要怎么配置呢？到官网文档查了下https://coredns.io/plugins/health/ 发现文档里面有这个说明 If you have multiple Server Blocks, health can only be enabled in one of them (as it is process wide). If you really need multiple endpoints, you must run health endpoints on different ports: com { whoami health :8080 } net { erratic health :8081 } 从上面的说明看，如果需要使用health插件，需要指定检查端口才行，否则就用默认的8080，会导致端口被占用。 解决方案 既然知道问题原因了，这里解决方案有2个 去掉health插件 在health插件检查健康检查的端口配置，注意不能和coredns的8080冲突。 © vishon all right reserved，powered by GitbookUpdated at 2024-01-09 10:05:20 "},"k8s/Docker-logs-and-kubectl-logs-view-logs-are-inconsistent.html":{"url":"k8s/Docker-logs-and-kubectl-logs-view-logs-are-inconsistent.html","title":"【k8s】docker logs和kubectl logs查看日志不一致","keywords":"","body":"docker logs和kubectl logs查看日志不一致 问题现象 节点docker logs查看容器日志和命令kubectl logs查看日志有点不一致，kubectl logs查看的日志会少一部分。 排查思路 docker logs和kubectl logs查看日志都是查看/var/lib/docker/containers/[containerd_id]这个目录下的日志文件，既然是查看的结果不一致，说明是日志文件存在差异，进入这个目录查看了下 [root@node1 e1a3e821aa55c9a77419a4bbb063c084f337336e4c46c3008be6752fbb8f8554]# ll total 906480 drwx------ 2 root root 4096 Nov 1 17:33 checkpoints -rw------- 1 root root 16564 Nov 1 17:33 config.v2.json -rw-r----- 1 root root 28125126 Nov 5 15:47 e1a3e821aa55c9a77419a4bbb063c084f337336e4c46c3008be6752fbb8f8554-json.log -rw-r----- 1 root root 100000382 Nov 5 15:45 e1a3e821aa55c9a77419a4bbb063c084f337336e4c46c3008be6752fbb8f8554-json.log.1 -rw-r----- 1 root root 100000267 Nov 5 15:40 e1a3e821aa55c9a77419a4bbb063c084f337336e4c46c3008be6752fbb8f8554-json.log.2 -rw-r----- 1 root root 100000279 Nov 5 15:35 e1a3e821aa55c9a77419a4bbb063c084f337336e4c46c3008be6752fbb8f8554-json.log.3 -rw-r----- 1 root root 100000078 Nov 5 15:28 e1a3e821aa55c9a77419a4bbb063c084f337336e4c46c3008be6752fbb8f8554-json.log.4 -rw-r----- 1 root root 100000330 Nov 5 15:23 e1a3e821aa55c9a77419a4bbb063c084f337336e4c46c3008be6752fbb8f8554-json.log.5 -rw-r----- 1 root root 100000788 Nov 5 15:18 e1a3e821aa55c9a77419a4bbb063c084f337336e4c46c3008be6752fbb8f8554-json.log.6 -rw-r----- 1 root root 100000187 Nov 5 15:11 e1a3e821aa55c9a77419a4bbb063c084f337336e4c46c3008be6752fbb8f8554-json.log.7 -rw-r----- 1 root root 100000916 Nov 5 15:06 e1a3e821aa55c9a77419a4bbb063c084f337336e4c46c3008be6752fbb8f8554-json.log.8 -rw-r----- 1 root root 100003481 Nov 5 15:00 e1a3e821aa55c9a77419a4bbb063c084f337336e4c46c3008be6752fbb8f8554-json.log.9 -rw-r--r-- 1 root root 2336 Nov 1 17:33 hostconfig.json drwx------ 2 root root 4096 Nov 1 17:33 mounts 从上面可以发现，容器的标准输出日志是有进行轮转的，这个是在/etc/docker/daemon.json的log-opts字段配置，既然kubectl logs查看的日志少了一部分，看了下kubectl logs日志最早时间是哪个，查看后发现和最早时间是e1a3e821aa55c9a77419a4bbb063c084f337336e4c46c3008be6752fbb8f8554-json.log这个文件的最开始的日志时间对应上，排查到这里，问题已经明确了，kubectl logs和docker logs查看的日志文件是不一样的，导致了日志有不同。 问题结论 kubectl logs查看的日志是xxxx-json.log这个日志文件的，而docker logs查看的是所有日志文件的日志，因此查看存在一定差异。 © vishon all right reserved，powered by GitbookUpdated at 2024-01-09 10:05:20 "},"k8s/k8s-deploys-hostport-mode-pod-error-report.html":{"url":"k8s/k8s-deploys-hostport-mode-pod-error-report.html","title":"【k8s】k8s部署hostport模式pod报错端口冲突","keywords":"","body":"k8s部署hostport模式pod报错端口冲突 最近有客户在使用hostport的模式部署deployment的时候，有出现这个错误0/4 nodes are available: 4 node(s) didn't have free ports for the requested pod ports，导致deployment部署失败，从报错看是端口冲突了。 因为hostport是用的占用的node节点的端口，既然是端口冲突，我们到节点上查看下对应端口是否有监听，登录节点用netstat查看对应端口的监听，发现并没有被占用，端口没有被占用，为什么会出现部署失败报错端口被占用的问题。 后面我根据报错的yaml进行测试下，具体现象如下 apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: hostport-test qcloud-app: hostport-test name: hostport-test namespace: tke-test spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: hostport-test qcloud-app: hostport-test strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: creationTimestamp: null labels: k8s-app: hostport-test qcloud-app: hostport-test spec: containers: - image: nginx imagePullPolicy: Always name: hostport-test ports: - containerPort: 80 hostPort: 80 name: http protocol: TCP resources: limits: cpu: 500m memory: 1Gi requests: cpu: 250m memory: 256Mi securityContext: privileged: false terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 部署了hostPort的pod后，我们登录节点查看80端口的监听，发现并没有被监听 [root@VM-0-10-centos ~]# netstat -an | grep tcp | grep 80 tcp 0 0 0.0.0.0:30080 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:32580 0.0.0.0:* LISTEN tcp 0 0 10.0.0.10:46548 169.254.0.71:80 ESTABLISHED tcp 0 0 10.0.0.10:33762 169.254.0.71:80 ESTABLISHED 接下来我们接着往hostport-test所在的节点部署一个hostport模式的pod看看 apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: hostport-new qcloud-app: hostport-new name: hostport-new namespace: tke-test spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: hostport-new qcloud-app: hostport-new strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: creationTimestamp: null labels: k8s-app: hostport-new qcloud-app: hostport-new spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - 10.0.0.10 containers: - image: nginx imagePullPolicy: Always name: hostport-new ports: - containerPort: 80 hostPort: 80 name: http protocol: TCP resources: limits: cpu: 500m memory: 1Gi requests: cpu: 250m memory: 256Mi securityContext: privileged: false terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst hostNetwork: true imagePullSecrets: - name: qcloudregistrykey restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 我们看下hostport-new的事件，看下是否有出现报错 [root@VM-0-10-centos ~]# kubectl describe pod -n tke-test hostport-new-86786bc46f-kl8ph Name: hostport-new-86786bc46f-kl8ph Namespace: tke-test ................. Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal FailedSchedulingOnEkletNodes 7m49s (x16 over 12m) admission-controller node(s) didn't match node selector Warning FailedScheduling 65s (x10 over 12m) default-scheduler 0/5 nodes are available: 1 Too many pods, 2 node(s) didn't have free ports for the requested pod ports, 2 node(s) didn't match node selector. 从事件日志看，部署报错，也是报错端口没法使用，但是我们到节点查看80端口的确没有被监听，这是怎么回事呢？ 这里肯定是哪里配置不正常导致，我们仔细分析了下部署的yaml文件，发现hostport-test配置的hostport，但是没有配置hostNetwork，在k8s里面如果要hostport生效，需要配置hostNetwork为true才行，hostport-test没有配置hostNetwork，所以到节点上查看80端口没有被监听。 既然节点端口没被监听，为什么部署hostport-new这个pod时候还是会报错呢？hostport-new是有配置hostNetwork，其实从事件可以看出pod是调度失败，pod的调度是从etcd里面获取信息去判断节点是否存在相同端口的pod，也就是说当你有一个deployment存在hostport为80的字段时，Scheduler就会认为节点80端口已经被占用了，Scheduler不会实际去检查你的节点是否有这个端口的监听。 从上面的分析，就可以看出为什么节点没有起80端口的监听，但是部署hostport会报错端口被占用的报错。 © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"k8s/k8s-introduces-external-service-access-exception-through-endpoint.html":{"url":"k8s/k8s-introduces-external-service-access-exception-through-endpoint.html","title":"【k8s】k8s通过endpoint引入外部服务访问异常","keywords":"","body":"k8s通过endpoint引入外部服务访问异常 手动创建endpoint和svc引入集群外的nginx服务 首先通过yaml在集群创建ep和svc kind: Endpoints apiVersion: v1 metadata: # 此处 metadata.name 的值要和 service 中的 metadata.name 的值保持一致 # endpoint 的名称必须和服务的名称相匹配 name: out-of-cluster-nginx namespace: weixnie subsets: - addresses: # 服务将连接重定向到 endpoint 的 IP 地址 - ip: 172.16.0.4 ports: # 外部服务端口 # endpoint 的目标端口 - port: 8081 name: http-8081 --- apiVersion: v1 kind: Service metadata: # 此处 metadata.name 的值要和 endpoints 中的 metadata.name 的值保持一致 name: out-of-cluster-nginx # 外部服务服务统一在固定的名称空间中 namespace: weixnie spec: type: NodePort ports: - protocol: TCP port: 8081 targetPort: 8081 name: tcp-8081 测试集群内通过svc访问服务 bash-5.1# kubectl get svc out-of-cluster-nginx -n weixnie NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE out-of-cluster-nginx NodePort 10.55.253.167 8081:30128/TCP 62s bash-5.1# curl 10.55.253.167:8081 curl: (7) Failed to connect to 10.55.253.167 port 8081: Connection refused bash-5.1# 这里svc的clusterip是10.55.253.167，但是通过ip却连不上nginx，这是为什么呢？ 这里确认下nginx服务本身是否正常，直接访问集群外nginx是正常的 bash-5.1# curl 172.16.0.4:8081 Welcome to nginx! html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } Welcome to nginx! If you see this page, the nginx web server is successfully installed and working. Further configuration is required. For online documentation and support please refer to nginx.org. Commercial support is available at nginx.com. Thank you for using nginx. 问题分析 nginx服务是正常的，但是通过svc访问就不行，这个是为什么呢？这里首先查看下svc后端的ep是否正常 bash-5.1# kubectl describe svc out-of-cluster-nginx -n weixnie Name: out-of-cluster-nginx Namespace: weixnie Labels: Annotations: Selector: Type: NodePort IP Families: IP: 10.55.253.167 IPs: 10.55.253.167 Port: tcp-8081 8081/TCP TargetPort: 8081/TCP NodePort: tcp-8081 30128/TCP Endpoints: Session Affinity: None External Traffic Policy: Cluster Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal EnsuringService 3m19s service-controller Deleted Loadbalancer Normal EnsureServiceSuccess 3m19s service-controller Service Sync Success. RetrunCode: S2000 可以发下svc的后端ep是空的，为什么是空的，是不是ep关联svc有问题，这里检查下yaml，名称是一致的，端口也是一样的，唯一的区别是port的名称不一样，难道是这个导致的？先我们修改下svc的port名称。 bash-5.1# kubectl describe svc out-of-cluster-nginx -n weixnie Name: out-of-cluster-nginx Namespace: weixnie Labels: Annotations: Selector: Type: NodePort IP Families: IP: 10.55.253.167 IPs: 10.55.253.167 Port: http-8081 8081/TCP TargetPort: 8081/TCP NodePort: http-8081 30128/TCP Endpoints: 172.16.0.4:8081 Session Affinity: None External Traffic Policy: Cluster Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal EnsuringService 9m47s service-controller Deleted Loadbalancer Normal EnsureServiceSuccess 9m47s service-controller Service Sync Success. RetrunCode: S2000 将spec.port.name改成和endpoint一样的为http-8081，再次查看svc时候，后端ep就有了，这里我们通过svc访问下看看是否正常 bash-5.1# curl 10.55.253.167:8081 Welcome to nginx! html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } Welcome to nginx! If you see this page, the nginx web server is successfully installed and working. Further configuration is required. For online documentation and support please refer to nginx.org. Commercial support is available at nginx.com. Thank you for using nginx. 通过svc也访问正常了，看来问题就是ep的port名称和svc名称不一样导致的 结论 如果是通过endpoint引入外部服务到k8s集群内，需要注意保证endpoint和svc下面字段都一样 metadata.name endpoint的subsets.ports.name和svc的spec.port.name © vishon all right reserved，powered by GitbookUpdated at 2024-01-09 10:05:20 "},"k8s/pod-mounting-nfs-data-volume-error-report-insufficient-permissions.html":{"url":"k8s/pod-mounting-nfs-data-volume-error-report-insufficient-permissions.html","title":"【k8s】pod挂载nfs数据卷报错权限不足","keywords":"","body":"pod挂载nfs数据卷报错权限不足 k8s中将pod的数据持久化，很多时候我们会用到nfs，最近我们遇到一个pod挂载nfs报错的问题，具体报错如下 MountVolume.SetUp failed for volume \"xxxxxxx\" : mount failed: exit status 32 Mounting command: mount Mounting arguments: -t nfs 10.0.0.10:/data/xxxx /var/lib/eklet-agent/pods/d43cc0d5-b294-4040-9b1b-f9672d8a54e5/volumes/kubernetes.io~nfs/xxxxx Output: Created symlink /run/systemd/system/remote-fs.target.wants/rpc-statd.service → /usr/lib/systemd/system/rpc-statd.service. mount.nfs: Operation not permitted 从报错可以看出是权限不足，首先我们到nfs的服务端，检查下对应目录的权限，查看挂载目录的权限是/data/xxxx *(rw) nfs的权限是所有客户端都可以读写这个目录的，既然权限是足够，那么为什么会出现这个报错呢 通过google，发现有人对这个错误进行了定位和说明 由于pod出去的端口是高位端口，nfs默认是不允许高位端口去连接，所以需要加上insecure这个参数，这个参数的意思是 insecure：NFS通过1024以上的端口发送 最终的解决方案就是在nfs的/etc/exports文件将挂载的目录加上insecure参数 /data/xxxx *(rw),insecure © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"k8s/Service-binding-backend-endpoint-exception.html":{"url":"k8s/Service-binding-backend-endpoint-exception.html","title":"【k8s】service绑定后端endpoint异常","keywords":"","body":"service绑定后端endpoint异常 问题现象 问题现象是k8s集群内通过service访问某个服务报错no healthy upsteam，这里我用一个nginx服务来讲下这个问题 排查思路 nginx的deploy和svc对应的yaml如下 apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: nginx qcloud-app: nginx name: nginx namespace: weixnie spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: nginx qcloud-app: nginx strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: creationTimestamp: null labels: k8s-app: nginx qcloud-app: nginx spec: containers: - image: nginx:latest imagePullPolicy: Always name: nginx resources: limits: cpu: 500m memory: 1Gi securityContext: privileged: false terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey restartPolicy: Always --- apiVersion: v1 kind: Service metadata: labels: k8s-app: nginx qcloud-app: nginx name: nginx-test namespace: weixnie spec: ports: - name: 80-80-tcp port: 80 protocol: TCP targetPort: http selector: k8s-app: nginx qcloud-app: nginx sessionAffinity: None type: ClusterIP 这里apply后，可以直接通过svc访问下nginx [root@VM-55-5-tlinux ~]$ k get svc,pod -o wide | grep nginx service/nginx-test ClusterIP 10.55.255.43 80/TCP 7m46s k8s-app=nginx,qcloud-app=nginx pod/nginx-6ccd9d7969-f4rfj 1/1 Running 1 16d 10.55.1.34 172.16.55.5 [root@VM-55-5-tlinux ~]# curl 10.55.255.43 no healthy upsteam 报错返回的是no healthy upsteam，那后端的pod服务是否正常，直接访问pod看看 [root@VM-55-5-tlinux ~]# curl 10.55.1.34 Welcome to nginx! html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } Welcome to nginx! If you see this page, the nginx web server is successfully installed and working. Further configuration is required. For online documentation and support please refer to nginx.org. Commercial support is available at nginx.com. Thank you for using nginx. 直接pod正常，说明pod本身是正常的，问题是出在svc的转发上，这里先describe svc看下 [root@VM-55-5-tlinux ~]$ k describe svc nginx-test Name: nginx-test Namespace: weixnie Labels: k8s-app=nginx qcloud-app=nginx Annotations: Selector: k8s-app=nginx,qcloud-app=nginx Type: ClusterIP IP: 10.55.255.43 Port: 80-80-tcp 80/TCP TargetPort: http/TCP Endpoints: Session Affinity: None Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal EnsuringService 11m service-controller Deleted Loadbalancer Normal EnsureServiceSuccess 11m service-controller Service Sync Success. RetrunCode: S2000 从describe的信息可以发现，Endpoints是空的，为什么svc没绑定成功到endpoint呢？这里端口设置没问题，Selector的label和pod也是能匹配上的。 这里又仔细的看了下svc的yaml配置，发现TargetPort是用的http这个名称，而不是写的端口，翻了下官网文档 从文档说明来看，如果svc的TargetPort配置成非端口，用名称，需要先在deploy里面定义才行，这里deploy是没有定义这个端口名称的，所以svc配置成名称，导致svc无法正确选择后端endpoint。 解决方案 上面分析了，svc没绑定后端endpoint是以为TargetPort设置成了名称，那么这里解决方案就有2个 修改svc的TargetPort的配置，改成端口 在deploy里面定义下端口的名称 © vishon all right reserved，powered by GitbookUpdated at 2024-01-09 10:05:20 "},"k8s/StatefulSet-type-pod-domain-name-cannot-be-resolved.html":{"url":"k8s/StatefulSet-type-pod-domain-name-cannot-be-resolved.html","title":"【k8s】StatefulSet类型pod域名无法解析","keywords":"","body":"StatefulSet类型pod域名无法解析 问题现象 k8s集群中创建了一个StatefulSet的工作负载，然后创建了一个headless类型的service，具体的yaml如下 apiVersion: apps/v1 kind: StatefulSet metadata: labels: k8s-app: headles-svc-test qcloud-app: headles-svc-test name: headles-svc-test namespace: weixnie spec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: headles-svc-test qcloud-app: headles-svc-test serviceName: \"\" template: metadata: labels: k8s-app: headles-svc-test qcloud-app: headles-svc-test spec: containers: - image: nginx imagePullPolicy: Always name: headles-svc-test resources: limits: cpu: 500m memory: 1Gi requests: cpu: 250m memory: 256Mi securityContext: privileged: false terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 --- apiVersion: v1 kind: Service metadata: labels: k8s-app: headles-svc-test qcloud-app: headles-svc-test name: headles-svc-test namespace: weixnie spec: clusterIP: None clusterIPs: - None ports: - name: 80-80-tcp port: 80 protocol: TCP targetPort: 80 selector: k8s-app: headles-svc-test qcloud-app: headles-svc-test sessionAffinity: None type: ClusterIP 但是当我在pod里面测试通过pod的域名访问nginx服务时候，提示找不到这个域名 bash-5.1# ping headles-svc-test-0.headles-svc-test.weixnie.svc.cluster.local ping: bad address 'headles-svc-test-0.headles-svc-test.weixnie.svc.cluster.local' bash-5.1# nslookup headles-svc-test-0.headles-svc-test.weixnie.svc.cluster.local Server: 10.55.255.31 Address: 10.55.255.31:53 ** server can't find headles-svc-test-0.headles-svc-test.weixnie.svc.cluster.local: NXDOMAIN ** server can't find headles-svc-test-0.headles-svc-test.weixnie.svc.cluster.local: NXDOMAIN 排查思路 svc是headless类型，并且域名也是全域名，配置都是正常到，符合规范，为什么不能解析不了呢，首先我们这里试试解析下svc的域名试试，看下是否可以解析 bash-5.1# nslookup headles-svc-test.weixnie.svc.cluster.local Server: 10.55.255.31 Address: 10.55.255.31:53 Name: headles-svc-test.weixnie.svc.cluster.local Address: 10.55.2.61 解析service的名称是正常的，但是加上pod的名称就不行了。这里肯定是哪里配置不对，这里仔细看了下官网文档介绍https://kubernetes.io/zh-cn/docs/concepts/workloads/controllers/statefulset/ 这里说明了pod的域名是由serviceName决定的，但是我们的yaml里面serviceName配置的是空。这里要想通过{pod_name}.{svc_name}.{ns}.svc.cluster.local域名访问到pod，statefulset的spec.serviceName需要配置成headless类型svc的名称才行。 解决方案 这里修改yaml，serviceName配置成headless类型svc的名称 spec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: headles-svc-test qcloud-app: headles-svc-test serviceName: headles-svc-test © vishon all right reserved，powered by GitbookUpdated at 2024-01-09 10:05:20 "},"k8s/The-hosts-configuration-in-coredns-cannot-be-correctly-parsed-in-pods-in-the-k8s-cluster.html":{"url":"k8s/The-hosts-configuration-in-coredns-cannot-be-correctly-parsed-in-pods-in-the-k8s-cluster.html","title":"【k8s】k8s集群内pod内无法正确解析coredns中的hosts配置","keywords":"","body":"k8s集群内pod内无法正确解析coredns中的hosts配置 问题现象 k8s的coredns中进行了hosts的全局域名解析配置，但是在容器内解析对应的域名发现走的是k8s集群内部的svc.cluster.local域名，并没有解析到配置的hosts上 排查思路 这边首先测试了下在节点上配置了hosts是能正常解析，并且将pod的dns策略改成default不走coredns也是能正常解析，那么为什么经过coredns就不行了呢？这里看了下coredns中配置hosts的方式也没有问题。 这里查看了下coredns的官方文档hosts配置https://coredns.io/plugins/hosts/#description，发现我配置的hosts插件中没有设置fallthrough这个字段，根据文档的说明 fallthrough If zone matches and no record can be generated, pass request to the next plugin. If [ZONES…] is omitted, then fallthrough happens for all zones for which the plugin is authoritative. If specific zones are listed (for example and ), then only queries for those zones will be subject to fallthrough.in-addr.arpaip6.arpa 如果区域匹配并且无法生成记录，fallthrough则将请求传递给下一个插件，如果不设置，则不会继续找其他插件进行解析。 If you want to pass the request to the rest of the plugin chain if there is no match in the hosts plugin, you must specify the option.fallthrough 如果在hosts插件中没有匹配项的情况下要将请求传递给插件链的其余部分，则必须指定fallthrough，所以这里大概就是fallthrough没有设置导致的。 解决方案 coredns的hosts插件中加上fallthrough配置，然后重建coredns的pod即可。 © vishon all right reserved，powered by GitbookUpdated at 2024-01-09 10:05:20 "},"k8s/The-prestop-configuration-in-k8s-does-not-take-effect.html":{"url":"k8s/The-prestop-configuration-in-k8s-does-not-take-effect.html","title":"【k8s】k8s中配置prestop不生效","keywords":"","body":"k8s中配置prestop不生效 k8s的prestop主要是用于pod的优雅停止，那么什么是优雅停止呢？ 何为优雅停止？ 优雅停止(Graceful shutdown)这个说法来自于操作系统，我们执行关机之后都得 OS 先完成一些清理操作，而与之相对的就是硬中止(Hard shutdown)，比如拔电源。 到了分布式系统中，优雅停止就不仅仅是单机上进程自己的事了，往往还要与系统中的其它组件打交道。比如说我们起一个微服务，网关把一部分流量分给我们，这时: 假如我们一声不吭直接把进程杀了，那这部分流量就无法得到正确处理，部分用户受到影响。不过还好，通常来说网关或者服务注册中心会和我们的服务保持一个心跳，过了心跳超时之后系统会自动摘除我们的服务，问题也就解决了；这是硬中止，虽然我们整个系统写得不错能够自愈，但还是会产生一些抖动甚至错误; 假如我们先告诉网关或服务注册中心我们要下线，等对方完成服务摘除操作再中止进程，那不会有任何流量受到影响；这是优雅停止，将单个组件的启停对整个系统影响最小化; 按照惯例，SIGKILL 是硬终止的信号，而 SIGTERM 是通知进程优雅退出的信号，因此很多微服务框架会监听 SIGTERM 信号，收到之后去做反注册等清理操作，实现优雅退出。 pod的停止流程 由于Pod所代表的是在集群中节点上运行的进程，当不再需要这些进程时允许其体面地 终止是很重要的。一般不应武断地使用KILL信号终止它们，导致这些进程没有机会完成清理操作。 设计的目标是令你能够请求删除进程，并且知道进程何时被终止，同时也能够确保删除操作终将完成。当你请求删除某个Pod时，集群会记录并跟踪Pod 体面终止周期， 而不是直接强制地杀死 Pod。在存在强制关闭设施的前提下， kubelet会尝试体面地终止 Pod。 通常情况下，容器运行时会发送一个TERM信号到每个容器中的主进程。 很多容器运行时都能够注意到容器镜像中 STOPSIGNAL 的值，并发送该信号而不是 TERM。 一旦超出了体面终止限期，容器运行时会向所有剩余进程发送 KILL信号，之后Pod就会被从API服务器上移除。如果kubelet或者容器运行时的管理服务在等待进程终止期间被重启， 集群会从头开始重试，赋予 Pod完整的体面终止限期。 Pod退出的流程大致如下： 1.用户删除 Pod。 2.1 Pod 进入 Terminating 状态。 2.2 与此同时，K8s 会将 Pod 从对应的 service 上摘除。 2.3 与此同时，针对有 PreStop Hook 的容器，kubelet 会调用每个容器的 PreStop Hook，假如 PreStop Hook 的运行时间超出了 grace period，kubelet 会发送 SIGTERM 并再等 2 秒。 2.4 与此同时，针对没有 PreStop Hook 的容器，kubelet 发送 SIGTERM。 3.grace period 超出之后，kubelet 发送 SIGKILL 干掉尚未退出的容器。 prestop配置不生效 为了服务做平滑升级或无损发布，我们会通过配置停止前的等待时间的prestop Hook，但是当我们sleep时间较长会发现prestop没有生效，其实pod的停止流程是有说明的 说明： 如果 preStop 回调所需要的时间长于默认的体面终止限期，你必须修改terminationGracePeriodSeconds属性值来使其正常工作。 terminationGracePeriodSeconds默认是30s，当我们的prostop配置的sleep 70s，就会出现pod不会等70s才被销毁，会马上kill掉，那么要怎么要配置才会生效呢？ 其实这里只需要将terminationGracePeriodSeconds时长修改比prestop所执行的时间长即可，也就是说你的prestop需要100s执行完成，你的terminationGracePeriodSeconds设置为101s就不会出现不生效的问题。 © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"tke/tke-used-docker-in-docker.html":{"url":"tke/tke-used-docker-in-docker.html","title":"【tke】tke上使用docker-in-docker","keywords":"","body":"这里我们讲一下如何在tke集群上部署docker in docker的pod，这里前提条件是集群的runtime需要用的是docker类型，如果是containerd是不行的。 apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: docker-in-docker qcloud-app: docker-in-docker name: docker-in-docker namespace: tke-test spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: docker-in-docker qcloud-app: docker-in-docker strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: creationTimestamp: null labels: k8s-app: docker-in-docker qcloud-app: docker-in-docker spec: containers: - command: - sleep - 70d image: docker:latest imagePullPolicy: Always name: docker-in-docker resources: limits: cpu: 500m memory: 1Gi requests: cpu: 250m memory: 256Mi securityContext: privileged: false terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/run/docker.sock name: vol subPath: docker.sock dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 volumes: - hostPath: path: /var/run type: DirectoryOrCreate name: vol 这里我们用官方提供的docker in docker镜像，镜像默认是没有常驻进程，需要加上sleep命令起一个常驻进程，然后我们将节点的/var/run/docker.sock挂载容器内。 接下来我们进入容器就可以执行docker命令了 [root@VM-0-13-centos ~]# kubectl exec -it docker-in-docker-5b49479696-cm6gd -n tke-test /bin/sh / # docker version Client: Version: 20.10.6 API version: 1.40 Go version: go1.13.15 Git commit: 370c289 Built: Fri Apr 9 22:42:10 2021 OS/Arch: linux/amd64 Context: default Experimental: true Server: Docker Engine - Community Engine: Version: 19.03.9 API version: 1.40 (minimum version 1.12) Go version: go1.13.10 Git commit: 9d988398e7 Built: Fri May 15 00:28:17 2020 OS/Arch: linux/amd64 Experimental: false containerd: Version: v1.2.13 GitCommit: 7ad184331fa3e55e52b890ea95e65ba581ae3429 runc: Version: 1.0.0-rc10 GitCommit: dc9208a3303feef5b3839f4323d9beb36df0a9dd docker-init: Version: 0.18.0 GitCommit: fec3683 / # © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"tke/tke-deploy-jumpserver.html":{"url":"tke/tke-deploy-jumpserver.html","title":"【tke】tke上部署Jumpserver跳板机","keywords":"","body":"本篇文章主要讲述如何在tke集群上部署Jumpserver跳板机，本次采用的1.18.4版本的集群。 部署mysql数据库 这里我们通过helm部署mysql数据库 helm install nwx-mysql stable/mysql --namespace mysql 这里我们需要获取下mysql的root用户数据库密码 kubectl get secret nwx-mysql -n mysql -o jsonpath={.data.mysql-root-password} |base64 -d 注意这里还需要给Jumpserver创建好数据库，登录mysql执行下面这条sql create database jumpserver default charset 'utf8'; 部署redis数据库 redis数据库我们也通过helm部署下，也部署在mysql命名空间 helm install nwx-redis bitnami/redis --namespace mysql 然后获取下redis数据库的密码 kubectl get secret nwx-redis -n mysql -o jsonpath={.data.redis-password} |base64 -d 部署Jumpserver 这里我们先在控制台创建一个jumpserver-datadir的pvc，使用了20G的云硬盘 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: jumpserver-datadir namespace: jumpserver annotations: volume.beta.kubernetes.io/storage-provisioner: cloud.tencent.com/qcloud-cbs spec: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi 然后生成下jumpserver需要到的SECRET_KEY和BOOTSTRAP_TOKEN # SECRET_KEY 生成方式： cat /dev/urandom | tr -dc A-Za-z0-9 | head -c 50 # BOOTSTRAP_TOKEN生成方式： cat /dev/urandom | tr -dc A-Za-z0-9 | head -c 16 apiVersion: apps/v1 kind: Deployment metadata: name: jumpserver namespace: jumpserver labels: app.kubernetes.io/instance: jumpserver app.kubernetes.io/name: jumpserver spec: replicas: 1 strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate selector: matchLabels: app.kubernetes.io/instance: jumpserver app.kubernetes.io/name: jumpserver template: metadata: labels: app.kubernetes.io/instance: jumpserver app.kubernetes.io/name: jumpserver spec: containers: - env: - name: SECRET_KEY value: \"j9fKwmVV39DzqZ27tWnMffpLzP6TsvQkHCaxJRcKn\" - name: BOOTSTRAP_TOKEN value: \"nWZStpQ1UTO\" - name: DB_ENGINE value: \"mysql\" - name: DB_HOST value: \"nwx-mysql.mysql\" - name: DB_PORT value: \"3306\" - name: DB_USER value: \"root\" - name: \"DB_PASSWORD\" value: \"2hblVjr\" - name: DB_NAME value: \"jumpserver\" - name: REDIS_HOST value: \"nwx-redis-master.mysql\" - name: REDIS_PORT value: \"6379\" - name: REDIS_PASSWORD value: \"twsnty9\" image: jumpserver/jms_all:1.5.9 imagePullPolicy: IfNotPresent name: jumpserver ports: - containerPort: 80 name: http protocol: TCP - containerPort: 2222 name: ssh protocol: TCP volumeMounts: - mountPath: /opt/jumpserver/data/media name: datadir volumes: - name: datadir persistentVolumeClaim: claimName: jumpserver-datadir --- apiVersion: v1 kind: Service metadata: name: jumpserver namespace: jumpserver labels: app.kubernetes.io/instance: jumpserver app.kubernetes.io/name: jumpserver spec: ports: - name: http port: 80 targetPort: 80 protocol: TCP - name: ssh port: 2222 targetPort: 2222 protocol: TCP selector: app.kubernetes.io/instance: jumpserver app.kubernetes.io/name: jumpserver 创建ingress提供访问域名 apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: ingress name: jumpserver-ingress namespace: jumpserver spec: rules: - host: jumpserver.tke.niewx.cn http: paths: - backend: serviceName: jumpserver servicePort: 80 path: / 然后再控制台输入访问域名 ，jumpserver默认的登录密码是admin/admin © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"tke/get-the-cbsId-of-pvc-in-the-tke-cluster.html":{"url":"tke/get-the-cbsId-of-pvc-in-the-tke-cluster.html","title":"【tke】获取pvc对应的cbs-id","keywords":"","body":"使用tke的过程中，有时候我们需要获取下pvc对应的cbs id，可以用下面get_pvc_cbs_id.sh脚本去快速获取 #!/bin/bash if [ $# = 0 ];then echo \"Use \"--h\" for more information about a given command.\" fi if [[ $1 = \"--h\" ]];then echo \" Please enter the pvc name for the first parameter, and empty the name for the second parameter Usage: sh get_pvc_cbs_id.sh pvc-name namespace\" fi ns=$2 pvc_name=$1 main(){ pv=`kubectl get pvc ${pvc_name} -n $ns -o jsonpath={.spec.volumeName}` cbs_id=`kubectl get pv ${pv} -o jsonpath={.spec.qcloudCbs.cbsDiskId}` echo $cbs_id } if [ $# = 2 ];then main fi 执行脚本，第一个参数输入pvc名称，第二个参数输入对应命名空间 [root@VM-0-13-centos script]# sh get_pvc_cbs_id.sh nwx-mysql mysql disk-xxxxx © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"tke/clean-the-evicted-state-pod-in-the-cluster.html":{"url":"tke/clean-the-evicted-state-pod-in-the-cluster.html","title":"【tke】清除集群中所有的evited状态pod","keywords":"","body":"有时候由于节点的内存或者磁盘使用率较高导致集群中产生了大量的evited状态pod，这些pod如果不手动删除，会一直存在集群中，这里我们提供了脚本clean-evicted-pod.sh来一键清理集群中的evited状态pod。 #!/bin/bash basepath=$(cd `dirname $0`; pwd) kubectl get pods -A | grep Evicted | awk -F \" \" '{print $1,$2}' >> $basepath/tmp.file while read line do ns=`echo $line | awk -F \" \" '{print $1}'` podname=`echo $line | awk -F \" \" '{print $2}'` kubectl delete pod $podname -n $ns done 复制上面脚本，直接执行即可，如果想清除其他状态的pod，可以将grep Evited改成其他的，比如说grep Pending © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"tke/adjust-workload-replicas-under-the-namespace.html":{"url":"tke/adjust-workload-replicas-under-the-namespace.html","title":"【tke】调整命名空间所有deploy和sts的副本数","keywords":"","body":"有时候我们为了测试我们的服务，不需要他一直运行着，当我们测试完，就希望将服务都停掉，避免占用集群资源，一般都是将负载的副本数设置为0，但是又希望对应的deployment还存在集群中，这样下次测试直接调整副本数就可以运行服务了。 下面提供了一个脚本scale-workload-replicas.sh可以调整命名空间的所有的deployment和statefulset的副本数量 #!/bin/sh ns=$1 replicas=$2 if [ $# = 0 ];then echo \"Run 'sh scale-workload-replicas.sh --h' for more information on a command.\" fi if [[ $1 = \"--h\" ]];then echo \"Please enter the first parameter enters the namespace, the second parameter enters the number of replicas Usage: sh scale-workload-replicas.sh [namespace] [replicas]\" fi main(){ for i in `kubectl get deploy,sts -n $ns |awk -F ' ' '{print $1}' | grep -v NAME`; do kubectl scale --replicas=$replicas -n $ns $i done } if [ $# = 2 ];then main fi 如果我们需要停掉这个命名空间下所有服务，则执行脚本第一个参数输入命名空间，第二个参数输入副本数0 [root@VM-0-13-centos script]# kubectl get pod -n mesh NAME READY STATUS RESTARTS AGE client-6fd64cfc6b-q9ml7 2/2 Running 0 4d23h nginx-5dbf784b68-bmr86 2/2 Running 0 16d nginx-v1-647887d8fd-qllgx 2/2 Running 0 39d nginx-v2-bcccc8f9f-jb926 2/2 Running 0 39d sleep-7f474bbcbd-tdgmf 2/2 Running 0 39d [root@VM-0-13-centos script]# sh scale-workload-replicas.sh mesh 0 deployment.apps/client scaled deployment.apps/nginx scaled deployment.apps/nginx-v1 scaled deployment.apps/nginx-v2 scaled deployment.apps/sleep scaled [root@VM-0-13-centos script]# kubectl get pod -n mesh No resources found. 如果需要拉起我们的服务，也只需要执行脚本，第一个参数输入命名空间，第二个参数输入副本数（输入1代表所有负载起一个副本） [root@VM-0-13-centos script]# sh scale-workload-replicas.sh mesh 1 deployment.apps/client scaled deployment.apps/nginx scaled deployment.apps/nginx-v1 scaled deployment.apps/nginx-v2 scaled deployment.apps/sleep scaled [root@VM-0-13-centos script]# kubectl get pod -n mesh NAME READY STATUS RESTARTS AGE client-6fd64cfc6b-ggz58 2/2 Running 0 23s nginx-5dbf784b68-c22hl 2/2 Running 0 22s nginx-v1-647887d8fd-n7bfh 2/2 Running 0 22s nginx-v2-bcccc8f9f-lnm6q 2/2 Running 0 22s sleep-7f474bbcbd-z2jj7 2/2 Running 0 22s © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"tke/modify-the-permissions-of-the-container-mount-directory.html":{"url":"tke/modify-the-permissions-of-the-container-mount-directory.html","title":"【tke】如何在非root用户启动的镜像中设置挂载目录权限","keywords":"","body":"我们在部署应用到k8s集群中，很多时候容器的启动用户不是root，并且会挂载数据目录到pvc上，但是又要通过启动用户写文件到挂载目录上，这样就会出现一个问题，就是启动用户没有权限读我们的数据挂载目录，今天我们提供一个init修改目录权限的方法来解决这个问题 首先我们部署一个grafana，并将grafana的数据存储目录/var/lib/grafana挂载到cbs上，这里pvc提前创建好了 apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: grafana qcloud-app: grafana name: grafana namespace: monitor spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: grafana qcloud-app: grafana strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: creationTimestamp: null labels: k8s-app: grafana qcloud-app: grafana spec: containers: - image: grafana/grafana:master imagePullPolicy: IfNotPresent name: grafana resources: limits: cpu: \"1\" memory: 2Gi requests: cpu: \"1\" memory: 1Gi securityContext: privileged: false terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/lib/grafana name: vol subPath: grafana dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 volumes: - name: vol persistentVolumeClaim: claimName: grafana 当我们启动pod之后会发现，pod一直在重启，查看日志报错是没有/var/lib/grafana这个目录的权限，因为grafana镜像的启动用户是grafana，而/var/lib/grafana这个目录，grafana是没有权限读写的。 [root@VM-0-13-centos ~]# kubectl logs -f grafana-84774b67d9-r79fv -n monitor GF_PATHS_DATA='/var/lib/grafana' is not writable. You may have issues with file permissions, more information here: http://docs.grafana.org/installation/docker/#migrate-to-v51-or-later mkdir: can't create directory '/var/lib/grafana/plugins': Permission denied 下面我们用一个init容器来修改/var/lib/grafana这个目录的权限， 再看下pod能否启动成功，这里重点关注init容器的配置 apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: grafana qcloud-app: grafana name: grafana namespace: monitor spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: grafana qcloud-app: grafana strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: creationTimestamp: null labels: k8s-app: grafana qcloud-app: grafana spec: containers: - image: grafana/grafana:master imagePullPolicy: IfNotPresent name: grafana resources: limits: cpu: \"1\" memory: 2Gi requests: cpu: \"1\" memory: 1Gi securityContext: privileged: false terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/lib/grafana name: vol subPath: grafana dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey initContainers: - args: - -c - mkdir -p /var/lib/grafana && chmod -R 777 /var/lib/grafana command: - /bin/sh image: centos:7 imagePullPolicy: Always name: init resources: limits: cpu: 500m memory: 1Gi requests: cpu: 250m memory: 256Mi securityContext: privileged: false terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/lib/grafana name: vol restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 volumes: - name: vol persistentVolumeClaim: claimName: grafana 这里将上面的yaml部署后，查看pod是running，直接访问3000端口也是可以通的，说明grafana是可以正常启动的。 [root@VM-0-13-centos ~]# kubectl get pod -n monitor -o wide | grep grafana grafana-84dd4d4454-sp2xx 1/1 Running 0 2m4s 10.0.2.83 10.0.0.3 1/1 [root@VM-0-13-centos ~]# curl 10.0.2.83:3000 Found. 这里我们简要说下我们的解决方案，其实就是用一个init容器创建/var/lib/grafana这个目录，然后挂载到cbs卷上，并修改权限为777，init容器执行完之后，grafana再运行去读这个目录的权限就是777，即使grafana的启动用户不是root，也有权限读/var/lib/grafana这个目录，容器也就可以正常运行了 [root@VM-0-13-centos ~]# kubectl exec -it grafana-84dd4d4454-sp2xx bash Error from server (NotFound): pods \"grafana-84dd4d4454-sp2xx\" not found [root@VM-0-13-centos ~]# kubectl exec -it grafana-84dd4d4454-sp2xx bash -n monitor bash-5.0$ cd var/lib/ bash-5.0$ ls -al total 24 drwxr-xr-x 1 root root 4096 Nov 26 2020 . drwxr-xr-x 1 root root 4096 Oct 21 2020 .. drwxr-xr-x 2 root root 4096 Oct 21 2020 apk drwxrwxrwx 4 root root 4096 Jun 13 12:03 grafana drwxr-xr-x 2 root root 4096 Oct 21 2020 misc drwxr-xr-x 2 root root 4096 Oct 21 2020 udhcpd © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"tke/The-domain-name-resolved-in-the-alpine-container-exceeds-5s.html":{"url":"tke/The-domain-name-resolved-in-the-alpine-container-exceeds-5s.html","title":"【tke】alpine镜像内解析域名超5s","keywords":"","body":"问题现象 tke集群的pod访问某个外部域名发现很慢，超5s以上，访问其他域名不会有问题，在节点访问是正常，而且不是所有的容器都有问题，这里到底是怎么回事呢？ 排查思路 首先只在某个pod出现这个问题，说明不是网络或者dns有问题，后面发现出现问题的都是alphie系统，为什么alphine会出现这个问题，这里我分别在centos和alpine抓包，看下导致耗时在哪里 抓包测试 首先我们在alpine镜像中测试访问域名，然后抓包 bash-4.4# time curl -w \"%{time_namelookup}\" --location --request POST 'https://elink.spic.com.cn/' {\"errcode\":40014,\"errmsg\":\"invalid access_token [logid:]\"}5.073069 real 0m5.278s user 0m0.017s sys 0m0.003s bash-4.4# tcpdump -i any port 53 tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes 09:13:49.074130 IP go-test-5f78b5569f-r2q9v.59071 > kube-dns.kube-system.svc.cluster.local.53: 47843+ A? elink.spic.com.cn.tke-test.svc.cluster.local. (62) 09:13:49.074164 IP go-test-5f78b5569f-r2q9v.59071 > kube-dns.kube-system.svc.cluster.local.53: 48334+ AAAA? elink.spic.com.cn.tke-test.svc.cluster.local. (62) 09:13:49.074257 IP go-test-5f78b5569f-r2q9v.44323 > kube-dns.kube-system.svc.cluster.local.53: 52788+ PTR? 140.52.16.172.in-addr.arpa. (44) 09:13:49.074596 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.59071: 47843 NXDomain*- 0/1/0 (155) 09:13:49.074660 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.59071: 48334 NXDomain*- 0/1/0 (155) 09:13:49.074710 IP go-test-5f78b5569f-r2q9v.42646 > kube-dns.kube-system.svc.cluster.local.53: 51281+ A? elink.spic.com.cn.svc.cluster.local. (53) 09:13:49.074744 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.44323: 52788*- 1/0/0 PTR kube-dns.kube-system.svc.cluster.local. (122) 09:13:49.075046 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.42646: 51682 NXDomain*- 0/1/0 (146) 09:13:49.075115 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.42646: 51281 NXDomain*- 0/1/0 (146) 09:13:49.075191 IP go-test-5f78b5569f-r2q9v.35865 > kube-dns.kube-system.svc.cluster.local.53: 61054+ A? elink.spic.com.cn.cluster.local. (49) 09:13:49.075242 IP go-test-5f78b5569f-r2q9v.35865 > kube-dns.kube-system.svc.cluster.local.53: 61474+ AAAA? elink.spic.com.cn.cluster.local. (49) 09:13:49.075419 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.35865: 61054 NXDomain*- 0/1/0 (142) 09:13:49.075477 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.35865: 61474 NXDomain*- 0/1/0 (142) 09:13:49.075535 IP go-test-5f78b5569f-r2q9v.56020 > kube-dns.kube-system.svc.cluster.local.53: 26687+ A? elink.spic.com.cn. (35) 09:13:49.075607 IP go-test-5f78b5569f-r2q9v.56020 > kube-dns.kube-system.svc.cluster.local.53: 26917+ AAAA? elink.spic.com.cn. (35) 09:13:49.085340 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.56020: 26687 1/0/0 A 39.155.244.138 (68) 09:13:50.761856 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.56020: 26917 ServFail 0/0/0 (35) 09:13:50.761896 IP go-test-5f78b5569f-r2q9v.56020 > kube-dns.kube-system.svc.cluster.local.53: 26917+ AAAA? elink.spic.com.cn. (35) 09:13:50.762014 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.56020: 26917 ServFail* 0/0/0 (35) 09:13:50.762043 IP go-test-5f78b5569f-r2q9v.56020 > kube-dns.kube-system.svc.cluster.local.53: 26917+ AAAA? elink.spic.com.cn. (35) 09:13:50.762114 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.56020: 26917 ServFail* 0/0/0 (35) 09:13:50.762149 IP go-test-5f78b5569f-r2q9v.56020 > kube-dns.kube-system.svc.cluster.local.53: 26917+ AAAA? elink.spic.com.cn. (35) 09:13:50.762247 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.56020: 26917 ServFail* 0/0/0 (35) 09:13:50.762280 IP go-test-5f78b5569f-r2q9v.56020 > kube-dns.kube-system.svc.cluster.local.53: 26917+ AAAA? elink.spic.com.cn. (35) 09:13:50.762341 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.56020: 26917 ServFail* 0/0/0 (35) 09:13:51.576211 IP go-test-5f78b5569f-r2q9v.56020 > kube-dns.kube-system.svc.cluster.local.53: 26917+ AAAA? elink.spic.com.cn. (35) 09:13:51.576403 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.56020: 26917 ServFail* 0/0/0 (35) 09:13:51.576442 IP go-test-5f78b5569f-r2q9v.56020 > kube-dns.kube-system.svc.cluster.local.53: 26917+ AAAA? elink.spic.com.cn. (35) 09:13:51.576660 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.56020: 26917 ServFail* 0/0/0 (35) 09:13:51.576687 IP go-test-5f78b5569f-r2q9v.56020 > kube-dns.kube-system.svc.cluster.local.53: 26917+ AAAA? elink.spic.com.cn. (35) 09:13:51.576851 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.56020: 26917 ServFail* 0/0/0 (35) 09:13:51.576886 IP go-test-5f78b5569f-r2q9v.56020 > kube-dns.kube-system.svc.cluster.local.53: 26917+ AAAA? elink.spic.com.cn. (35) 09:13:51.577039 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.56020: 26917 ServFail* 0/0/0 (35) 09:13:51.577073 IP go-test-5f78b5569f-r2q9v.56020 > kube-dns.kube-system.svc.cluster.local.53: 26917+ AAAA? elink.spic.com.cn. (35) 09:13:51.577153 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.56020: 26917 ServFail* 0/0/0 (35) 从上面发现，在alphine镜像会多次解析AAAA记录，造成netfilter race，详细的分析可以参考这个https://blog.quentin-machu.fr/2018/06/24/5-15s-dns-lookups-on-kubernetes/ 我们又在对应的centos上测试了下，发现每次轮询dns server只会出现一个AAAA记录，时间比起alpine会短很多 [root@centos-684c48fccd-k24tp /]# time curl -w \"%{time_namelookup}\" --location --request POST 'https://elink.spic.com.cn/' {\"errcode\":40014,\"errmsg\":\"invalid access_token [logid:]\"}1.510 real 0m1.836s user 0m0.035s sys 0m0.075s 09:17:13.970833 IP centos-684c48fccd-k24tp.41304 > kube-dns.kube-system.svc.cluster.local.domain: 7375+ A? elink.spic.com.cn.tke-test.svc.cluster.local. (62) 09:17:13.970870 IP centos-684c48fccd-k24tp.41304 > kube-dns.kube-system.svc.cluster.local.domain: 38125+ AAAA? elink.spic.com.cn.tke-test.svc.cluster.local. (62) 09:17:13.971520 IP kube-dns.kube-system.svc.cluster.local.domain > centos-684c48fccd-k24tp.41304: 38125 NXDomain*- 0/1/0 (155) 09:17:13.971567 IP kube-dns.kube-system.svc.cluster.local.domain > centos-684c48fccd-k24tp.41304: 7375 NXDomain*- 0/1/0 (155) 09:17:13.971625 IP centos-684c48fccd-k24tp.45222 > kube-dns.kube-system.svc.cluster.local.domain: 65110+ A? elink.spic.com.cn.svc.cluster.local. (53) 09:17:13.971669 IP centos-684c48fccd-k24tp.45222 > kube-dns.kube-system.svc.cluster.local.domain: 3169+ AAAA? elink.spic.com.cn.svc.cluster.local. (53) 09:17:13.971996 IP kube-dns.kube-system.svc.cluster.local.domain > centos-684c48fccd-k24tp.45222: 65110 NXDomain*- 0/1/0 (146) 09:17:13.972018 IP kube-dns.kube-system.svc.cluster.local.domain > centos-684c48fccd-k24tp.45222: 3169 NXDomain*- 0/1/0 (146) 09:17:13.972044 IP centos-684c48fccd-k24tp.36358 > kube-dns.kube-system.svc.cluster.local.domain: 23192+ A? elink.spic.com.cn.cluster.local. (49) 09:17:13.972069 IP centos-684c48fccd-k24tp.36358 > kube-dns.kube-system.svc.cluster.local.domain: 34973+ AAAA? elink.spic.com.cn.cluster.local. (49) 09:17:13.972427 IP kube-dns.kube-system.svc.cluster.local.domain > centos-684c48fccd-k24tp.36358: 23192 NXDomain*- 0/1/0 (142) 09:17:13.972658 IP kube-dns.kube-system.svc.cluster.local.domain > centos-684c48fccd-k24tp.36358: 34973 NXDomain*- 0/1/0 (142) 09:17:13.972698 IP centos-684c48fccd-k24tp.36416 > kube-dns.kube-system.svc.cluster.local.domain: 11372+ A? elink.spic.com.cn. (35) 09:17:13.972728 IP centos-684c48fccd-k24tp.36416 > kube-dns.kube-system.svc.cluster.local.domain: 3185+ AAAA? elink.spic.com.cn. (35) 09:17:13.973772 IP kube-dns.kube-system.svc.cluster.local.domain > centos-684c48fccd-k24tp.36416: 11372 1/0/0 A 39.155.244.138 (68) 09:17:15.218386 IP kube-dns.kube-system.svc.cluster.local.domain > centos-684c48fccd-k24tp.36416: 3185 ServFail 0/0/0 (35) 这边用优化后的alphine镜像geekidea/alpine-a:3.7，该镜像移除了AAAA记录，然后测试发现没有AAAA解析记录后会非常快 / # time curl -w \"%{time_namelookup}\" --location --request POST 'https://elink.spic.com.cn/' {\"errcode\":40014,\"errmsg\":\"invalid access_token [logid:]\"}0.003296 real 0m 0.17s user 0m 0.01s sys 0m 0.00s 09:12:07.919297 IP alpine-a-5684fcc8c7-mnfhp.37258 > kube-dns.kube-system.svc.cluster.local.53: 11480+ A? elink.spic.com.cn.tke-test.svc.cluster.local. (62) 09:12:07.919630 IP kube-dns.kube-system.svc.cluster.local.53 > alpine-a-5684fcc8c7-mnfhp.37258: 11480 NXDomain*- 0/1/0 (155) 09:12:07.919686 IP alpine-a-5684fcc8c7-mnfhp.59507 > kube-dns.kube-system.svc.cluster.local.53: 18200+ A? elink.spic.com.cn.svc.cluster.local. (53) 09:12:07.920076 IP kube-dns.kube-system.svc.cluster.local.53 > alpine-a-5684fcc8c7-mnfhp.59507: 18200 NXDomain*- 0/1/0 (146) 09:12:07.920119 IP alpine-a-5684fcc8c7-mnfhp.33037 > kube-dns.kube-system.svc.cluster.local.53: 63110+ A? elink.spic.com.cn.cluster.local. (49) 09:12:07.920426 IP kube-dns.kube-system.svc.cluster.local.53 > alpine-a-5684fcc8c7-mnfhp.33037: 63110 NXDomain*- 0/1/0 (142) 09:12:07.920508 IP alpine-a-5684fcc8c7-mnfhp.58282 > kube-dns.kube-system.svc.cluster.local.53: 51705+ A? elink.spic.com.cn. (35) 09:12:07.921322 IP kube-dns.kube-system.svc.cluster.local.53 > alpine-a-5684fcc8c7-mnfhp.58282: 51705 1/0/0 A 106.38.29.46 (68) 结论 这里在容器内解析域名需要5s以上是因为基础镜像的操作系统导致，主要看镜像内对AAAA记录解析次数，像alpine默认会对域名解析多次AAAA记录。 参考文档 https://openforum.hand-china.com/t/topic/1111 © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"tke/TKE-redeploys-cert-manager-and-keeps-getting-stuck.html":{"url":"tke/TKE-redeploys-cert-manager-and-keeps-getting-stuck.html","title":"【tke】TKE重新部署cert-manager一直卡主","keywords":"","body":"最近研究了一下在tke上部署cert-manager来实现为ingress免费签发域名证书，具体的部署文档可以参考https://www.niewx.cn/cert-manager/2021/08/07/cert-manager-issues-free-certificates-for-domain-names/ 这里为了测试，我先部署了cert-manager，然后又删除了，第二次部署的时候发现一直卡在创建crd上，导致一直部署不成功，具体现象如下。 [root@VM-0-13-centos ~]# kubectl apply --validate=false -f https://raw.githubusercontent.com/TencentCloudContainerTeam/manifest/master/cert-manager/cert-manager.yaml customresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io created ^C 那么这里是怎么回事呢？既然创建crd一直卡在，那么肯定是创建crd的过程哪里除了问题，当时想的是不是之前创建的crd资源没有清理干净导致的呢？ 首先我查下和cert-manager相关的crd资源 [root@VM-0-13-centos ~]# kubectl get crd | grep cert certificaterequests.cert-manager.io 2021-08-09T10:55:18Z certificates.cert-manager.io 2021-08-09T10:55:18Z challenges.acme.cert-manager.io 2021-06-02T06:45:43Z 这里我们清理下未删除的crd，当我们删除challenges.acme.cert-manager.io 这个crd一直无法删除成功，那么问题就明朗了，就是crd无法删除，导致创建crd一直卡主，我这边尝试强制删除下 [root@VM-0-13-centos ~]# kubectl delete crd challenges.acme.cert-manager.io --grace-period=0 --force warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely. customresourcedefinition.apiextensions.k8s.io \"challenges.acme.cert-manager.io\" force deleted [root@VM-0-13-centos ~]# kubectl get crd challenges.acme.cert-manager.io NAME CREATED AT challenges.acme.cert-manager.io 2021-06-02T06:45:43Z 强制删除后发现crd资源还存在，这里删除失败了，那么要清楚这种删除不掉的crd资源呢？ [root@VM-0-13-centos ~]# kubectl patch crd challenges.acme.cert-manager.io -p '{\"metadata\":{\"finalizers\":[]}}' --type=merge customresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io patched [root@VM-0-13-centos ~]# kubectl delete crd challenges.acme.cert-manager.io --grace-period=0 --force warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely. Error from server (NotFound): customresourcedefinitions.apiextensions.k8s.io \"challenges.acme.cert-manager.io\" not found 执行上面命令，先将finalizers字段配置信息删除，然后强制删除即可，删除了相关crd后，我们再部署下 [root@VM-0-13-centos ~]# kubectl apply --validate=false -f https://raw.githubusercontent.com/TencentCloudContainerTeam/manifest/master/cert-manager/cert-manager.yaml customresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io created namespace/cert-manager created serviceaccount/cert-manager-cainjector created serviceaccount/cert-manager created serviceaccount/cert-manager-webhook created clusterrole.rbac.authorization.k8s.io/cert-manager-cainjector unchanged clusterrole.rbac.authorization.k8s.io/cert-manager-controller-issuers unchanged clusterrole.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers unchanged clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificates unchanged clusterrole.rbac.authorization.k8s.io/cert-manager-controller-orders unchanged clusterrole.rbac.authorization.k8s.io/cert-manager-controller-challenges unchanged clusterrole.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim unchanged clusterrole.rbac.authorization.k8s.io/cert-manager-view unchanged clusterrole.rbac.authorization.k8s.io/cert-manager-edit unchanged clusterrolebinding.rbac.authorization.k8s.io/cert-manager-cainjector unchanged clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-issuers unchanged clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers unchanged clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificates unchanged clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-orders unchanged clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-challenges unchanged clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim unchanged role.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection unchanged role.rbac.authorization.k8s.io/cert-manager:leaderelection unchanged role.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created rolebinding.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection unchanged rolebinding.rbac.authorization.k8s.io/cert-manager:leaderelection configured rolebinding.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created service/cert-manager created service/cert-manager-webhook created deployment.apps/cert-manager-cainjector created deployment.apps/cert-manager created deployment.apps/cert-manager-webhook created mutatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook configured validatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook configured 再次部署就可以成功了。 总结 当我们在集群重新部署crd资源卡主的时候，可以搜下集群是不是存在上次未删除的crd资源，然后清理下，如强制删也无法删除，用patch修改下finalizers字段后再删除，清理成功后，再部署即可。 © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"tke/Cannot-access-back-end-services-through-load-balancing-type-ingress.html":{"url":"tke/Cannot-access-back-end-services-through-load-balancing-type-ingress.html","title":"【tke】无法通过负载均衡类型ingress访问后端服务","keywords":"","body":"无法通过负载均衡类型ingress访问后端服务 问题现象 tke集群部署了一个nginx服务，并通过一个负载均衡类型的ingress暴露域名提供访问，但是通过域名无法访问到后端nginx服务。 排查思路 首先查看了ingress对应的clb监听后端是否正常，发现后端监听没有创建。 没有成功创建监听说明ingress同步规则到clb失败，这时候查看ingress事件是否有报错，看了下事件也没有具体报错。 事件不能看出来，这时候再去查看下ingress-controller日志，对应的pod是部署在集群内kube-system命名空间下的l7-lb-controller这个deploy，根据ingress名称搜下日志发现有报错，从报错中基本可以看出问题所在，一般主要存在下面几种情况。 1.后端的Service类型不匹配 sync ingress(coding01/nginx) error!err:Ingress Sync ClientError. ErrorCode: E4047 Details: Ingress: coding01/nginx. Service(coding01/nginx) is no able to be the backend of ingress. NodePort service or LoadBalancer service is support for ingress. OriginError: 2.ingress后端的svc被删除 2021-12-22T17:23:35.35178018+08:00 W1222 09:23:35.351592 1 controller.go:2729] sync ingress(coding01/nginx.) error!err:Ingress Sync ClientError. ErrorCode: E4041 Details: Ingress: coding01/nginx. Service(xxx/xxxx) is not found. OriginError: 解决方案 如果是service类型不匹配，需要将后端service为odePort或者LoadBalancer才行，这个问题只会出现在用yaml创建ingress，如果是控制创建的ingress是无法选择后端ClusterIP类型的service。 ingress后端svc被删除，需要将被删除的svc对应的ingress规则去掉，或者将svc重新创建好。 © vishon all right reserved，powered by GitbookUpdated at 2024-01-09 10:05:20 "},"tke/Pod-startup-error-report-mount-destination-xxxx-not-absolute.html":{"url":"tke/Pod-startup-error-report-mount-destination-xxxx-not-absolute.html","title":"【tke】pod启动报错mount destination xxxx not absolute","keywords":"","body":"pod启动报错mount destination xxxx not absolute 问题现象 部署镜像到tke集群上，pod启动报错。 Error: failed to start container \"volume-test\": Error response from daemon: OCI runtime create failed: invalid mount {Destination:data/test Type:bind Source:/var/lib/docker/volumes/afb1908160a4aab18f754325b4df4968fcb4846cb4c9367c326d2e3f70d59f3b/_data Options:[rbind]}: mount destination data/test not absolute: unknown 对应的Dockerfile如下 FROM nginx VOLUME data/test 排查思路 查看pod事件报错，提示是因为data/test不是绝对路径导致的，这里是dockerfile编写不规范导致的。 解决方案 将volume字段的路径改成绝对路径即可。 © vishon all right reserved，powered by GitbookUpdated at 2024-01-09 10:05:20 "},"tke/Pod-uses-components-to-mount-cfs-to-report-errors.html":{"url":"tke/Pod-uses-components-to-mount-cfs-to-report-errors.html","title":"【tke】pod采用组件挂载cfs报错","keywords":"","body":"pod采用组件挂载cfs报错 问题现象 pod通过cfs组件挂载cfs一直pending，pod起不来，查看事件报错MountVolume.MountDevice failed for volume \"xxxxxxx\" : kubernetes.io/csi: attacher.MountDevice failed to create newCsiDriverClient: driver name com.tencent.cloud.csi.cfs not found in the list of registered CSI drivers。 排查思路 根据事件日志报错是因为cfs组件agent异常导致，检查cfs组件的agent是正常运行。 查看事件发现有一条日志Starting pod sandbox eks-xxxxx，这个说明是调度到虚拟节点的，但是cfs组件的agent是csi-nodeplugin-cfsplugin(kube-system)，这个负载是DaemonSet类型，虚拟节点是无法运行DaemonSet，所以组件的agent无法在虚拟节点上运行。导致pod挂载cfs异常。 解决方案 将pod调度到正常节点上，不要调度到虚拟节点上，或者采用nfs的方式进行挂载。 © vishon all right reserved，powered by GitbookUpdated at 2024-01-09 10:05:20 "},"tke/The-pod-in-the-tke-cluster-cannot-resolve-the-service-name.html":{"url":"tke/The-pod-in-the-tke-cluster-cannot-resolve-the-service-name.html","title":"【tke】TKE集群内pod无法解析service name","keywords":"","body":"TKE集群内pod无法解析service name 问题现象 tke集群内的pod无法解析service name到cluserip上，服务之间访问出现异常。 本次问题现象就是在demo的pod无法解析同命名空间下的nginx这个service name，导致demo访问nginx异常。 排查思路 k8s集群内解析service name都是通过coredns来解析的，pod内会去请求coredns的service然后请求coredns的pod来解析service name，原理是这样，现在无法解析service name，是不是访问coredns的pod不通呢？ 进入demo直接访问coredns的pod，发现ping都不通，问题就出在这里，demo为什么无法访问coredns呢？要么就是coredns异常了，要么就是集群网络哪里有异常，这里看了下coredns的状态和日志都是正常的。 那说明是集群网络哪里有问题了，这里首先检查下节点的安全组，看下是否有问题。 查看了下安全组的入站和出站规则，出站都是放通，入站的话有放通vpc的网段，但是容器网段只放通了一个小的网段。 查看集群的容器网段发现是16的掩码，但是安全组只放通了26的掩码，这里安全组肯定是有问题。 解决方案 上面我们排查到了问题是出在安全组，这里我们将安全改下，放通全部容器网段看看。 放通全部容器网段后，再进入pod测试，发现可以正常解析域名了，并且也能正常访问coredns了。 集群内pod无法解析service-name问题就是节点安全组导致，入站没有放通全部容器网段导致的。 这里顺便解释下为什么要安全组放通容器网段，tke集群内默认部署了一个ip-masq-agent组件，这个组件的作用是决定pod访问哪些网段不做nat，默认配置的是容器网段和vpc网段，也就是说demo这个pod访问coredns不会做nat，coredns看到的是demo真实的客户端ip，也就是demo的pod-ip，当coredns和demo不在一个节点，就会跨节点访问，节点安全组没有放通demo所在的网段，导致demo无法访问coredns，最终在pod内的现象就是无法解析service name。 © vishon all right reserved，powered by GitbookUpdated at 2024-01-09 10:05:20 "},"tke/tke-cluster-pod-mount-cos-or-cfs-pvc-timeout.html":{"url":"tke/tke-cluster-pod-mount-cos-or-cfs-pvc-timeout.html","title":"【tke】TKE集群内pod挂载cos/cfs类型pvc超时","keywords":"","body":"TKE集群内pod挂载cos/cfs类型pvc超时 问题现象 cfs/cos类型的pvc卷挂载超时 报错内容： timed out waiting for the condition; skipping pod 问题原因 业务负载中指定了 fsGroup，导致 kubelet 在完成 cos 的挂载后，会把挂载目录下所有文件进行一次权限修改，修改为 fsGroup 指定权限。 cos 桶中文件过多，导致 kubelet 存储准备工作卡在了权限修改这一步。（用户的cos桶内有5万多文件）（https://github.com/kubernetes/kubernetes/issues/69699 ） 后续旧 pod 的删除以及新 pod 的创建的存储相关工作都因上一步而卡住。cbs刷写速度较快，不容易出现该问题，cfs/cos这种会延时很厉害，容易超时 解决方案 有四种解决方案： （推荐）选择文件数不多的子目录进行挂载，需要修改 pv 中的 path 参数。 （推荐）若可以保证每次修改文件权限一致的话，可以在 pod 中添加如下参数 spec.securityContext.fsGroupChangePolicy: OnRootMismatch 这样只要目录下文件权限已匹配，就不会去刷权限了。 加上这个参数后，pod第一次启动还是会刷所有的文件权限，有超时问题。 但是后续pod重建就不需要刷权限，所以设置了该参数后如果仍有超时现象，请让用户耐心等待文件权限刷写完成 修改cfs csidriver中fsGroupPolicy设置成非File,或者删除掉[None]。 这样无论客户是否有配置fsgroup 都会直接skip掉。 pod会立即启动 当然，也可以不指定 fsGroup，这个是最简单的修复方式。 © vishon all right reserved，powered by GitbookUpdated at 2024-01-09 10:05:20 "},"prometheus/basic-knowledge-of-prometheus.html":{"url":"prometheus/basic-knowledge-of-prometheus.html","title":"【prometheus】prometheus入门知识概念","keywords":"","body":"prometheus简介 Prometheus是一个开源监控系统，它前身是SoundCloud的警告工具包。从2012年开始，许多公司和组织开始使用Prometheus。该项目的开发人员和用户社区非常活跃，越来越多的开发人员和用户参与到该项目中。目前它是一个独立的开源项目，且不依赖与任何公司。 为了强调这点和明确该项目治理结构，Prometheus在2016年继Kurberntes之后，加入了Cloud Native Computing Foundation。 特征 Prometheus的主要特征有： 多维度数据模型 灵活的查询语言 不依赖分布式存储，单个服务器节点是自主的 以HTTP方式，通过pull模型拉去时间序列数据 也通过中间网关支持push模型 通过服务发现或者静态配置，来发现目标服务对象 支持多种多样的图表和界面展示，grafana也支持它 组件 Prometheus生态包括了很多组件，它们中的一些是可选的： 主服务Prometheus Server负责抓取和存储时间序列数据 客户库负责检测应用程序代码 支持短生命周期的PUSH网关 基于Rails/SQL仪表盘构建器的GUI 多种导出工具，可以支持Prometheus存储数据转化为HAProxy、StatsD、Graphite等工具所需要的数据存储格式 警告管理器 命令行查询工具 其他各种支撑工具 多数Prometheus组件是Go语言写的，这使得这些组件很容易编译和部署。 架构 下面这张图说明了Prometheus的整体架构，以及生态中的一些组件作用: Prometheus服务，可以直接通过目标拉取数据，或者间接地通过中间网关拉取数据。它在本地存储抓取的所有数据，并通过一定规则进行清理和整理数据，并把得到的结果存储到新的时间序列中，PromQL和其他API可视化地展示收集的数据 适用场景 Prometheus在记录纯数字时间序列方面表现非常好。它既适用于面向服务器等硬件指标的监控，也适用于高动态的面向服务架构的监控。对于现在流行的微服务，Prometheus的多维度数据收集和数据筛选查询语言也是非常的强大。 Prometheus是为服务的可靠性而设计的，当服务出现故障时，它可以使你快速定位和诊断问题。它的搭建过程对硬件和服务没有很强的依赖关系。 不适用场景 Prometheus，它的价值在于可靠性，甚至在很恶劣的环境下，你都可以随时访问它和查看系统服务各种指标的统计信息。 如果你对统计数据需要100%的精确，它并不适用，例如：它不适用于实时计费系统 基本概念 job: Prometheus的采集任务由配置文件中一个个的Job组成，一个Job里包含该Job下的所有监控目标的公共配置，比如使用哪种服务发现去获取监控目标，比如抓取时使用的证书配置，请求参数配置等等。 target: 一个监控目标就是一个target，一个job通过服务发现会得到多个需要监控的target，其包含一些label用于描述target的一些属性。 relabel_configs: 每个job都可以配置一个或多个relabel_config，relabel_config会对target的label集合进行处理，可以根据label过滤一些target或者修改，增加，删除一些label。relabel_config过程发生在target开始进行采集之前,针对的是通过服务发现得到的label集合。 metrics_relabel_configs：每个job还可以配置一个或者多个metrics_relabel_config，其配置方式和relabel_configs一模一样，但是其用于处理的是从target采集到的数据中的label。也就是发送在采集之后。 series(序列)：一个series就是指标名+label集合。 head series：Prometheus会将近2小时的series缓存在内测中，称为head series。 指标类型 Prometheus定义了4中不同的指标类型(metric type)：Counter（计数器）、Gauge（仪表盘）、Histogram（直方图）、Summary（摘要） Counter: 一种累加的metric，如请求的个数，结束的任务数，出现的错误数等 Gauge: 常规的metric,如温度，可任意加减。其为瞬时的，与时间没有关系的，可以任意变化的数据。 Histogram: 柱状图，用于观察结果采样，分组及统计，如：请求持续时间，响应大小。其主要用于表示一段时间内对数据的采样，并能够对其指定区间及总数进行统计。根据统计区间计算 Summary: 类似Histogram，用于表示一段时间内数据采样结果，其直接存储quantile数据，而不是根据统计区间计算出来的。不需要计算，直接存储结果 promql PromQL (Prometheus Query Language) 是 Prometheus 自己开发的数据查询 DSL 语言。 结果类型 查询结果类型： 瞬时数据 (Instant vector): 包含一组时序，每个时序只有一个点，例如：http_requests_total 区间数据 (Range vector): 包含一组时序，每个时序有多个点，例如：http_requests_total[5m] 纯量数据 (Scalar): 纯量只有一个数字，没有时序，例如：count(http_requests_total) 匹配模式 PromQL支持使用=和!=两种完全匹配模式： 通过使用label=value可以选择那些标签满足表达式定义的时间序列； 反之使用label!=value则可以根据标签匹配排除时间序列； 除了使用完全匹配的方式对时间序列进行过滤以外，PromQL还可以支持使用正则表达式作为匹配条件，多个表达式之间使用|进行分离： 使用label=~regx表示选择那些标签符合正则表达式定义的时间序列； 反之使用label!~regx进行排除 时间位移 如果我们想查询，5分钟前的瞬时样本数据，或昨天一天的区间内的样本数据呢? 这个时候我们就可以使用位移操作，位移操作的关键字为offset。 可以使用offset时间位移操作： http_request_total{} offset 5m http_request_total{}[1d] offset 1d 数学运算符 PromQL支持的所有数学运算符如下所示： + (加法) - (减法) * (乘法) / (除法) % (求余) ^ (幂运算) node_memory_free_bytes_total / (1024 * 1024) 布尔运算符 Prometheus支持以下布尔运算符如下： == (相等) != (不相等) > (大于) >= (大于等于) (node_memory_bytes_total - node_memory_free_bytes_total) / node_memory_bytes_total > 0.95 布尔运算符的默认行为是对时序数据进行过滤。而在其它的情况下我们可能需要的是真正的布尔结果。例如，只需要知道当前模块的HTTP请求量是否>=1000，如果大于等于1000则返回1（true）否则返回0（false）。这时可以使用bool修饰符改变布尔运算的默认行为 2 == bool 2 # 结果为1 集合运算符 使用瞬时向量表达式能够获取到一个包含多个时间序列的集合，我们称为瞬时向量。 通过集合运算，可以在两个瞬时向量与瞬时向量之间进行相应的集合操作。目前，Prometheus支持以下集合运算符： and (并且) or (或者) unless (排除) vector1 and vector2 会产生一个由vector1的元素组成的新的向量。该向量包含vector1中完全匹配vector2中的元素组成。 vector1 or vector2 会产生一个新的向量，该向量包含vector1中所有的\b样本数据，以及vector2中没有与vector1匹配到的样本数据。 vector1 unless vector2 会产生一个新的向量，新向量中的元素由vector1中没有与vector2匹配的元素组成。 聚合操作符 sum (在维度上求和) max (在维度上求最大值) min (在维度上求最小值) avg (在维度上求平均值) stddev (求标准差) stdvar (求方差) count (统计向量元素的个数) count_values (统计相同数据值的元素数量) bottomk (样本值第k个最小值) topk (样本值第k个最大值) quantile (统计分位数) 这些操作符被用于聚合所有标签维度，或者通过without或者by子句来保留不同的维度。 ([parameter,] ) [without | by ()] [keep_common] parameter只能用于count_values, quantile, topk和bottomk。without移除结果向量中的标签集合，其他标签被保留输出。by关键字的作用正好相反，即使它们的标签值在向量的所有元素之间。keep_common子句允许保留额外的标签（在元素之间相同，但不在by子句中的标签） count_values对每个唯一的样本值输出一个时间序列。每个时间序列都附加一个标签。这个标签的名字有聚合参数指定，同时这个标签值是唯一的样本值。每一个时间序列值是结果样本值出现的次数。 topk和bottomk与其他输入样本子集聚合不同，返回的结果中包括原始标签。by和without仅仅用在输入向量的桶中 如果度量指标名称http_requests_total包含由group, application, instance的标签组成的时间序列数据，我们可以通过以下方式计算去除instance标签的http请求总数： sum(http_requests_total) without (instance) 如果我们对所有应用程序的http请求总数，我们可以简单地写下： sum(http_requests_total) 统计每个编译版本的二进制文件数量，我们可以如下写： count_values(\"version\", build_version) 通过所有实例，获取http请求第5个最大值，我们可以简单地写下： topk(5, http_requests_total) 函数 函数一般使用较少，具体参考官方文档https://prometheus.io/docs/prometheus/latest/querying/functions/ 向量匹配 向量之间的匹配是指右边向量中的每一个元素，在左边向量中也存在。这里有两种基本匹配行为特征： 一对一，找到这个操作符的两边向量元素的相同元素。默认情况下，操作符的格式是vector1 [operate] vector2。如果它们有相同的标签和值，则表示相匹配。ingoring关键字是指，向量匹配时，可以忽略指定标签。on关键字是指，在指定标签上进行匹配。格式如下所示： [vector expr] [bin-op] ignoring([label list]) [vector expr] [vector expr] [bin-op] on([lable list]) [vector expr] 例如样本数据： method_code:http_errors:rate5m{method=\"get\", code=\"500\"} 24 method_code:http_errors:rate5m{method=\"get\", code=\"404\"} 30 method_code:http_errors:rate5m{method=\"put\", code=\"501\"} 3 method_code:http_errors:rate5m{method=\"post\", code=\"404\"} 21 method:http_requests:rate5m{method=\"get\"} 600 method:http_requests:rate5m{method=\"delete\"} 34 method:http_requests:rate5m{method=\"post\"} 120 查询例子： method_code:http_errors:rate5m{code=\"500\"} / ignoring(code) method:http_requests:rate5m 两个向量之间的除法操作运算的向量结果是，每一个向量样本http请求方法标签值是500，且在过去5分钟的运算值。如果没有忽略code=\"500\"的标签，这里不能匹配到向量样本数据。两个向量的请求方法是put和delete的样本数据不会出现在结果列表中 {method=\"get\"} 0.04 // 24 / 600 {method=\"post\"} 0.05 // 6 / 120 多对一和一对多的匹配，是指向量元素中的一个样本数据匹配标签到了多个样本数据标签。这里必须直接指定两个修饰符group_left或者group_right， 左或者右决定了哪边的向量具有较高的子集。 ignoring() group_left() ignoring() group_right() on() group_left() on() group_right() 这个group带标签的修饰符标签列表包含了“一对多”中的“一”一侧的额外标签。对于on标签只能是这些列表中的一个。结果向量中的每一个时间序列数据都是唯一的。 group修饰符只能被用在比较操作符和算术运算符。 method_code:http_errors:rate5m / ignoring(code) group_left method:http_requests:rate5m 在这个例子中，左向量的标签数量多于左边向量的标签数量，所以我们使用group_left。右边向量的时间序列元素匹配左边的所有相同method标签: {method=\"get\", code=\"500\"} 0.04 // 24 /600 {method=\"get\", code=\"404\"} 0.05 // 30 /600 {method=\"post\", code=\"500\"} 0.05 // 6 /600 {method=\"post\", code=\"404\"} 0.175 // 21 /600 多对一和一对多匹配应该更多地被谨慎使用。经常使用ignoring(\\)输出想要的结果。 api访问prometheus k8s集群内我们可以通过kubectl get --raw / 获取根下的接口，然后层层下探，直到找到需要的监控数据。 pod：kubectl get --raw /api/v1/nodes/10.168.1.4/proxy/metrics/cadvisor node: kubectl get --raw /api/v1/nodes/10.168.1.4:9100/proxy/metrics kubelet: kubectl get --raw /api/v1/nodes/10.168.1.4:10250/proxy/metrics 用curl请求prometheus的地址来进行查询，查询所有up的job curl http://10.0.0.234:9090/api/v1/query?query=up http api来查询prometheus可以参考官方文档https://prometheus.io/docs/prometheus/latest/querying/api/ © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"prometheus/Prometheus-collects-Docker-Engine-Metrics.html":{"url":"prometheus/Prometheus-collects-Docker-Engine-Metrics.html","title":"【prometheus】Prometheus采集Docker Engine Metrics","keywords":"","body":"Prometheus采集Docker Engine Metrics 从Docker 1.13开始引入了一个特性：将Docker Engine Metrix以Prometheus语法暴露出来，使得Prometheus服务器可以收集并展示。 开启节点docker mertrics 节点的docker的daemon文件中加上如下配置，就可以开启 { \"metrics-addr\" : \"0.0.0.0:9323\", \"experimental\" : true } prometheus配置Rawjob 配置rawjob采集数据到prometheus scrape_configs: - job_name: docker-daemon honor_timestamps: true metrics_path: /metrics scheme: http kubernetes_sd_configs: - role: node relabel_configs: - separator: ; regex: __meta_kubernetes_node_label_(.+) replacement: $1 action: labelmap - source_labels: [__address__] separator: ; regex: ([^:;]+):(\\d+) target_label: __address__ replacement: ${1}:9323 action: replace 查看指标 通过接口查看数据指标，根据指标编写promsql进行数据的展示和聚合 [root@VM-0-2-centos ~]# curl http://10.0.0.2:9323/metrics # HELP builder_builds_failed_total Number of failed image builds # TYPE builder_builds_failed_total counter builder_builds_failed_total{reason=\"build_canceled\"} 0 builder_builds_failed_total{reason=\"build_target_not_reachable_error\"} 0 builder_builds_failed_total{reason=\"command_not_supported_error\"} 0 builder_builds_failed_total{reason=\"dockerfile_empty_error\"} 0 builder_builds_failed_total{reason=\"dockerfile_syntax_error\"} 0 builder_builds_failed_total{reason=\"error_processing_commands_error\"} 0 builder_builds_failed_total{reason=\"missing_onbuild_arguments_error\"} 0 builder_builds_failed_total{reason=\"unknown_instruction_error\"} 0 # HELP builder_builds_triggered_total Number of triggered image builds # TYPE builder_builds_triggered_total counter builder_builds_triggered_total 0 # HELP engine_daemon_container_actions_seconds The number of seconds it takes to process each container action # TYPE engine_daemon_container_actions_seconds histogram engine_daemon_container_actions_seconds_bucket{action=\"changes\",le=\"0.005\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"changes\",le=\"0.01\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"changes\",le=\"0.025\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"changes\",le=\"0.05\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"changes\",le=\"0.1\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"changes\",le=\"0.25\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"changes\",le=\"0.5\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"changes\",le=\"1\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"changes\",le=\"2.5\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"changes\",le=\"5\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"changes\",le=\"10\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"changes\",le=\"+Inf\"} 1 engine_daemon_container_actions_seconds_sum{action=\"changes\"} 0 engine_daemon_container_actions_seconds_count{action=\"changes\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"commit\",le=\"0.005\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"commit\",le=\"0.01\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"commit\",le=\"0.025\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"commit\",le=\"0.05\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"commit\",le=\"0.1\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"commit\",le=\"0.25\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"commit\",le=\"0.5\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"commit\",le=\"1\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"commit\",le=\"2.5\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"commit\",le=\"5\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"commit\",le=\"10\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"commit\",le=\"+Inf\"} 1 engine_daemon_container_actions_seconds_sum{action=\"commit\"} 0 engine_daemon_container_actions_seconds_count{action=\"commit\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"create\",le=\"0.005\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"create\",le=\"0.01\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"create\",le=\"0.025\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"create\",le=\"0.05\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"create\",le=\"0.1\"} 2 engine_daemon_container_actions_seconds_bucket{action=\"create\",le=\"0.25\"} 5 engine_daemon_container_actions_seconds_bucket{action=\"create\",le=\"0.5\"} 5 engine_daemon_container_actions_seconds_bucket{action=\"create\",le=\"1\"} 5 engine_daemon_container_actions_seconds_bucket{action=\"create\",le=\"2.5\"} 5 engine_daemon_container_actions_seconds_bucket{action=\"create\",le=\"5\"} 5 engine_daemon_container_actions_seconds_bucket{action=\"create\",le=\"10\"} 5 engine_daemon_container_actions_seconds_bucket{action=\"create\",le=\"+Inf\"} 5 engine_daemon_container_actions_seconds_sum{action=\"create\"} 0.430104514 engine_daemon_container_actions_seconds_count{action=\"create\"} 5 engine_daemon_container_actions_seconds_bucket{action=\"delete\",le=\"0.005\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"delete\",le=\"0.01\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"delete\",le=\"0.025\"} 3 engine_daemon_container_actions_seconds_bucket{action=\"delete\",le=\"0.05\"} 6 engine_daemon_container_actions_seconds_bucket{action=\"delete\",le=\"0.1\"} 6 engine_daemon_container_actions_seconds_bucket{action=\"delete\",le=\"0.25\"} 6 engine_daemon_container_actions_seconds_bucket{action=\"delete\",le=\"0.5\"} 7 engine_daemon_container_actions_seconds_bucket{action=\"delete\",le=\"1\"} 7 engine_daemon_container_actions_seconds_bucket{action=\"delete\",le=\"2.5\"} 7 engine_daemon_container_actions_seconds_bucket{action=\"delete\",le=\"5\"} 7 engine_daemon_container_actions_seconds_bucket{action=\"delete\",le=\"10\"} 7 engine_daemon_container_actions_seconds_bucket{action=\"delete\",le=\"+Inf\"} 7 engine_daemon_container_actions_seconds_sum{action=\"delete\"} 0.45672375400000004 engine_daemon_container_actions_seconds_count{action=\"delete\"} 7 engine_daemon_container_actions_seconds_bucket{action=\"start\",le=\"0.005\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"start\",le=\"0.01\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"start\",le=\"0.025\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"start\",le=\"0.05\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"start\",le=\"0.1\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"start\",le=\"0.25\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"start\",le=\"0.5\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"start\",le=\"1\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"start\",le=\"2.5\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"start\",le=\"5\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"start\",le=\"10\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"start\",le=\"+Inf\"} 1 engine_daemon_container_actions_seconds_sum{action=\"start\"} 0 engine_daemon_container_actions_seconds_count{action=\"start\"} 1 # HELP engine_daemon_container_states_containers The count of containers in various states # TYPE engine_daemon_container_states_containers gauge engine_daemon_container_states_containers{state=\"paused\"} 0 engine_daemon_container_states_containers{state=\"running\"} 143 engine_daemon_container_states_containers{state=\"stopped\"} 19 # HELP engine_daemon_engine_cpus_cpus The number of cpus that the host system of the engine has # TYPE engine_daemon_engine_cpus_cpus gauge engine_daemon_engine_cpus_cpus 8 # HELP engine_daemon_engine_info The information related to the engine and the OS it is running on # TYPE engine_daemon_engine_info gauge engine_daemon_engine_info{architecture=\"x86_64\",commit=\"9d988398e7\",daemon_id=\"YCLF:XBU5:6B75:3CYI:YM2J:4ES4:JH7P:XAUT:Q2NK:SR6B:3LQP:XOVR\",graphdriver=\"overlay2\",kernel=\"3.10.0-1062.1.2.el7.x86_64\",os=\"CentOS Linux 7 (Core)\",os_type=\"linux\",version=\"19.03.9\"} 1 # HELP engine_daemon_engine_memory_bytes The number of bytes of memory that the host system of the engine has # TYPE engine_daemon_engine_memory_bytes gauge engine_daemon_engine_memory_bytes 1.655640064e+10 # HELP engine_daemon_events_subscribers_total The number of current subscribers to events # TYPE engine_daemon_events_subscribers_total gauge engine_daemon_events_subscribers_total 0 # HELP engine_daemon_events_total The number of events logged # TYPE engine_daemon_events_total counter engine_daemon_events_total 3166 # HELP engine_daemon_health_checks_failed_total The total number of failed health checks # TYPE engine_daemon_health_checks_failed_total counter engine_daemon_health_checks_failed_total 0 # HELP engine_daemon_health_checks_total The total number of health checks # TYPE engine_daemon_health_checks_total counter engine_daemon_health_checks_total 0 # HELP engine_daemon_network_actions_seconds The number of seconds it takes to process each network action # TYPE engine_daemon_network_actions_seconds histogram engine_daemon_network_actions_seconds_bucket{action=\"release\",le=\"0.005\"} 0 engine_daemon_network_actions_seconds_bucket{action=\"release\",le=\"0.01\"} 0 engine_daemon_network_actions_seconds_bucket{action=\"release\",le=\"0.025\"} 0 engine_daemon_network_actions_seconds_bucket{action=\"release\",le=\"0.05\"} 0 engine_daemon_network_actions_seconds_bucket{action=\"release\",le=\"0.1\"} 0 engine_daemon_network_actions_seconds_bucket{action=\"release\",le=\"0.25\"} 1 engine_daemon_network_actions_seconds_bucket{action=\"release\",le=\"0.5\"} 1 engine_daemon_network_actions_seconds_bucket{action=\"release\",le=\"1\"} 1 engine_daemon_network_actions_seconds_bucket{action=\"release\",le=\"2.5\"} 1 engine_daemon_network_actions_seconds_bucket{action=\"release\",le=\"5\"} 1 engine_daemon_network_actions_seconds_bucket{action=\"release\",le=\"10\"} 1 engine_daemon_network_actions_seconds_bucket{action=\"release\",le=\"+Inf\"} 1 engine_daemon_network_actions_seconds_sum{action=\"release\"} 0.109992905 engine_daemon_network_actions_seconds_count{action=\"release\"} 1 # HELP etcd_debugging_snap_save_marshalling_duration_seconds The marshalling cost distributions of save called by snapshot. # TYPE etcd_debugging_snap_save_marshalling_duration_seconds histogram etcd_debugging_snap_save_marshalling_duration_seconds_bucket{le=\"0.001\"} 0 etcd_debugging_snap_save_marshalling_duration_seconds_bucket{le=\"0.002\"} 0 etcd_debugging_snap_save_marshalling_duration_seconds_bucket{le=\"0.004\"} 0 etcd_debugging_snap_save_marshalling_duration_seconds_bucket{le=\"0.008\"} 0 etcd_debugging_snap_save_marshalling_duration_seconds_bucket{le=\"0.016\"} 0 etcd_debugging_snap_save_marshalling_duration_seconds_bucket{le=\"0.032\"} 0 etcd_debugging_snap_save_marshalling_duration_seconds_bucket{le=\"0.064\"} 0 etcd_debugging_snap_save_marshalling_duration_seconds_bucket{le=\"0.128\"} 0 etcd_debugging_snap_save_marshalling_duration_seconds_bucket{le=\"0.256\"} 0 etcd_debugging_snap_save_marshalling_duration_seconds_bucket{le=\"0.512\"} 0 etcd_debugging_snap_save_marshalling_duration_seconds_bucket{le=\"1.024\"} 0 etcd_debugging_snap_save_marshalling_duration_seconds_bucket{le=\"2.048\"} 0 etcd_debugging_snap_save_marshalling_duration_seconds_bucket{le=\"4.096\"} 0 etcd_debugging_snap_save_marshalling_duration_seconds_bucket{le=\"8.192\"} 0 etcd_debugging_snap_save_marshalling_duration_seconds_bucket{le=\"+Inf\"} 0 etcd_debugging_snap_save_marshalling_duration_seconds_sum 0 etcd_debugging_snap_save_marshalling_duration_seconds_count 0 # HELP etcd_debugging_snap_save_total_duration_seconds The total latency distributions of save called by snapshot. # TYPE etcd_debugging_snap_save_total_duration_seconds histogram etcd_debugging_snap_save_total_duration_seconds_bucket{le=\"0.001\"} 0 etcd_debugging_snap_save_total_duration_seconds_bucket{le=\"0.002\"} 0 etcd_debugging_snap_save_total_duration_seconds_bucket{le=\"0.004\"} 0 etcd_debugging_snap_save_total_duration_seconds_bucket{le=\"0.008\"} 0 etcd_debugging_snap_save_total_duration_seconds_bucket{le=\"0.016\"} 0 etcd_debugging_snap_save_total_duration_seconds_bucket{le=\"0.032\"} 0 etcd_debugging_snap_save_total_duration_seconds_bucket{le=\"0.064\"} 0 etcd_debugging_snap_save_total_duration_seconds_bucket{le=\"0.128\"} 0 etcd_debugging_snap_save_total_duration_seconds_bucket{le=\"0.256\"} 0 etcd_debugging_snap_save_total_duration_seconds_bucket{le=\"0.512\"} 0 etcd_debugging_snap_save_total_duration_seconds_bucket{le=\"1.024\"} 0 etcd_debugging_snap_save_total_duration_seconds_bucket{le=\"2.048\"} 0 etcd_debugging_snap_save_total_duration_seconds_bucket{le=\"4.096\"} 0 etcd_debugging_snap_save_total_duration_seconds_bucket{le=\"8.192\"} 0 etcd_debugging_snap_save_total_duration_seconds_bucket{le=\"+Inf\"} 0 etcd_debugging_snap_save_total_duration_seconds_sum 0 etcd_debugging_snap_save_total_duration_seconds_count 0 # HELP etcd_disk_wal_fsync_duration_seconds The latency distributions of fsync called by wal. # TYPE etcd_disk_wal_fsync_duration_seconds histogram etcd_disk_wal_fsync_duration_seconds_bucket{le=\"0.001\"} 0 etcd_disk_wal_fsync_duration_seconds_bucket{le=\"0.002\"} 0 etcd_disk_wal_fsync_duration_seconds_bucket{le=\"0.004\"} 0 etcd_disk_wal_fsync_duration_seconds_bucket{le=\"0.008\"} 0 etcd_disk_wal_fsync_duration_seconds_bucket{le=\"0.016\"} 0 etcd_disk_wal_fsync_duration_seconds_bucket{le=\"0.032\"} 0 etcd_disk_wal_fsync_duration_seconds_bucket{le=\"0.064\"} 0 etcd_disk_wal_fsync_duration_seconds_bucket{le=\"0.128\"} 0 etcd_disk_wal_fsync_duration_seconds_bucket{le=\"0.256\"} 0 etcd_disk_wal_fsync_duration_seconds_bucket{le=\"0.512\"} 0 etcd_disk_wal_fsync_duration_seconds_bucket{le=\"1.024\"} 0 etcd_disk_wal_fsync_duration_seconds_bucket{le=\"2.048\"} 0 etcd_disk_wal_fsync_duration_seconds_bucket{le=\"4.096\"} 0 etcd_disk_wal_fsync_duration_seconds_bucket{le=\"8.192\"} 0 etcd_disk_wal_fsync_duration_seconds_bucket{le=\"+Inf\"} 0 etcd_disk_wal_fsync_duration_seconds_sum 0 etcd_disk_wal_fsync_duration_seconds_count 0 # HELP etcd_snap_db_fsync_duration_seconds The latency distributions of fsyncing .snap.db file # TYPE etcd_snap_db_fsync_duration_seconds histogram etcd_snap_db_fsync_duration_seconds_bucket{le=\"0.001\"} 0 etcd_snap_db_fsync_duration_seconds_bucket{le=\"0.002\"} 0 etcd_snap_db_fsync_duration_seconds_bucket{le=\"0.004\"} 0 etcd_snap_db_fsync_duration_seconds_bucket{le=\"0.008\"} 0 etcd_snap_db_fsync_duration_seconds_bucket{le=\"0.016\"} 0 etcd_snap_db_fsync_duration_seconds_bucket{le=\"0.032\"} 0 etcd_snap_db_fsync_duration_seconds_bucket{le=\"0.064\"} 0 etcd_snap_db_fsync_duration_seconds_bucket{le=\"0.128\"} 0 etcd_snap_db_fsync_duration_seconds_bucket{le=\"0.256\"} 0 etcd_snap_db_fsync_duration_seconds_bucket{le=\"0.512\"} 0 etcd_snap_db_fsync_duration_seconds_bucket{le=\"1.024\"} 0 etcd_snap_db_fsync_duration_seconds_bucket{le=\"2.048\"} 0 etcd_snap_db_fsync_duration_seconds_bucket{le=\"4.096\"} 0 etcd_snap_db_fsync_duration_seconds_bucket{le=\"8.192\"} 0 etcd_snap_db_fsync_duration_seconds_bucket{le=\"+Inf\"} 0 etcd_snap_db_fsync_duration_seconds_sum 0 etcd_snap_db_fsync_duration_seconds_count 0 # HELP etcd_snap_db_save_total_duration_seconds The total latency distributions of v3 snapshot save # TYPE etcd_snap_db_save_total_duration_seconds histogram etcd_snap_db_save_total_duration_seconds_bucket{le=\"0.1\"} 0 etcd_snap_db_save_total_duration_seconds_bucket{le=\"0.2\"} 0 etcd_snap_db_save_total_duration_seconds_bucket{le=\"0.4\"} 0 etcd_snap_db_save_total_duration_seconds_bucket{le=\"0.8\"} 0 etcd_snap_db_save_total_duration_seconds_bucket{le=\"1.6\"} 0 etcd_snap_db_save_total_duration_seconds_bucket{le=\"3.2\"} 0 etcd_snap_db_save_total_duration_seconds_bucket{le=\"6.4\"} 0 etcd_snap_db_save_total_duration_seconds_bucket{le=\"12.8\"} 0 etcd_snap_db_save_total_duration_seconds_bucket{le=\"25.6\"} 0 etcd_snap_db_save_total_duration_seconds_bucket{le=\"51.2\"} 0 etcd_snap_db_save_total_duration_seconds_bucket{le=\"+Inf\"} 0 etcd_snap_db_save_total_duration_seconds_sum 0 etcd_snap_db_save_total_duration_seconds_count 0 # HELP go_gc_duration_seconds A summary of the GC invocation durations. # TYPE go_gc_duration_seconds summary go_gc_duration_seconds{quantile=\"0\"} 1.0529e-05 go_gc_duration_seconds{quantile=\"0.25\"} 1.4528e-05 go_gc_duration_seconds{quantile=\"0.5\"} 1.7833e-05 go_gc_duration_seconds{quantile=\"0.75\"} 2.4036e-05 go_gc_duration_seconds{quantile=\"1\"} 0.004878993 go_gc_duration_seconds_sum 0.023893318 go_gc_duration_seconds_count 397 # HELP go_goroutines Number of goroutines that currently exist. # TYPE go_goroutines gauge go_goroutines 647 # HELP go_memstats_alloc_bytes Number of bytes allocated and still in use. # TYPE go_memstats_alloc_bytes gauge go_memstats_alloc_bytes 6.322868e+07 # HELP go_memstats_alloc_bytes_total Total number of bytes allocated, even if freed. # TYPE go_memstats_alloc_bytes_total counter go_memstats_alloc_bytes_total 1.2068908064e+10 # HELP go_memstats_buck_hash_sys_bytes Number of bytes used by the profiling bucket hash table. # TYPE go_memstats_buck_hash_sys_bytes gauge go_memstats_buck_hash_sys_bytes 1.984451e+06 # HELP go_memstats_frees_total Total number of frees. # TYPE go_memstats_frees_total counter go_memstats_frees_total 9.9325482e+07 # HELP go_memstats_gc_sys_bytes Number of bytes used for garbage collection system metadata. # TYPE go_memstats_gc_sys_bytes gauge go_memstats_gc_sys_bytes 4.907008e+06 # HELP go_memstats_heap_alloc_bytes Number of heap bytes allocated and still in use. # TYPE go_memstats_heap_alloc_bytes gauge go_memstats_heap_alloc_bytes 6.322868e+07 # HELP go_memstats_heap_idle_bytes Number of heap bytes waiting to be used. # TYPE go_memstats_heap_idle_bytes gauge go_memstats_heap_idle_bytes 5.7245696e+07 # HELP go_memstats_heap_inuse_bytes Number of heap bytes that are in use. # TYPE go_memstats_heap_inuse_bytes gauge go_memstats_heap_inuse_bytes 6.8386816e+07 # HELP go_memstats_heap_objects Number of allocated objects. # TYPE go_memstats_heap_objects gauge go_memstats_heap_objects 433408 # HELP go_memstats_heap_released_bytes_total Total number of heap bytes released to OS. # TYPE go_memstats_heap_released_bytes_total counter go_memstats_heap_released_bytes_total 5.0003968e+07 # HELP go_memstats_heap_sys_bytes Number of heap bytes obtained from system. # TYPE go_memstats_heap_sys_bytes gauge go_memstats_heap_sys_bytes 1.25632512e+08 # HELP go_memstats_last_gc_time_seconds Number of seconds since 1970 of last garbage collection. # TYPE go_memstats_last_gc_time_seconds gauge go_memstats_last_gc_time_seconds 1.6289054392397587e+09 # HELP go_memstats_lookups_total Total number of pointer lookups. # TYPE go_memstats_lookups_total counter go_memstats_lookups_total 0 # HELP go_memstats_mallocs_total Total number of mallocs. # TYPE go_memstats_mallocs_total counter go_memstats_mallocs_total 9.975889e+07 # HELP go_memstats_mcache_inuse_bytes Number of bytes in use by mcache structures. # TYPE go_memstats_mcache_inuse_bytes gauge go_memstats_mcache_inuse_bytes 13888 # HELP go_memstats_mcache_sys_bytes Number of bytes used for mcache structures obtained from system. # TYPE go_memstats_mcache_sys_bytes gauge go_memstats_mcache_sys_bytes 16384 # HELP go_memstats_mspan_inuse_bytes Number of bytes in use by mspan structures. # TYPE go_memstats_mspan_inuse_bytes gauge go_memstats_mspan_inuse_bytes 814776 # HELP go_memstats_mspan_sys_bytes Number of bytes used for mspan structures obtained from system. # TYPE go_memstats_mspan_sys_bytes gauge go_memstats_mspan_sys_bytes 884736 # HELP go_memstats_next_gc_bytes Number of heap bytes when next garbage collection will take place. # TYPE go_memstats_next_gc_bytes gauge go_memstats_next_gc_bytes 7.104408e+07 # HELP go_memstats_other_sys_bytes Number of bytes used for other system allocations. # TYPE go_memstats_other_sys_bytes gauge go_memstats_other_sys_bytes 1.384757e+06 # HELP go_memstats_stack_inuse_bytes Number of bytes in use by the stack allocator. # TYPE go_memstats_stack_inuse_bytes gauge go_memstats_stack_inuse_bytes 8.585216e+06 # HELP go_memstats_stack_sys_bytes Number of bytes obtained from system for stack allocator. # TYPE go_memstats_stack_sys_bytes gauge go_memstats_stack_sys_bytes 8.585216e+06 # HELP go_memstats_sys_bytes Number of bytes obtained by system. Sum of all system allocations. # TYPE go_memstats_sys_bytes gauge go_memstats_sys_bytes 1.43395064e+08 # HELP http_request_duration_microseconds The HTTP request latencies in microseconds. # TYPE http_request_duration_microseconds summary http_request_duration_microseconds{handler=\"prometheus\",quantile=\"0.5\"} 2470.569 http_request_duration_microseconds{handler=\"prometheus\",quantile=\"0.9\"} 2847.254 http_request_duration_microseconds{handler=\"prometheus\",quantile=\"0.99\"} 8325.889 http_request_duration_microseconds_sum{handler=\"prometheus\"} 241802.613 http_request_duration_microseconds_count{handler=\"prometheus\"} 92 # HELP http_request_size_bytes The HTTP request sizes in bytes. # TYPE http_request_size_bytes summary http_request_size_bytes{handler=\"prometheus\",quantile=\"0.5\"} 238 http_request_size_bytes{handler=\"prometheus\",quantile=\"0.9\"} 238 http_request_size_bytes{handler=\"prometheus\",quantile=\"0.99\"} 439 http_request_size_bytes_sum{handler=\"prometheus\"} 23035 http_request_size_bytes_count{handler=\"prometheus\"} 92 # HELP http_requests_total Total number of HTTP requests made. # TYPE http_requests_total counter http_requests_total{code=\"200\",handler=\"prometheus\",method=\"get\"} 92 # HELP http_response_size_bytes The HTTP response sizes in bytes. # TYPE http_response_size_bytes summary http_response_size_bytes{handler=\"prometheus\",quantile=\"0.5\"} 4159 http_response_size_bytes{handler=\"prometheus\",quantile=\"0.9\"} 4167 http_response_size_bytes{handler=\"prometheus\",quantile=\"0.99\"} 4174 http_response_size_bytes_sum{handler=\"prometheus\"} 381997 http_response_size_bytes_count{handler=\"prometheus\"} 92 # HELP logger_log_entries_size_greater_than_buffer_total Number of log entries which are larger than the log buffer # TYPE logger_log_entries_size_greater_than_buffer_total counter logger_log_entries_size_greater_than_buffer_total 3901 # HELP logger_log_read_operations_failed_total Number of log reads from container stdio that failed # TYPE logger_log_read_operations_failed_total counter logger_log_read_operations_failed_total 0 # HELP logger_log_write_operations_failed_total Number of log write operations that failed # TYPE logger_log_write_operations_failed_total counter logger_log_write_operations_failed_total 0 # HELP process_cpu_seconds_total Total user and system CPU time spent in seconds. # TYPE process_cpu_seconds_total counter process_cpu_seconds_total 105.94 # HELP process_max_fds Maximum number of open file descriptors. # TYPE process_max_fds gauge process_max_fds 1.048576e+06 # HELP process_open_fds Number of open file descriptors. # TYPE process_open_fds gauge process_open_fds 808 # HELP process_resident_memory_bytes Resident memory size in bytes. # TYPE process_resident_memory_bytes gauge process_resident_memory_bytes 8.4279296e+07 # HELP process_start_time_seconds Start time of the process since unix epoch in seconds. # TYPE process_start_time_seconds gauge process_start_time_seconds 1.62890400767e+09 # HELP process_virtual_memory_bytes Virtual memory size in bytes. # TYPE process_virtual_memory_bytes gauge process_virtual_memory_bytes 5.925912576e+09 # HELP swarm_dispatcher_scheduling_delay_seconds Scheduling delay is the time a task takes to go from NEW to RUNNING state. # TYPE swarm_dispatcher_scheduling_delay_seconds histogram swarm_dispatcher_scheduling_delay_seconds_bucket{le=\"0.005\"} 0 swarm_dispatcher_scheduling_delay_seconds_bucket{le=\"0.01\"} 0 swarm_dispatcher_scheduling_delay_seconds_bucket{le=\"0.025\"} 0 swarm_dispatcher_scheduling_delay_seconds_bucket{le=\"0.05\"} 0 swarm_dispatcher_scheduling_delay_seconds_bucket{le=\"0.1\"} 0 swarm_dispatcher_scheduling_delay_seconds_bucket{le=\"0.25\"} 0 swarm_dispatcher_scheduling_delay_seconds_bucket{le=\"0.5\"} 0 swarm_dispatcher_scheduling_delay_seconds_bucket{le=\"1\"} 0 swarm_dispatcher_scheduling_delay_seconds_bucket{le=\"2.5\"} 0 swarm_dispatcher_scheduling_delay_seconds_bucket{le=\"5\"} 0 swarm_dispatcher_scheduling_delay_seconds_bucket{le=\"10\"} 0 swarm_dispatcher_scheduling_delay_seconds_bucket{le=\"+Inf\"} 0 swarm_dispatcher_scheduling_delay_seconds_sum 0 swarm_dispatcher_scheduling_delay_seconds_count 0 # HELP swarm_manager_configs_total The number of configs in the cluster object store # TYPE swarm_manager_configs_total gauge swarm_manager_configs_total 0 # HELP swarm_manager_leader Indicates if this manager node is a leader # TYPE swarm_manager_leader gauge swarm_manager_leader 0 # HELP swarm_manager_networks_total The number of networks in the cluster object store # TYPE swarm_manager_networks_total gauge swarm_manager_networks_total 0 # HELP swarm_manager_nodes The number of nodes # TYPE swarm_manager_nodes gauge swarm_manager_nodes{state=\"disconnected\"} 0 swarm_manager_nodes{state=\"down\"} 0 swarm_manager_nodes{state=\"ready\"} 0 swarm_manager_nodes{state=\"unknown\"} 0 # HELP swarm_manager_secrets_total The number of secrets in the cluster object store # TYPE swarm_manager_secrets_total gauge swarm_manager_secrets_total 0 # HELP swarm_manager_services_total The number of services in the cluster object store # TYPE swarm_manager_services_total gauge swarm_manager_services_total 0 # HELP swarm_manager_tasks_total The number of tasks in the cluster object store # TYPE swarm_manager_tasks_total gauge swarm_manager_tasks_total{state=\"accepted\"} 0 swarm_manager_tasks_total{state=\"assigned\"} 0 swarm_manager_tasks_total{state=\"complete\"} 0 swarm_manager_tasks_total{state=\"failed\"} 0 swarm_manager_tasks_total{state=\"new\"} 0 swarm_manager_tasks_total{state=\"orphaned\"} 0 swarm_manager_tasks_total{state=\"pending\"} 0 swarm_manager_tasks_total{state=\"preparing\"} 0 swarm_manager_tasks_total{state=\"ready\"} 0 swarm_manager_tasks_total{state=\"rejected\"} 0 swarm_manager_tasks_total{state=\"remove\"} 0 swarm_manager_tasks_total{state=\"running\"} 0 swarm_manager_tasks_total{state=\"shutdown\"} 0 swarm_manager_tasks_total{state=\"starting\"} 0 # HELP swarm_node_manager Whether this node is a manager or not # TYPE swarm_node_manager gauge swarm_node_manager 0 # HELP swarm_raft_snapshot_latency_seconds Raft snapshot create latency. # TYPE swarm_raft_snapshot_latency_seconds histogram swarm_raft_snapshot_latency_seconds_bucket{le=\"0.005\"} 0 swarm_raft_snapshot_latency_seconds_bucket{le=\"0.01\"} 0 swarm_raft_snapshot_latency_seconds_bucket{le=\"0.025\"} 0 swarm_raft_snapshot_latency_seconds_bucket{le=\"0.05\"} 0 swarm_raft_snapshot_latency_seconds_bucket{le=\"0.1\"} 0 swarm_raft_snapshot_latency_seconds_bucket{le=\"0.25\"} 0 swarm_raft_snapshot_latency_seconds_bucket{le=\"0.5\"} 0 swarm_raft_snapshot_latency_seconds_bucket{le=\"1\"} 0 swarm_raft_snapshot_latency_seconds_bucket{le=\"2.5\"} 0 swarm_raft_snapshot_latency_seconds_bucket{le=\"5\"} 0 swarm_raft_snapshot_latency_seconds_bucket{le=\"10\"} 0 swarm_raft_snapshot_latency_seconds_bucket{le=\"+Inf\"} 0 swarm_raft_snapshot_latency_seconds_sum 0 swarm_raft_snapshot_latency_seconds_count 0 # HELP swarm_raft_transaction_latency_seconds Raft transaction latency. # TYPE swarm_raft_transaction_latency_seconds histogram swarm_raft_transaction_latency_seconds_bucket{le=\"0.005\"} 0 swarm_raft_transaction_latency_seconds_bucket{le=\"0.01\"} 0 swarm_raft_transaction_latency_seconds_bucket{le=\"0.025\"} 0 swarm_raft_transaction_latency_seconds_bucket{le=\"0.05\"} 0 swarm_raft_transaction_latency_seconds_bucket{le=\"0.1\"} 0 swarm_raft_transaction_latency_seconds_bucket{le=\"0.25\"} 0 swarm_raft_transaction_latency_seconds_bucket{le=\"0.5\"} 0 swarm_raft_transaction_latency_seconds_bucket{le=\"1\"} 0 swarm_raft_transaction_latency_seconds_bucket{le=\"2.5\"} 0 swarm_raft_transaction_latency_seconds_bucket{le=\"5\"} 0 swarm_raft_transaction_latency_seconds_bucket{le=\"10\"} 0 swarm_raft_transaction_latency_seconds_bucket{le=\"+Inf\"} 0 swarm_raft_transaction_latency_seconds_sum 0 swarm_raft_transaction_latency_seconds_count 0 # HELP swarm_store_batch_latency_seconds Raft store batch latency. # TYPE swarm_store_batch_latency_seconds histogram swarm_store_batch_latency_seconds_bucket{le=\"0.005\"} 0 swarm_store_batch_latency_seconds_bucket{le=\"0.01\"} 0 swarm_store_batch_latency_seconds_bucket{le=\"0.025\"} 0 swarm_store_batch_latency_seconds_bucket{le=\"0.05\"} 0 swarm_store_batch_latency_seconds_bucket{le=\"0.1\"} 0 swarm_store_batch_latency_seconds_bucket{le=\"0.25\"} 0 swarm_store_batch_latency_seconds_bucket{le=\"0.5\"} 0 swarm_store_batch_latency_seconds_bucket{le=\"1\"} 0 swarm_store_batch_latency_seconds_bucket{le=\"2.5\"} 0 swarm_store_batch_latency_seconds_bucket{le=\"5\"} 0 swarm_store_batch_latency_seconds_bucket{le=\"10\"} 0 swarm_store_batch_latency_seconds_bucket{le=\"+Inf\"} 0 swarm_store_batch_latency_seconds_sum 0 swarm_store_batch_latency_seconds_count 0 # HELP swarm_store_lookup_latency_seconds Raft store read latency. # TYPE swarm_store_lookup_latency_seconds histogram swarm_store_lookup_latency_seconds_bucket{le=\"0.005\"} 0 swarm_store_lookup_latency_seconds_bucket{le=\"0.01\"} 0 swarm_store_lookup_latency_seconds_bucket{le=\"0.025\"} 0 swarm_store_lookup_latency_seconds_bucket{le=\"0.05\"} 0 swarm_store_lookup_latency_seconds_bucket{le=\"0.1\"} 0 swarm_store_lookup_latency_seconds_bucket{le=\"0.25\"} 0 swarm_store_lookup_latency_seconds_bucket{le=\"0.5\"} 0 swarm_store_lookup_latency_seconds_bucket{le=\"1\"} 0 swarm_store_lookup_latency_seconds_bucket{le=\"2.5\"} 0 swarm_store_lookup_latency_seconds_bucket{le=\"5\"} 0 swarm_store_lookup_latency_seconds_bucket{le=\"10\"} 0 swarm_store_lookup_latency_seconds_bucket{le=\"+Inf\"} 0 swarm_store_lookup_latency_seconds_sum 0 swarm_store_lookup_latency_seconds_count 0 # HELP swarm_store_memory_store_lock_duration_seconds Duration for which the raft memory store lock was held. # TYPE swarm_store_memory_store_lock_duration_seconds histogram swarm_store_memory_store_lock_duration_seconds_bucket{le=\"0.005\"} 0 swarm_store_memory_store_lock_duration_seconds_bucket{le=\"0.01\"} 0 swarm_store_memory_store_lock_duration_seconds_bucket{le=\"0.025\"} 0 swarm_store_memory_store_lock_duration_seconds_bucket{le=\"0.05\"} 0 swarm_store_memory_store_lock_duration_seconds_bucket{le=\"0.1\"} 0 swarm_store_memory_store_lock_duration_seconds_bucket{le=\"0.25\"} 0 swarm_store_memory_store_lock_duration_seconds_bucket{le=\"0.5\"} 0 swarm_store_memory_store_lock_duration_seconds_bucket{le=\"1\"} 0 swarm_store_memory_store_lock_duration_seconds_bucket{le=\"2.5\"} 0 swarm_store_memory_store_lock_duration_seconds_bucket{le=\"5\"} 0 swarm_store_memory_store_lock_duration_seconds_bucket{le=\"10\"} 0 swarm_store_memory_store_lock_duration_seconds_bucket{le=\"+Inf\"} 0 swarm_store_memory_store_lock_duration_seconds_sum 0 swarm_store_memory_store_lock_duration_seconds_count 0 # HELP swarm_store_read_tx_latency_seconds Raft store read tx latency. # TYPE swarm_store_read_tx_latency_seconds histogram swarm_store_read_tx_latency_seconds_bucket{le=\"0.005\"} 0 swarm_store_read_tx_latency_seconds_bucket{le=\"0.01\"} 0 swarm_store_read_tx_latency_seconds_bucket{le=\"0.025\"} 0 swarm_store_read_tx_latency_seconds_bucket{le=\"0.05\"} 0 swarm_store_read_tx_latency_seconds_bucket{le=\"0.1\"} 0 swarm_store_read_tx_latency_seconds_bucket{le=\"0.25\"} 0 swarm_store_read_tx_latency_seconds_bucket{le=\"0.5\"} 0 swarm_store_read_tx_latency_seconds_bucket{le=\"1\"} 0 swarm_store_read_tx_latency_seconds_bucket{le=\"2.5\"} 0 swarm_store_read_tx_latency_seconds_bucket{le=\"5\"} 0 swarm_store_read_tx_latency_seconds_bucket{le=\"10\"} 0 swarm_store_read_tx_latency_seconds_bucket{le=\"+Inf\"} 0 swarm_store_read_tx_latency_seconds_sum 0 swarm_store_read_tx_latency_seconds_count 0 # HELP swarm_store_write_tx_latency_seconds Raft store write tx latency. # TYPE swarm_store_write_tx_latency_seconds histogram swarm_store_write_tx_latency_seconds_bucket{le=\"0.005\"} 0 swarm_store_write_tx_latency_seconds_bucket{le=\"0.01\"} 0 swarm_store_write_tx_latency_seconds_bucket{le=\"0.025\"} 0 swarm_store_write_tx_latency_seconds_bucket{le=\"0.05\"} 0 swarm_store_write_tx_latency_seconds_bucket{le=\"0.1\"} 0 swarm_store_write_tx_latency_seconds_bucket{le=\"0.25\"} 0 swarm_store_write_tx_latency_seconds_bucket{le=\"0.5\"} 0 swarm_store_write_tx_latency_seconds_bucket{le=\"1\"} 0 swarm_store_write_tx_latency_seconds_bucket{le=\"2.5\"} 0 swarm_store_write_tx_latency_seconds_bucket{le=\"5\"} 0 swarm_store_write_tx_latency_seconds_bucket{le=\"10\"} 0 swarm_store_write_tx_latency_seconds_bucket{le=\"+Inf\"} 0 swarm_store_write_tx_latency_seconds_sum 0 swarm_store_write_tx_latency_seconds_count 0 © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"prometheus/Blackbox_exporter-monitoring-url.html":{"url":"prometheus/Blackbox_exporter-monitoring-url.html","title":"【prometheus】使用Blackbox_exporter服务探测","keywords":"","body":"Blackbox_exporter监控url 有的时候，我们需要对域名或者一些接口url进行检测，看下是否可以，我们可以部署Blackbox_exporter来采集这些监控数据到prometheus Blackbox_exporter是Prometheus官方提供的exporter之一，可以提供 http、dns、tcp、icmp的监控数据采集，具体部署可以参考文档https://cloud.tencent.com/developer/article/1808998 blackbox模块配置 下面我们说说怎么配置各种协议的探测，首先需要在blackbox的配置文件设置模块 modules: http_2xx: prober: http timeout: 15s http: prober: http timeout: 2s http: valid_http_versions: [\"HTTP/1.1\", \"HTTP/2\"] valid_status_codes: [200,301,302] method: GET preferred_ip_protocol: \"ip4\" tcp_connect: prober: tcp ssh: prober: tcp tcp: query_response: - expect: \"^SSH-2.0-\" icmp: prober: icmp timeout: 5s icmp: 上面我们定义了4个模块，下面我们只需要在job中配置下模块的检查内容 http测试job配置 scrape_configs: - job_name: http-blackbox honor_timestamps: true params: module: - http_2xx metrics_path: /probe scheme: http static_configs: - targets: - https://www.niewx.cn/ labels: domain: www.niewx.cn instance: https://www.niewx.cn/ ip: githubpage port: \"443\" project: blog service: none - targets: - https://www.niewx.cn/mybook/ labels: domain: www.niewx.cn instance: https://www.niewx.cn/mybook/ ip: githubpage port: \"443\" project: mybook service: none relabel_configs: - source_labels: [__address__] separator: ; regex: (.*) target_label: __param_target replacement: $1 action: replace - separator: ; regex: (.*) target_label: __address__ replacement: blackbox-exporter.monitor.svc.cluster.local:9115 action: replace tcp测试job配置 scrape_configs: - job_name: tcp-blackbox honor_timestamps: true params: module: - tcp_connect metrics_path: /probe static_configs: - targets: - xxx.xxx.xxx.xxx:80 labels: instance: xxx.xxx.xxx.xxx:80 ip: xxx.xxx.xxx.xxx port: \"80\" - targets: - xxx.xxx.xxx.xxx:8080 labels: instance: xxx.xxx.xxx.xxx:8080 ip: xxx.xxx.xxx.xxx port: \"8080\" relabel_configs: - source_labels: [__address__] separator: ; regex: (.*) target_label: __param_target replacement: $1 action: replace - separator: ; regex: (.*) target_label: __address__ replacement: blackbox-exporter.monitor.svc.cluster.local:9115 action: replace icmp测试job配置 scrape_configs: - job_name: icmp-blackbox honor_timestamps: true params: module: - icmp metrics_path: /probe static_configs: - targets: - xx.xx.xx.xx labels: instance: xx.xx.xx.xx ip: xx.xx.xx.xx - targets: - xxx.xxx.xxx.xxx labels: instance: xxx.xxx.xxx.xxx ip: xxx.xxx.xxx.xxx relabel_configs: - source_labels: [__address__] separator: ; regex: (.*) target_label: __param_target replacement: $1 action: replace - separator: ; regex: (.*) target_label: __address__ replacement: blackbox-exporter.monitor.svc.cluster.local:9115 action: replace 配置好之后，数据就可以采集到prometheus了，然后配置下grafana面板查看我们的监控信息即可。 问题 问题：当http探测域名的是时候发现面板的连通性是离线，但是http的状态却是200 解决方案：可以将http模块中配置的协议HTTP/2改成HTTP/2.0，然后再重启blackbox即可 © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"prometheus/prometheus-monitors-k8s-cluster-coredns.html":{"url":"prometheus/prometheus-monitors-k8s-cluster-coredns.html","title":"【prometheus】prometheus监控k8s集群coredns","keywords":"","body":"prometheus监控k8s集群coredns k8s中的域名解析的主流方案是coredns，kubeadm默认是会在集群部署coredns，一般k8s都会部署coredns，并且coredns有默认提供监控metrics的接口，下面我们来来说说如何用prometheus来采集coredns的监控数据。这里我们prometheus的用的是operater创建的，通过serviceMontor来监控coredns的service。 创建监控接口的servcie 正常我们在集群默认部署的coredns，会自动创建一个kube-dns的service，业务pod内就是通过访问这个service来用coredns进行域名的解析。coredns默认用9153端口提供了metrics接口，因此我们还需要新建一个service，用来暴露coredns的9153端口。 apiVersion: v1 kind: Service metadata: name: coredns-metrics namespace: kube-system labels: app: cordns spec: ports: - name: 9153-9153-tcp port: 9153 protocol: TCP targetPort: 9153 selector: k8s-app: kube-dns sessionAffinity: None type: ClusterIP 配置serviceMontor采集监控数据 apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: coredns-servicemonitors namespace: kube-system spec: endpoints: - bearerTokenSecret: key: \"\" interval: 15s port: 9153-9153-tcp namespaceSelector: matchNames: - kube-system selector: matchLabels: app: cordns 除了用serviceMontor配置，也可以用RawJobs配置 scrape_configs: - job_name: coredns-metrics honor_labels: true honor_timestamps: true scrape_interval: 15s metrics_path: /metrics scheme: http kubernetes_sd_configs: - role: endpoints namespaces: names: - kube-system relabel_configs: - source_labels: [__config_type] separator: ; regex: service target_label: __config_type replacement: $1 action: replace - source_labels: [__meta_kubernetes_endpoint_port_name] separator: ; regex: 9153-9153-tcp replacement: $1 action: keep - source_labels: [__meta_kubernetes_service_name] separator: ; regex: coredns-metrics replacement: $1 action: keep - source_labels: [__meta_kubernetes_pod_node_name] separator: ; regex: (.*) target_label: node replacement: $1 action: replace - source_labels: [__meta_kubernetes_namespace] separator: ; regex: (.*) target_label: namespace replacement: $1 action: replace - source_labels: [__meta_kubernetes_service_name] separator: ; regex: (.*) target_label: service replacement: $1 action: replace - source_labels: [__meta_kubernetes_pod_name] separator: ; regex: (.*) target_label: pod replacement: $1 action: replace - source_labels: [__meta_kubernetes_endpoint_port_name] separator: ; regex: (.*) target_label: endpoint replacement: $1 action: replace grafana配置coredns监控面板 可以到grafana的官网文档中找相关的dashboard，可以用5926这个面板，导入面板后，就可以查看coredns的监控了。 © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"prometheus/Prometheus-common-alarm-promsql.html":{"url":"prometheus/Prometheus-common-alarm-promsql.html","title":"【prometheus】prometheus常用告警promsql","keywords":"","body":"prometheus常用告警promsql k8s中，告警监控我们都是通过prometheus加上alertmanager来实现的，在alertmanager里面通过写promsql来进行告警的设置，然后发生到不同的告警渠道，下面我们来说说常用告警promsql配置。 master告警配置 证书过期告警，表示证书将在24小时后到期 apiserver_client_certificate_expiration_seconds_count{job=\"apiserver\"} > 0 and on(job) histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job=\"apiserver\"}[5m]))) apiserver状态异常告警 sum(up{job=\"kube-apiserver\"}) == 0 kube-scheduler状态异常告警 sum(up{job=\"kube-scheduler\"}) == 0 kube-controller-manager状态异常告警 sum(up{job=\"kube-controller-manager\"}) == 0 节点告警配置 节点状态异常告警 kube_node_status_condition{condition=\"Ready\",status=\"true\"} == 0 节点存在MemoryPressure告警 kube_node_status_condition{condition=\"MemoryPressure\",status=\"true\"} == 1 节点存在DiskPressure告警 kube_node_status_condition{condition=\"DiskPressure\",status=\"true\"} == 1 节点存在OutOfDisk告警 kube_node_status_condition{condition=\"OutOfDisk\",status=\"true\"} == 1 节点pod数量超过节点容纳最大pod数量的90%告警 sum by (node) ((kube_pod_status_phase{phase=\"Running\"} == 1) + on(uid) group_left(node) (0 * kube_pod_info{pod_template_hash=\"\"})) / sum by (node) (kube_node_status_allocatable{resource=\"pods\"}) * 100 > 90 节点的时钟同步异常告警 min_over_time(node_timex_sync_status[5m]) == 0 and node_timex_maxerror_seconds >= 16 节点的Conntrack超过上限设置的75%告警 (node_nf_conntrack_entries / node_nf_conntrack_entries_limit) > 0.75 节点磁盘空间不足告警 ( node_filesystem_avail_bytes{job=\"node-exporter\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node-exporter\",fstype!=\"\"} * 100 pod告警配置 pod的cpu占limit使用率超过80% sum(rate(container_cpu_usage_seconds_total{job=\"cadvisor\", image!=\"\", container!=\"POD\"}[1m])) by (namespace, pod, container) / sum(kube_pod_container_resource_limits_cpu_cores) by (namespace, pod, container) > 0.8 pod的cpu占request使用率超过80% sum(rate(container_cpu_usage_seconds_total{job=\"cadvisor\", image!=\"\", container!=\"POD\"}[1m])) by (namespace, pod, container) / sum(kube_pod_container_resource_requests_cpu_cores) by (namespace, pod, container) > 0.8 pod的内存占limit使用率超过80% sum(rate(container_memory_working_set_bytes{job=\"cadvisor\", image!=\"\", container!=\"POD\"}[1m])) by (namespace, pod, container) / sum(kube_pod_container_resource_limits_memory_bytes) by (namespace, pod, container) > 0.8 pod的内存占request使用率超过80% sum(rate(container_memory_working_set_bytes{job=\"cadvisor\", image!=\"\", container!=\"POD\"}[1m])) by (namespace, pod, container) / sum(kube_pod_container_resource_requests_memory_bytes) by (namespace, pod, container) > 0.8 workload告警配置 deployment的副本数不匹配 kube_deployment_spec_replicas != kube_deployment_status_replicas_available StatefulSet和期望实例副本数不匹配 kube_statefulset_status_replicas_ready != kube_statefulset_status_replicas 负载5分钟内容器重启次数 rate(kube_pod_container_status_restarts_total{job=\"kube-state-metrics\"}[5m]) * 60 * 5 > 0 job执行失败 kube_job_failed{job=\"kube-state-metrics\"} > 0 hpa的副本数和预期不一致告警 (kube_hpa_status_desired_replicas{job=\"kube-state-metrics\"} != kube_hpa_status_current_replicas{job=\"kube-state-metrics\"}) and changes(kube_hpa_status_current_replicas[15m]) == 0 hpa副本数达到最大值告警 kube_hpa_status_current_replicas{job=\"kube-state-metrics\"} == kube_hpa_spec_max_replicas{job=\"kube-state-metrics\"} 存储告警配置 pv磁盘剩余可用空间小于3% kubelet_volume_stats_available_bytes{job=\"kubelet\"} / kubelet_volume_stats_capacity_bytes{job=\"kubelet\"} 预测卷的磁盘空间是否4天后用尽 ( kubelet_volume_stats_available_bytes{job=\"kubelet\"} / kubelet_volume_stats_capacity_bytes{job=\"kubelet\"} ) pv状态异常告警 kube_persistentvolume_status_phase{phase=~\"Failed|Pending\",job=\"kube-state-metrics\"} > 0 © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"istio/istio-annotation-list.html":{"url":"istio/istio-annotation-list.html","title":"【istio】istio注解列表","keywords":"","body":"istio注解列表 注解加在workload的spec.template.metadata.annotations字段下 apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: annotations: sidecar.istio.io/status: xxxxx Istio支持控制其行为的各种资源注释列表如下 Annotation Name Resource Types Description kubernetes.io/ingress.class [Ingress] Ingress 资源上的注释，表示负责它的控制器类。 networking.istio.io/exportTo [Service] 指定此服务应导出到的命名空间。 '*' 值表示它可以在网格 '.' 内到达。表示它在其命名空间内是可访问的。 policy.istio.io/check [pod] 确定无法连接到 Mixer 时的行为策略。如果未设置，则设置 FAIL_CLOSE，拒绝请求。 policy.istio.io/checkBaseRetryWaitTime [pod] 重试之间等待的基本时间，将通过退避和抖动进行调整。以持续时间格式。如果未设置，则为 80 毫秒。 policy.istio.io/checkMaxRetryWaitTime [pod] 重试 Mixer 之间等待的最长时间。以持续时间格式。如果未设置，则为 1000 毫秒。 policy.istio.io/checkRetries [pod] 传输错误到 Mixer 的最大重试次数。如果未设置，则为 0，表示不重试。 policy.istio.io/lang [pod] 为 Mixer 选择属性表达式语言运行时。 readiness.status.sidecar.istio.io/applicationPorts [pod] 指定应用容器暴露的端口列表。由 Envoy sidecar 就绪探测器使用，以确定 Envoy 已配置并准备好接收流量。 readiness.status.sidecar.istio.io/failureThreshold [pod] 指定 Envoy sidecar 就绪探测的失败阈值。 readiness.status.sidecar.istio.io/initialDelaySeconds [pod] 指定 Envoy sidecar 就绪探测的初始延迟（以秒为单位）。 readiness.status.sidecar.istio.io/periodSeconds [pod] 指定 Envoy sidecar 就绪探测的周期（以秒为单位）。 sidecar.istio.io/bootstrapOverride [pod] 指定一个替代的 Envoy 引导程序配置文件。 sidecar.istio.io/componentLogLevel [pod] 指定 Envoy 的组件日志级别。 sidecar.istio.io/controlPlaneAuthPolicy [pod] 指定 Istio 控制平面使用的身份验证策略。如果为 NONE，则不会加密流量。如果是 MUTUAL_TLS，Envoy sidecar 之间的流量将被包装到相互的 TLS 连接中。 sidecar.istio.io/discoveryAddress [pod] 指定 Envoy sidecar 使用的 XDS 发现地址。 sidecar.istio.io/inject [pod] 指定 Envoy sidecar 是否应该自动注入到工作负载中。 sidecar.istio.io/interceptionMode [pod] 指定用于将入站连接重定向到 Envoy（REDIRECT 或 TPROXY）的模式。 sidecar.istio.io/logLevel [pod] 指定 Envoy 的日志级别。 sidecar.istio.io/proxyCPU [pod] 为 Envoy sidecar 指定请求的 CPU 设置。 sidecar.istio.io/proxyCPULimit [pod] 为 Envoy sidecar 指定limit的 CPU 设置。 sidecar.istio.io/proxyImage [pod] 指定 Envoy sidecar 使用的 Docker 镜像。 sidecar.istio.io/proxyMemory [pod] 指定 Envoy sidecar 请求的内存设置。 sidecar.istio.io/proxyMemoryLimit [pod] 指定 Envoy sidecar limit的内存设置。 sidecar.istio.io/rewriteAppHTTPProbers [pod] 重写 HTTP 准备和活动探测器以重定向到 Envoy sidecar。 sidecar.istio.io/statsInclusionPrefixes [pod] 指定由 Envoy 发出的统计信息前缀的逗号分隔列表。 sidecar.istio.io/statsInclusionRegexps [pod] 指定要由 Envoy 发出的统计信息应匹配的逗号分隔的正则表达式列表。 sidecar.istio.io/statsInclusionSuffixes [pod] 指定由 Envoy 发出的统计数据后缀的逗号分隔列表。 sidecar.istio.io/status [pod] 由 Envoy sidecar 注入生成，指示操作的状态。包括执行模板的版本哈希，以及注入资源的名称。 sidecar.istio.io/userVolume [pod] 指定要添加到 Envoy sidecar 的一个或多个用户卷（作为 JSON 数组）。 sidecar.istio.io/userVolumeMount [pod] 指定要添加到 Envoy sidecar 的一个或多个用户卷挂载（作为 JSON 数组）。 status.sidecar.istio.io/port [pod] 指定 Envoy sidecar 的 HTTP 状态端口。如果为零，sidecar 将不提供状态。 traffic.sidecar.istio.io/excludeInboundPorts [pod] 一个逗号分隔的入站端口列表，要从重定向到 Envoy 中排除。仅当所有入站流量（即“*”）被重定向时才适用。 traffic.sidecar.istio.io/excludeOutboundIPRanges [pod] 以逗号分隔的 CIDR 格式的 IP 范围列表，要从重定向中排除。仅当所有出站流量（即“*”）都被重定向时才适用。 traffic.sidecar.istio.io/excludeOutboundPorts [pod] 一个逗号分隔的出站端口列表，要从重定向到 Envoy 中排除。 traffic.sidecar.istio.io/includeInboundPorts [pod] 以逗号分隔的入站端口列表，其流量将被重定向到 Envoy。通配符“*”可用于为所有端口配置重定向。空列表将禁用所有入站重定向。 traffic.sidecar.istio.io/includeOutboundIPRanges [pod] 以逗号分隔的 CIDR 形式的 IP 范围列表，用于重定向到 Envoy（可选）。通配符“*”可用于重定向所有出站流量。空列表将禁用所有出站重定向。 traffic.sidecar.istio.io/kubevirtInterfaces [pod] 以逗号分隔的虚拟接口列表，其入站流量（来自 VM）将被视为出站流量。 © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"istio/Sidecar-startup-sequence-problem-in-istio.html":{"url":"istio/Sidecar-startup-sequence-problem-in-istio.html","title":"【istio】istio中sidecar启动顺序问题","keywords":"","body":"问题 当我们的一些服务往istio上迁移的时候，会出现一个问题，就是某些依赖数据库的服务会一直起不了，pod启动失败，这里排查原因是envoy容器还没起来，服务容器就起来了，导致业务流量无法被转发出去，从而连接数据库异常。 解决方案 这里解决问题的方案就是保证envoy这个sidecar容器先于业务容器启动，那么怎么保证sidecar容器先于业务容器启动呢？ istio1.7之后版本解决方案 istio1.7通过给istio-injector注入逻辑增加一个叫HoldApplicationUntilProxyStarts的开关来解决了该问题 添加了 values.global.proxy.holdApplicationUntilProxyStarts config选项，它使sidecar注入器在pod容器列表的开始处注入sidecar，并将其配置为阻止所有其他容器的开始，直到代理就绪为止。默认情况下禁用此选项。(#11130) 也就是说只要开启个这个特性，那么就可以保证pod里的sidecar容器内先于业务容器启动，这样就不会出现pod内容器启动顺序的问题 这里我们可以全局开启这个特性和单独给某个deployment开启这个特性 全局开启HoldApplicationUntilProxyStarts 全局开启只需要修改istiod的全局配置即可 kubectl edit cm -n istio-system istio-1-8-1 在defaultConfig字段下加上holdApplicationUntilProxyStarts: true apiVersion: v1 data: mesh: | accessLogEncoding: JSON accessLogFile: /dev/stdout accessLogFormat: \"\" defaultConfig: holdApplicationUntilProxyStarts: true discoveryAddress: istiod-1-8-1.istio-system.svc:15012 局部开启HoldApplicationUntilProxyStarts 如果单独给某个deployment开启这个特性，需要在pod的注解加上proxy.istio.io/config，将 holdApplicationUntilProxyStarts 置为 true apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: annotations: proxy.istio.io/config: | holdApplicationUntilProxyStarts: true labels: app: nginx spec: containers: - name: nginx image: \"nginx\" istio1.7版本之前的解决方案 如果是istio1.7之前的版本，是没有这个特性的，那么需要采用另外一种方案来解决这个问题，我们在业务容器启动前判断下envoy服务是否已经启动成功了 command: [\"/bin/bash\", \"-c\"] args: [\"while [[ \\\"$(curl -s -o /dev/null -w ''%{http_code}'' localhost:15020/healthz/ready)\\\" != '200' ]]; do echo Waiting for Sidecar;sleep 1; done; echo Sidecar available; start-app-cmd\"] 这里直接在deployment加上启动命令，如果探测envoy服务的15020端口返回200，则启动业务进程，这样可以保证sidecar先比业务容器启动 © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"istio/istio-traffic-shift.html":{"url":"istio/istio-traffic-shift.html","title":"【istio】istio流量转移实践","keywords":"","body":"有时候我们需要将服务的部分流量转移到另外一个服务，灰度测试的时候，为了测试新版本是否有问题，我们需要将部分流量打到新版本上，本次任务，我们实现将请求流量20%打到v1版本，80%的流量打到v2版本。 部署nginx的v1和v2版本 apiVersion: v1 data: index.html: I am v1 version kind: ConfigMap metadata: name: index-v1 namespace: mesh --- apiVersion: v1 data: index.html: I am v2 version kind: ConfigMap metadata: name: index-v2 namespace: mesh --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: nginx version: v1 name: nginx-v1 namespace: mesh spec: replicas: 1 selector: matchLabels: app: nginx version: v1 template: metadata: labels: app: nginx version: v1 spec: containers: - image: nginx imagePullPolicy: Always name: nginx-v1 resources: limits: cpu: 500m memory: 1Gi requests: cpu: 250m memory: 256Mi volumeMounts: - mountPath: /usr/share/nginx/html/index.html name: vol subPath: index.html dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey restartPolicy: Always volumes: - configMap: defaultMode: 420 name: index-v1 name: vol --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: nginx version: v2 name: nginx-v2 namespace: mesh spec: replicas: 1 selector: matchLabels: app: nginx version: v2 template: metadata: labels: app: nginx version: v2 spec: containers: - image: nginx imagePullPolicy: Always name: nginx-v2 resources: limits: cpu: 500m memory: 1Gi requests: cpu: 250m memory: 256Mi volumeMounts: - mountPath: /usr/share/nginx/html/index.html name: vol subPath: index.html dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey restartPolicy: Always volumes: - configMap: defaultMode: 420 name: index-v2 name: vol --- apiVersion: v1 kind: Service metadata: name: nginx-v1 labels: app: nginx service: nginx spec: ports: - port: 80 name: http selector: app: nginx 这里部署了一个统一的入口service关联到后端的v1和v2版本pod里面 创建DestinationRule apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: nginx namespace: mesh spec: host: nginx subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 创建gateway apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: nginx-gw namespace: mesh spec: servers: - port: number: 88 name: HTTP-88-ebsi protocol: HTTP hosts: - '*' selector: app: istio-ingressgateway istio: ingressgateway 挂载ingressgateway作为统一的访问入口 创建VirtualService apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: nginx namespace: mesh spec: hosts: - '*' gateways: - mesh/nginx-gw http: - route: - destination: host: nginx subset: v1 weight: 20 - destination: host: nginx subset: v2 weight: 80 VirtualService里面的weight字段代表的是流量转发的比例，这里将流量20%转发到v1，流量80%转到v2 访问测试 我们直接用ingress的LoadBalancer的vip和88端口访问10次看下 [root@VM-0-13-centos ~]# for i in {1..10};do curl 106.xx.xx.93:88 ; echo \"\" ;done I am v2 version I am v2 version I am v2 version I am v2 version I am v2 version I am v2 version I am v2 version I am v2 version I am v1 version I am v1 version 从上面的测试结果看，\"I am v1 version\"出现了2次，\"I am v2 version\"出现了8次，说明请求的流量是符合VirtualService的分配比例的 © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"istio/istio-multi-cluster-nearby-access.html":{"url":"istio/istio-multi-cluster-nearby-access.html","title":"【istio】istio多集群就近接入","keywords":"","body":"随着业务的扩展，有时候希望客户能就近访问我们的网站，减少网络延迟，但是很多时候会在多地域部署相同的服务，其实有了istio后，我们可以利用就近接入来解决这个问题，这样无需在另一地域集群部署整套业务，只需在网格管理的另一个集群中部署边缘代理网关并配置好监听规则，即可以另一集群为入口访问电商网站业务 腾讯云上的服务网格如果不同地域通过云联网打通了可以通过同一个网格管理，下面我们通过同一个vpc下的2个集群模拟多个地域来配置下就近访问 首先我们在网格主集群A中部署一个nginx服务，通过gateway提供访问 apiVersion: apps/v1 kind: Deployment metadata: generation: 5 labels: k8s-app: nginx qcloud-app: nginx name: nginx namespace: mesh spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: nginx qcloud-app: nginx strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: creationTimestamp: null labels: k8s-app: nginx qcloud-app: nginx spec: containers: - image: nginx imagePullPolicy: Always name: nginx resources: limits: cpu: 500m memory: 1Gi requests: cpu: 250m memory: 256Mi securityContext: privileged: false terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey restartPolicy: Always --- apiVersion: v1 kind: Service metadata: name: nginx-test namespace: mesh spec: ports: - name: 80-80-tcp port: 80 protocol: TCP targetPort: 80 selector: k8s-app: nginx qcloud-app: nginx sessionAffinity: None type: ClusterIP 主集群A部署gateway apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: nginx-gw namespace: mesh spec: servers: - port: number: 88 name: HTTP-88-ebsi protocol: HTTP hosts: - '*' selector: app: istio-ingressgateway istio: ingressgateway 我们将子集群B加入网格内，也会部署一个istio-ingressgateway作为访问的入口，然后在B集群部署一个gateway来访问我们A集群部署的服务 apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: child-gw namespace: mesh spec: servers: - port: number: 80 name: HTTP-80-6pnq protocol: HTTP hosts: - '*' selector: app: istio-ingressgateway-2 istio: ingressgateway 然后我们创建一个VirtualService来关联这2个gateway apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: nearest-access-vs namespace: mesh spec: hosts: - '*' gateways: - mesh/child-gw - mesh/nginx-gw http: - route: - destination: host: nginx-test.mesh.svc.cluster.local port: number: 80 现在我们在B集群没有部署nginx服务，然后我们用B集群的gateway来访问我们服务看是否能访问到 [root@VM-17-4-centos ~]# kubectl get svc -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway-2 LoadBalancer 172.22.253.209 10.0.0.31 80:31199/TCP,15021:30299/TCP 4h5m istiod-1-8-1 LoadBalancer 172.22.255.147 10.0.0.141 15012:32190/TCP,443:31540/TCP 4h28m istiod-1-8-1-injector ClusterIP None 443/TCP 4h28m kube-mesh LoadBalancer 172.22.254.32 10.0.0.144 443:30431/TCP 4h28m zipkin ClusterIP 172.22.255.45 9411/TCP 4h27m [root@VM-17-4-centos ~]# kubectl get pod -n mesh No resources found in mesh namespace. [root@VM-17-4-centos ~]# kubectl get svc -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway-2 LoadBalancer 172.22.253.209 10.0.0.31 80:31199/TCP,15021:30299/TCP 4h5m istiod-1-8-1 LoadBalancer 172.22.255.147 10.0.0.141 15012:32190/TCP,443:31540/TCP 4h28m istiod-1-8-1-injector ClusterIP None 443/TCP 4h28m kube-mesh LoadBalancer 172.22.254.32 10.0.0.144 443:30431/TCP 4h28m zipkin ClusterIP 172.22.255.45 9411/TCP 4h27m [root@VM-17-4-centos ~]# kubectl get pod -n mesh No resources found in mesh namespace. [root@VM-17-4-centos ~]# curl 10.0.0.31:80 Welcome to nginx! body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } Welcome to nginx! If you see this page, the nginx web server is successfully installed and working. Further configuration is required. For online documentation and support please refer to nginx.org. Commercial support is available at nginx.com. Thank you for using nginx. 从上面测试结果看，我们的ingressgateway的service是一个内网lb，并且我们在mesh命名空间下没有部署服务，我们通过lb的vip和80端口是可以访问到nginx服务的，这里说明我们访问B集群的流量被路由到了主集群A中。 © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"istio/VirtualService-host-configuration-and-use.html":{"url":"istio/VirtualService-host-configuration-and-use.html","title":"【istio】VirtualService中hosts字段的配置使用","keywords":"","body":"VirtualService中hosts字段的配置使用 VirtualService配置示例 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: nginx namespace: mesh spec: hosts: - 172.16.85.174 - nginx-test.mesh.svc.cluster.local - nginx-test gateways: - mesh/vpc-gw http: - route: - destination: host: nginx subset: v1 weight: 20 - destination: host: nginx subset: v2 weight: 80 使用hosts字段列举虚拟服务的主机——即用户指定的目标或是路由规则设定的目标。这是客户端向服务发送请求时使用的一个或多个地址。只有访问客户端的Host字段为hosts配置的地址才能路由到后端服务。 hosts: 172.16.85.174 nginx-test.mesh.svc.cluster.local nginx-test 虚拟服务主机名可以是IP 地址、DNS 名称，或者依赖于平台的一个简称（例如 Kubernetes 服务的短名称），隐式或显式地指向一个完全限定域名（FQDN）。您也可以使用通配符（“*”）前缀，让您创建一组匹配所有服务的路由规则。虚拟服务的 hosts 字段实际上不必是 Istio 服务注册的一部分，它只是虚拟的目标地址。这让您可以为没有路由到网格内部的虚拟主机建模。 上面的VirtualService配置了多个hosts，并且挂载了一个gateways，客户端直接访问后端的service是可以通的，但是我们通过域名访问后端服务时候就需要指定host了。 首先我们直接访问下gateway域名，是无法访问通的，因为VirtualService流量规则指定了hosts，我们的请求Host没在配置列表中。 bash-4.4# curl nginx.istio.niewx.top 如果你希望能通过域名直接访问，可以将域名配置到hosts下，默认发起请求的Host就是域名本身 spec: hosts: - 172.16.85.174 - nginx-test.mesh.svc.cluster.local - nginx-test - nginx.istio.niewx.top 直接访问域名 bash-4.4# curl -Iv nginx.istio.niewx.top * Rebuilt URL to: nginx.istio.niewx.top/ * Trying 10.0.0.183... * TCP_NODELAY set * Connected to nginx.istio.niewx.top (10.0.0.183) port 80 (#0) > HEAD / HTTP/1.1 > Host: nginx.istio.niewx.top > User-Agent: curl/7.61.1 > Accept: */* > 下面我们在请求中指定Host为172.16.85.174看下 bash-4.4# curl --silent -H \"Host: 172.16.85.174\" \"nginx.istio.niewx.top\" Welcome to nginx! html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } Welcome to nginx! If you see this page, the nginx web server is successfully installed and working. Further configuration is required. For online documentation and support please refer to nginx.org. Commercial support is available at nginx.com. Thank you for using nginx. 指定Host为nginx-test.mesh.svc.cluster.local域名 bash-4.4# curl --silent -H \"Host: nginx-test.mesh.svc.cluster.local\" \"nginx.istio.niewx.top\" Welcome to nginx! html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } Welcome to nginx! If you see this page, the nginx web server is successfully installed and working. Further configuration is required. For online documentation and support please refer to nginx.org. Commercial support is available at nginx.com. Thank you for using nginx. 当我们在请求中指定配置的hosts列表中Host时候是可以访问成功的。 © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"istio/Gateway-configuration-http-forced-to-https.html":{"url":"istio/Gateway-configuration-http-forced-to-https.html","title":"【istio】gateway配置http强转https","keywords":"","body":"gateway配置http强转https 使用istio的过程中，有时候不想让用户可以http访问，这时候就需要在gateway配置http强转为https访问，下面我们来说明下如何在gateway配置http强转https。 首先我们测试下正常配置http和https，看下是否分别通过http和https访问到后端的服务。 apiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: vpc-gw namespace: mesh spec: selector: app: istio-ingressgateway-vpc istio: ingressgateway servers: - hosts: - nginx.istio.niewx.top port: name: HTTP-80-iy2r number: 80 protocol: HTTP - hosts: - nginx.istio.niewx.top port: name: HTTPS-443-1krv number: 443 protocol: HTTPS tls: credentialName: vpc-gw-https-443-1krv mode: SIMPLE 分别通过http和https都可以成功的访问到后端。 [root@VM-0-13-centos ~]# curl -I http://nginx.istio.niewx.top HTTP/1.1 200 OK server: istio-envoy date: Tue, 21 Sep 2021 15:41:56 GMT content-type: text/html content-length: 15 last-modified: Sat, 18 Sep 2021 18:32:54 GMT etag: \"614630d6-f\" accept-ranges: bytes x-envoy-upstream-service-time: 26 [root@VM-0-13-centos ~]# curl -I https://nginx.istio.niewx.top HTTP/1.1 200 OK server: istio-envoy date: Tue, 21 Sep 2021 15:42:16 GMT content-type: text/html content-length: 15 last-modified: Sat, 18 Sep 2021 18:32:54 GMT etag: \"614630d6-f\" accept-ranges: bytes x-envoy-upstream-service-time: 1 gateway配置http强转https，只需要在gateway的http的配置中加上如下配置即可。 tls: httpsRedirect: true 下面我们在gateway中加上强制跳转的配置，再来通过http访问下。 apiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: vpc-gw namespace: mesh spec: selector: app: istio-ingressgateway-vpc istio: ingressgateway servers: - hosts: - nginx.istio.niewx.top port: name: HTTP-80-iy2r number: 80 protocol: HTTP tls: httpsRedirect: true - hosts: - nginx.istio.niewx.top port: name: HTTPS-443-1krv number: 443 protocol: HTTPS tls: credentialName: vpc-gw-https-443-1krv mode: SIMPLE 从下面的测试结果可以发现，访问http的时候会出现301，说明我们配置的永久重定向成功了。 [root@VM-0-13-centos ~]# curl -I http://nginx.istio.niewx.top HTTP/1.1 301 Moved Permanently location: https://nginx.istio.niewx.top/ date: Tue, 21 Sep 2021 15:45:12 GMT server: istio-envoy transfer-encoding: chunked [root@VM-0-13-centos ~]# curl -I https://nginx.istio.niewx.top HTTP/1.1 200 OK server: istio-envoy date: Tue, 21 Sep 2021 15:45:14 GMT content-type: text/html content-length: 15 last-modified: Sat, 18 Sep 2021 18:32:54 GMT etag: \"614630d6-f\" accept-ranges: bytes x-envoy-upstream-service-time: 1 © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"istio/istio-sidecar-injection.html":{"url":"istio/istio-sidecar-injection.html","title":"【istio】istio sidecar注入","keywords":"","body":"istio sidecar注入 istio网格中， Pod都必须伴随一个Istio兼容的Sidecar一同运行，sidecar的注入方式有2种，一直是自动，还有一种是手动注入，下面我们来说下这2种注入方式要怎么配置。 自动注入 自动注入的原理是通过webhook实现的，MutatingWebhookConfiguration 配置了Webhook 何时会被 Kubernetes调用。Isti提供的缺省配置，会在带有istio-injection=enabled标签的命名空间中选择Pod。使用kubectl edit mutatingwebhookconfiguration istio-sidecar-injector 命令可以编辑目标命名空间的范围。 如果是用的腾讯云的服务网格，则需要改下命名空间的标签，如果版本是1.10.3，则label是istio.io/rev=1-10-3，后面的版本根据你的服务网格版本确认。 打上label后，新创建的pod，都会默认注入一个sidecar，如果标签是新加的，存量的pod则需要重建才会注入sidecar。 [root@vm-0-3-centos ~]# kubectl get ns mesh --show-labels NAME STATUS AGE LABELS mesh Active 292d field.cattle.io/projectId=p-w6scm,istio.io/rev=1-10-3,kubesphere.io/namespace=mesh [root@vm-0-3-centos ~]# kubectl get pod -n mesh -o=custom-columns=pod_name:.metadata.name,container_name:.spec.containers[*].name pod_name container_name client-76cf54b466-bpprz istio-proxy,client nginx-54cdc9d45c-f29wc istio-proxy,nginx nginx-54cdc9d45c-sdnlb istio-proxy,nginx nginx-v1-5b446c9fbc-8qtxv istio-proxy,nginx-v1 nginx-v1-5b446c9fbc-dlnnh istio-proxy,nginx-v1 nginx-v2-58455fc8d-4mnzr istio-proxy,nginx-v2 sleep-86b8f9575c-p56gv istio-proxy,sleep 从上面的结果看，mesh命名空间加了istio.io/rev这个label后，pod都自动注入了sidecar。 手动注入 通常手动注入，会有下面2种场景： 希望某个deployment不注入sidecar 只希望某个deployment注入sidecar 下面我们来针对上面的场景来说明下，如何在yaml配置。 某个pod不注入sidecar 一般我们给命名空间打上了自动注入的label后，新建的pod默认就会注入sidecar，但是有时候我们希望某些pod不被istio管理。这里则需要给这些pod不注入sidecar，其实可以在deploy的yaml配置下，是否注入sidecar，来实现单个pod不注入sidecar，具体yaml如下 apiVersion: apps/v1 kind: Deployment metadata: name: tke-debug namespace: default labels: app: debug spec: replicas: 1 selector: matchLabels: app: debug template: metadata: labels: app: debug annotations: sidecar.istio.io/inject: \"false\" spec: containers: - name: debug image: ccr.ccs.tencentyun.com/nwx_registry/troubleshooting:latest tolerations: - operator: Exists dnsPolicy: ClusterFirst 在Pod模板中加入 sidecar.istio.io/inject 注解并赋值为false才能覆盖缺省值并阻止对这一 Pod 的sidecar注入。 只给某个pod注入sidecar 只给某个pod注入sidecar，这里有2种方案，一种就是参考上面的方案，给不需要注入的sidecar的pod加上注解，剩下需要注入sidecar的pod设置为true，或者不配置，采用namespace自动注入即可。 上面的方法比较麻烦，如果一个命名空间下有较多的deploy，需要一个个配置才行，其实istio-system 命名空间中的ConfigMap istio-sidecar-injector中包含了缺省的注入策略以及 Sidecar的注入模板，腾讯云的服务网格是istio-sidecar-injector-1-10-3这个configmap。 apiVersion: v1 data: config: |- # defaultTemplates defines the default template to use for pods that do not explicitly specify a template defaultTemplates: [sidecar] policy: disabled alwaysInjectSelector: [] neverInjectSelector: [] ........ 配置中有个polocy字段，里面配置了是否自动注入的策略 disabled：Sidecar注入器缺省不会向Pod进行注入。在Pod模板中加入sidecar.istio.io/inject注解并赋值为true才能覆盖缺省值并启用注入。 enabled：Sidecar注入器缺省会对Pod进行注入。在Pod模板中加入sidecar.istio.io/inject注解并赋值为false才能覆盖缺省值并阻止对这一Pod的注入。 这里我们将policy改成disabled，这里打上了label的命名空间里面pod也不会自动注入sidecar，我们只需要将需要注入sidecar的pod在模板中加入sidecar.istio.io/inject注解并赋值为true即可，这样就只有配置了注解的pod才注入sidcar。 apiVersion: apps/v1 kind: Deployment metadata: name: tke-debug namespace: default labels: app: debug spec: replicas: 1 selector: matchLabels: app: debug template: metadata: labels: app: debug annotations: sidecar.istio.io/inject: \"true\" spec: containers: - name: debug image: ccr.ccs.tencentyun.com/nwx_registry/troubleshooting:latest tolerations: - operator: Exists dnsPolicy: ClusterFirst 打label给单个deployment的pod注入sidecar 如果你用的istio是腾讯云托管的服务网格，并且版本是1.10.3，可以通过给deployment的.spec.template.metadata.labels中加istio.io/rev: 1-10-3这个label，来实现sidecar的自动注入，这里不需要你的namespace是否有加上istio.io/rev这个label。具体的yaml如下 apiVersion: apps/v1 kind: Deployment metadata: name: tke-debug namespace: tke-test labels: app: debug spec: replicas: 1 selector: matchLabels: app: debug template: metadata: labels: app: debug istio.io/rev: 1-10-3 spec: containers: - name: debug image: ccr.ccs.tencentyun.com/nwx_registry/troubleshooting:latest tolerations: - operator: Exists dnsPolicy: ClusterFirst © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"istio/How-to-modify-the-sidecar-resource-configuration.html":{"url":"istio/How-to-modify-the-sidecar-resource-configuration.html","title":"【istio】如何修改istio的sidecar资源配置","keywords":"","body":"如何修改sidecar资源配置 istio的sidecar自动注入是依赖于一个istio-sidecar-injector的mutatingwebhookconfigurations，一般自动注入的sidecar容器配置是哪里设置的呢？其实是在一个istio-sidecar-injector的configmap里面，如何istio-proxy容器资源占用大小的默认配置不符合要求时，可按照实际需求进行修改，下面我们来说说如何配置。 修改单个sidecar资源配置 如何你只想调整某个服务的sidecar容器资源配置，可以通过注解的方式来进行配置，单个工作负载注解配置的优先级会大于全局配置。 apiVersion: apps/v1 kind: Deployment metadata: name: tke-debug namespace: default labels: app: debug spec: replicas: 1 selector: matchLabels: app: debug template: metadata: labels: app: debug annotations: sidecar.istio.io/proxyCPU: 100m sidecar.istio.io/proxyCPULimit: 1 sidecar.istio.io/proxyMemory: 100mi sidecar.istio.io/proxyMemoryLimit: 1Gi spec: containers: - name: debug image: ccr.ccs.tencentyun.com/nwx_registry/troubleshooting:latest tolerations: - operator: Exists dnsPolicy: ClusterFirst 修改全局的sidecar资源配置 如果你是希望修改全局的sidecar资源配置设置，需要改下istio-sidecar-injector这个configmap，如果你是用云厂商托管的istio，控制面是接触不到，一般需要提工单让后端修改。如果是自建的istio或者独立的网格，可以自行修改。 # kubectl edit configmap istio-sidecar-injector -n istio-system 然后在configmap找到这个字段 containers: - name: istio-proxy 这个容器找下Values.global.proxy.resources这个配置 {- else } {- if .Values.global.proxy.resources } { toYaml .Values.global.proxy.resources | indent 6 } {- end } 修改方式有2中，可以直接修改value对应的值，修改resources即可 values: |- { \"global\": { \"caAddress\": \"\", \"configValidation\": true, \"defaultNodeSelector\": {}, \"imagePullPolicy\": \"\", \"imagePullSecrets\": [], \"istioNamespace\": \"istio-system\", \"istiod\": { \"enableAnalysis\": false }, \"jwtPolicy\": \"first-party-jwt\", \"priorityClassName\": \"\", \"proxy\": { \"autoInject\": \"enabled\", \"clusterDomain\": \"cluster.local\", \"componentLogLevel\": \"misc:error\", \"enableCoreDump\": false, \"excludeIPRanges\": \"\", \"logLevel\": \"warning\", \"privileged\": false, \"readinessFailureThreshold\": 30, \"readinessInitialDelaySeconds\": 1, \"readinessPeriodSeconds\": 2, \"resources\": { \"limits\": { \"cpu\": \"2000m\", \"memory\": \"1024Mi\" }, \"requests\": { \"cpu\": \"100m\", \"memory\": \"128Mi\" } }, 还有就是直接配置resource配置，不通过变量引用的方式。 {- else } {- if .Values.global.proxy.resources } #{ toYaml .Values.global.proxy.resources | indent 6 } requests: cpu: 100m memory: 12Mi limits: cpu: 1 memory: 1Gi {- end } © vishon all right reserved，powered by GitbookUpdated at 2022-06-02 10:40:28 "},"istio/Cannot-connect-to-mysql-after-pod-injects-sidecar.html":{"url":"istio/Cannot-connect-to-mysql-after-pod-injects-sidecar.html","title":"【istio】pod注入sidecar后无法连接mysql","keywords":"","body":"pod注入sidecar后无法连接mysql 问题现象 业务pod注入了envoy之后无法连接mysql，去掉envoy注入后，连接mysql正常。 处理过程 看了下isito的版本是1.3.6，是属于老版本，查看集群service，发现集群中存在一个service也是3306端口，这个svc是一个http类型的，有个http filter， 流量在sidecar里是tcp数据，在 这个http filter里解码不出来，这是老版本的一个bug。 解决方案 将service的端口改成其他的，彻底解决需要升级isito版本。 © vishon all right reserved，powered by GitbookUpdated at 2024-01-09 10:05:20 "},"interview/redis.html":{"url":"interview/redis.html","title":"【面试】面试之redis篇","keywords":"","body":"面试题之redis篇 什么是redis？ Redis 是一个开源（BSD 许可）、基于内存、支持多种数据结构的存储系统，可以作为数据库、缓存和消息中间件。它支持的数据结构有字符串（strings）、哈希（hashes）、列表（lists）、集合（sets）、有序集合（sorted sets）等，除此之外还支持 bitmaps、hyperloglogs 和地理空间（ geospatial ）索引半径查询等功能 Redis的数据类型有哪些？ string、hash、list、set、sorted set Redis的数据同步机制？ Redis 支持主从同步、从从同步。如果是第一次进行主从同步，主节点需要使用 bgsave 命令，再将后续修改操作记录到内存的缓冲区，等 RDB 文件全部同步到复制节点，复制节点接受完成后将 RDB 镜像记载到内存中。等加载完成后，复制节点通知主节点将复制期间修改的操作记录同步到复制节点，即可完成同步过程。 Redis如何设置密码？ 永久生效：修改配置文件，需要重启 Redis。在 redis.conf 中可以找到 requirepass 参数，设置 Redis 的访问密码。配置方法为：requirepass 访问密码 临时生效：使用命令设置，不需要重启 Redis。使用命令设置的方法为：config set requirepass 访问密码，redis重启后，密码失效 Redis的一个字符串类型的值能存储最大容量是多少？ 字符串类型是最基本的数据类型，是二进制安全的字符串，最大 512M。 Redis key的过期时间和永久有效分别怎么设置？ 可以使用 EXPIRE 和 PERSIST 命令。对一个已经带有生存时间的 key 执行 EXPIRE 命令，新指定的生存时间会取代旧的生存时间。PERSIST 命令可以移除给定 key 的生存时间，将这个 key 从带生存时间转换成持久的。 如何设置 Redis 的最大连接数？ redis-server -maxclients 100000（连接数）; 什么是bigkey，怎么查找redis的bigkey? bigkey 是指键值占用内存空间非常大的key，查找bigkey可以用下面命令 redis-cli –bigkeys 什么是缓存穿透？怎么解决？ 大量的请求瞬时涌入系统，而这个数据在 Redis 中不存在，所有的请求都落到了数据库上把数据库打死。造成这种情况的原因有系统设计不合理、缓存数据更新不及时，或爬虫等恶意攻击。 解决办法：如果从数据库查询的结果为空，依然把这个结果进行缓存，那么当用 key 获取数据时，即使数据不存在，Redis 也可以直接返回结果，避免多次访问数据库 什么是缓存雪崩？怎么解决？ 缓存雪崩是指当大量缓存失效时，大量的请求访问直接请求数据库，导致数据库服务器无法抗住请求或挂掉的情况。这时网站常常会出现 502 错误，导致网站不可用问题。 解决办法： 合理规划缓存的失效时间，可以给缓存时间加一个随机数，防止统一时间过期。 对数据库进行过载保护或应用层限流，这种情况下一般是在网站处于大流量、高并发时，服务器整体不能承受时，可以采用的一种限流保护措施 最后还可以考虑多级缓存设计，实现缓存的高可用。 什么是redis的哨兵模式? Redis 的哨兵作用是管理多个 Redis 服务器，提供了监控、提醒以及自动的故障转移的功能。哨兵可以保证当主服务器挂了后，可以从从服务器选择一台当主服务器，把别的从服务器转移到读新的主机。 Redis持久化机制有哪些？ Redis 主要支持的持久化机制为 RDB（快照）和 AOF（追加文件）。 RDB 持久化是在指定时间间隔内保存数据快照到硬盘中。但 RDB 的持久化方式没有办法实现实时性的持久化。当应用使用 RDB 持久化后，如果 Redis 系统发生崩溃，那么使用 RDB 恢复数据时，恢复后的数据中，存在丢失最近一次生成快照之后更改的所有数据。所以 RDB 持久化不适用于丢失一部分数据也会对应用造成很大影响的备份中。 AOF 持久化是把命令追加到操作日志的尾部，然后保存所有历史操作。AOF 主要是解决数据持久化的实时性。Redis 服务器默认开启 RDB，关闭 AOF；要开启 AOF，需要在配置文件中配置：appendonly yes。AOF 持久化相对于 RDB 持久化的优点在于可以实时的对 Redis 缓存进行写入记录，保证快速恢复缓存时的完整性。 Redis集群架构模式有哪几种? Redis 集群架构是支持单节点单机模式的，也支持一主多从的主从结构，还支持带有哨兵的集群部署模式 © vishon all right reserved，powered by GitbookUpdated at 2024-01-17 19:35:42 "},"interview/go.html":{"url":"interview/go.html","title":"【面试】面试之go篇","keywords":"","body":"面试题之go篇 Go 有异常类型吗？ 有。Go用error类型代替try...catch语句，这样可以节省资源。同时增加代码可读性： _, err := funcDemo() if err != nil { fmt.Println(err) return } 也可以用errors.New()来定义自己的异常。errors.Error()会返回异常的字符串表示。只要实现error接口就可以定义自己的异常， type errorString struct { s string } func (e *errorString) Error() string { return e.s } // 多一个函数当作构造函数 func New(text string) error { return &errorString{text} } 什么是协程（Goroutine） 协程是用户态轻量级线程，它是线程调度的基本单位。通常在函数前加上go关键字就能实现并发。一个Goroutine会以一个很小的栈启动2KB或4KB，当遇到栈空间不足时，栈会自动伸缩， 因此可以轻易实现成千上万个goroutine同时启动。 如何高效地拼接字符串 拼接字符串的方式有：+ , fmt.Sprintf , strings.Builder, bytes.Buffer, strings.Join 1 \"+\" 使用+操作符进行拼接时，会对字符串进行遍历，计算并开辟一个新的空间来存储原来的两个字符串。 2 fmt.Sprintf 由于采用了接口参数，必须要用反射获取值，因此有性能损耗。 3 strings.Builder： 用WriteString()进行拼接，内部实现是指针+切片，同时String()返回拼接后的字符串，它是直接把[]byte转换为string，从而避免变量拷贝。 4 bytes.Buffer bytes.Buffer是一个一个缓冲byte类型的缓冲器，这个缓冲器里存放着都是byte， bytes.buffer底层也是一个[]byte切片。 5 strings.join strings.join也是基于strings.builder来实现的,并且可以自定义分隔符，在join方法内调用了b.Grow(n)方法，这个是进行初步的容量分配，而前面计算的n的长度就是我们要拼接的slice的长度，因为我们传入切片长度固定，所以提前进行容量分配可以减少内存分配，很高效。 性能比较： strings.Join ≈ strings.Builder > bytes.Buffer > \"+\" > fmt.Sprintf 什么是 rune 类型 ASCII 码只需要 7 bit 就可以完整地表示，但只能表示英文字母在内的128个字符，为了表示世界上大部分的文字系统，发明了 Unicode， 它是ASCII的超集，包含世界上书写系统中存在的所有字符，并为每个代码分配一个标准编号（称为Unicode CodePoint），在 Go 语言中称之为 rune，是 int32 类型的别名。 Go 语言中，字符串的底层表示是 byte (8 bit) 序列，而非 rune (32 bit) 序列。 sample := \"我爱GO\" runeSamp := []rune(sample) runeSamp[0] = '你' fmt.Println(string(runeSamp)) // \"你爱GO\" fmt.Println(len(runeSamp)) // 4 Go 支持默认参数或可选参数吗？ 不支持。但是可以利用结构体参数，或者...传入参数切片数组。 // 这个函数可以传入任意数量的整型参数 func sum(nums ...int) { total := 0 for _, num := range nums { total += num } fmt.Println(total) } 如何判断 map 中是否包含某个 key ？ var sample map[int]int if _, ok := sample[10]; ok { } else { } defer 的执行顺序 defer执行顺序和调用顺序相反，类似于栈后进先出(LIFO)。 defer在return之后执行，但在函数退出之前，defer可以修改返回值。下面是一个例子： func test() int { i := 0 defer func() { fmt.Println(\"defer1\") }() defer func() { i += 1 fmt.Println(\"defer2\") }() return i } func main() { fmt.Println(\"return\", test()) } // defer2 // defer1 // return 0 上面这个例子中，test返回值并没有修改，这是由于Go的返回机制决定的，执行Return语句后，Go会创建一个临时变量保存返回值。如果是有名返回（也就是指明返回值func test() (i int)） func test() (i int) { i = 0 defer func() { i += 1 fmt.Println(\"defer2\") }() return i } func main() { fmt.Println(\"return\", test()) } // defer2 // return 1 这个例子中，返回值被修改了。对于有名返回值的函数，执行 return 语句时，并不会再创建临时变量保存，因此，defer 语句修改了 i，即对返回值产生了影响。 Go 语言 tag 的用处？ tag可以为结构体成员提供属性。常见的： json序列化或反序列化时字段的名称 db: sqlx模块中对应的数据库字段名 form: gin框架中对应的前端的数据字段名 binding: 搭配 form 使用, 默认如果没查找到结构体中的某个字段则不报错值为空, binding为 required 代表没找到返回错误给前端 如何获取一个结构体的所有tag？ 利用反射： import reflect type Author struct { Name int `json:Name` Publications []string `json:Publication,omitempty` } func main() { t := reflect.TypeOf(Author{}) for i := 0; i 上述例子中，reflect.TypeOf方法获取对象的类型，之后NumField()获取结构体成员的数量。 通过Field(i)获取第i个成员的名字。 再通过其Tag 方法获得标签。 如何判断 2 个字符串切片（slice) 是相等的？ reflect.DeepEqual() ， 但反射非常影响性能。 结构体打印时，%v 和 %+v 的区别 %v输出结构体各成员的值； %+v输出结构体各成员的名称和值； %#v输出结构体名称和结构体各成员的名称和值 Go 语言中如何表示枚举值(enums)？ 在常量中用iota可以表示枚举。iota从0开始。 const ( B = 1 空 struct{} 的用途 用map模拟一个set，那么就要把值置为struct{}，struct{}本身不占任何空间，可以避免任何多余的内存分配。 type Set map[string]struct{} func main() { set := make(Set) for _, item := range []string{\"A\", \"A\", \"B\", \"C\"} { set[item] = struct{}{} } fmt.Println(len(set)) // 3 if _, ok := set[\"A\"]; ok { fmt.Println(\"A exists\") // A exists } } 有时候给通道发送一个空结构体,channel func main() { ch := make(chan struct{}, 1) go func() { 仅有方法的结构体 type Lamp struct{} go里面的int和int32是同一个概念吗？ 不是一个概念！千万不能混淆。go语言中的int的大小是和操作系统位数相关的，如果是32位操作系统，int类型的大小就是4字节。如果是64位操作系统，int类型的大小就是8个字节。除此之外uint也与操作系统有关。 int8占1个字节，int16占2个字节，int32占4个字节，int64占8个字节。 init() 函数是什么时候执行的？ 简答： 在main函数之前执行。 详细：init()函数是go初始化的一部分，由runtime初始化每个导入的包，初始化不是按照从上到下的导入顺序，而是按照解析的依赖关系，没有依赖的包最先初始化。 每个包首先初始化包作用域的常量和变量（常量优先于变量），然后执行包的init()函数。同一个包，甚至是同一个源文件可以有多个init()函数。init()函数没有入参和返回值，不能被其他函数调用，同一个包内多个init()函数的执行顺序不作保证。 执行顺序：import –> const –> var –>init()–>main() 一个文件可以有多个init()函数！ 如何知道一个对象是分配在栈上还是堆上？ Go和C++不同，Go局部变量会进行逃逸分析。如果变量离开作用域后没有被引用，则优先分配到栈上，否则分配到堆上。那么如何判断是否发生了逃逸呢？ go build -gcflags '-m -m -l' xxx.go. 关于逃逸的可能情况：变量大小不确定，变量类型不确定，变量分配的内存超过用户栈最大值，暴露给了外部指针。 2 个 interface 可以比较吗 ？ Go 语言中，interface 的内部实现包含了 2 个字段，类型 T 和 值 V，interface 可以使用 == 或 != 比较。2 个 interface 相等有以下 2 种情况 两个 interface 均等于 nil（此时 V 和 T 都处于 unset 状态） 类型 T 相同，且对应的值 V 相等。 看下面的例子： type Stu struct { Name string } type StuInt interface{} func main() { var stu1, stu2 StuInt = &Stu{\"Tom\"}, &Stu{\"Tom\"} var stu3, stu4 StuInt = Stu{\"Tom\"}, Stu{\"Tom\"} fmt.Println(stu1 == stu2) // false fmt.Println(stu3 == stu4) // true } stu1 和 stu2 对应的类型是 *Stu，值是 Stu 结构体的地址，两个地址不同，因此结果为 false。 stu3 和 stu4 对应的类型是 Stu，值是 Stu 结构体，且各字段相等，因此结果为 true。 2 个 nil 可能不相等吗？ 可能不等。interface在运行时绑定值，只有值为nil接口值才为nil，但是与指针的nil不相等。举个例子： var p *int = nil var i interface{} = nil if(p == i){ fmt.Println(\"Equal\") } 两者并不相同。总结：两个nil只有在类型相同时才相等。 简述 Go 语言GC(垃圾回收)的工作原理 垃圾回收机制是Go一大特(nan)色(dian)。Go1.3采用标记清除法， Go1.5采用三色标记法，Go1.8采用三色标记法+混合写屏障。 标记清除法\\ 分为两个阶段：标记和清除 标记阶段：从根对象出发寻找并标记所有存活的对象。 清除阶段：遍历堆中的对象，回收未标记的对象，并加入空闲链表。 缺点是需要暂停程序STW。 三色标记法\\： 将对象标记为白色，灰色或黑色。 白色：不确定对象（默认色）；黑色：存活对象。灰色：存活对象，子对象待处理。 标记开始时，先将所有对象加入白色集合（需要STW）。首先将根对象标记为灰色，然后将一个对象从灰色集合取出，遍历其子对象，放入灰色集合。同时将取出的对象放入黑色集合，直到灰色集合为空。最后的白色集合对象就是需要清理的对象。 这种方法有一个缺陷，如果对象的引用被用户修改了，那么之前的标记就无效了。因此Go采用了写屏障技术，当对象新增或者更新会将其着色为灰色。 一次完整的GC分为四个阶段： 准备标记（需要STW），开启写屏障。 开始标记 标记结束（STW），关闭写屏障 清理（并发） 基于插入写屏障和删除写屏障在结束时需要STW来重新扫描栈，带来性能瓶颈。混合写屏障分为以下四步： GC开始时，将栈上的全部对象标记为黑色（不需要二次扫描，无需STW）； GC期间，任何栈上创建的新对象均为黑色 被删除引用的对象标记为灰色 被添加引用的对象标记为灰色 总而言之就是确保黑色对象不能引用白色对象，这个改进直接使得GC时间从 2s降低到2us。 函数返回局部变量的指针是否安全？ 这一点和C++不同，在Go里面返回局部变量的指针是安全的。因为Go会进行逃逸分析，如果发现局部变量的作用域超过该函数则会把指针分配到堆区，避免内存泄漏。 非接口的任意类型 T() 都能够调用 *T 的方法吗？反过来呢？ 一个T类型的值可以调用T类型声明的方法，当且仅当T是*可寻址的。 反之：*T 可以调用T()的方法，因为指针可以解引用。 无缓冲的 channel 和有缓冲的 channel 的区别？ 对于无缓冲区channel： 发送的数据如果没有被接收方接收，那么发送方阻塞；如果一直接收不到发送方的数据，接收方阻塞； 有缓冲的channel： 发送方在缓冲区满的时候阻塞，接收方不阻塞；接收方在缓冲区为空的时候阻塞，发送方不阻塞。 可以类比生产者与消费者问题。 为什么有协程泄露(Goroutine Leak)？ 协程泄漏是指协程创建之后没有得到释放。主要原因有： 缺少接收器，导致发送阻塞 缺少发送器，导致接收阻塞 死锁。多个协程由于竞争资源导致死锁。 创建协程的没有回收。 Go 可以限制运行时操作系统线程的数量吗？ 常见的goroutine操作函数有哪些？ 可以，使用runtime.GOMAXPROCS(num int)可以设置线程数目。该值默认为CPU逻辑核数，如果设的太大，会引起频繁的线程切换，降低性能。 runtime.Gosched()，用于让出CPU时间片，让出当前goroutine的执行权限，调度器安排其它等待的任务运行，并在下次某个时候从该位置恢复执行。 runtime.Goexit()，调用此函数会立即使当前的goroutine的运行终止（终止协程），而其它的goroutine并不会受此影响。runtime.Goexit在终止当前goroutine前会先执行此goroutine的还未执行的defer语句。请注意千万别在主函数调用runtime.Goexit，因为会引发panic。 new和make的区别？ new只用于分配内存，返回一个指向地址的指针。它为每个新类型分配一片内存，初始化为0且返回类型*T的内存地址，它相当于&T{} make只可用于slice,map,channel的初始化,返回的是引用。 请你讲一下Go面向对象是如何实现的？ Go实现面向对象的两个关键是struct和interface。 封装：对于同一个包，对象对包内的文件可见；对不同的包，需要将对象以大写开头才是可见的。 继承：继承是编译时特征，在struct内加入所需要继承的类即可： type A struct{} type B struct{ A } 多态：多态是运行时特征，Go多态通过interface来实现。类型和接口是松耦合的，某个类型的实例可以赋给它所实现的任意接口类型的变量。 Go支持多重继承，就是在类型中嵌入所有必要的父类型。 uint型变量值分别为 1，2，它们相减的结果是多少？ var a uint = 1 var b uint = 2 fmt.Println(a - b) 答案，结果会溢出，如果是32位系统，结果是2^32-1，如果是64位系统，结果2^64-1. 讲一下go有没有函数在main之前执行？怎么用？ go的init函数在main函数之前执行，它有如下特点： func init() { ... } init函数非常特殊： 初始化不能采用初始化表达式初始化的变量； 程序运行前执行注册 实现sync.Once功能 不能被其它函数调用 init函数没有入口参数和返回值： 每个包可以有多个init函数，每个源文件也可以有多个init函数。 同一个包的init执行顺序，golang没有明确定义，编程时要注意程序不要依赖这个执行顺序。 不同包的init函数按照包导入的依赖关系决定执行顺序。 golang的内存管理的原理清楚吗？简述go内存管理机制。 golang内存管理基本是参考tcmalloc来进行的。go内存管理本质上是一个内存池，只不过内部做了很多优化：自动伸缩内存池大小，合理的切割内存块。 一些基本概念： 页Page：一块8K大小的内存空间。Go向操作系统申请和释放内存都是以页为单位的。 span : 内存块，一个或多个连续的 page 组成一个 span 。如果把 page 比喻成工人， span 可看成是小队，工人被分成若干个队伍，不同的队伍干不同的活。 sizeclass : 空间规格，每个 span 都带有一个 sizeclass ，标记着该 span 中的 page 应该如何使用。使用上面的比喻，就是 sizeclass 标志着 span 是一个什么样的队伍。 object : 对象，用来存储一个变量数据内存空间，一个 span 在初始化时，会被切割成一堆等大的 object 。假设 object 的大小是 16B ， span 大小是 8K ，那么就会把 span 中的 page 就会被初始化 8K / 16B = 512 个 object 。所谓内存分配，就是分配一个 object 出去。 mheap 一开始go从操作系统索取一大块内存作为内存池，并放在一个叫mheap的内存池进行管理，mheap将一整块内存切割为不同的区域，并将一部分内存切割为合适的大小。 mheap.spans ：用来存储 page 和 span 信息，比如一个 span 的起始地址是多少，有几个 page，已使用了多大等等。 mheap.bitmap 存储着各个 span 中对象的标记信息，比如对象是否可回收等等。 mheap.arena_start : 将要分配给应用程序使用的空间。 mcentral 用途相同的span会以链表的形式组织在一起存放在mcentral中。这里用途用sizeclass来表示，就是该span存储哪种大小的对象。 找到合适的 span 后，会从中取一个 object 返回给上层使用。 mcache 为了提高内存并发申请效率，加入缓存层mcache。每一个mcache和处理器P对应。Go申请内存首先从P的mcache中分配，如果没有可用的span再从mcentral中获取。 mutex有几种模式？ mutex有两种模式：normal 和 starvation 正常模式 所有goroutine按照FIFO的顺序进行锁获取，被唤醒的goroutine和新请求锁的goroutine同时进行锁获取，通常新请求锁的goroutine更容易获取锁(持续占有cpu)，被唤醒的goroutine则不容易获取到锁。公平性：否。 饥饿模式 所有尝试获取锁的goroutine进行等待排队，新请求锁的goroutine不会进行锁获取(禁用自旋)，而是加入队列尾部等待获取锁。公平性：是。 go竞态条件了解吗？ 所谓竞态竞争，就是当两个或以上的goroutine访问相同资源时候，对资源进行读/写。 比如var a int = 0，有两个协程分别对a+=1，我们发现最后a不一定为2.这就是竞态竞争。 通常我们可以用go run -race xx.go来进行检测。 解决方法是，对临界区资源上锁，或者使用原子操作(atomics)，原子操作的开销小于上锁。 如果若干个goroutine，有一个panic会怎么做？ 有一个panic，那么剩余goroutine也会退出，程序退出。如果不想程序退出，那么必须通过调用 recover() 方法来捕获 panic 并恢复将要崩掉的程序。 defer可以捕获goroutine的子goroutine吗？ 不可以。它们处于不同的调度器P中。对于子goroutine，必须通过 recover() 机制来进行恢复，然后结合日志进行打印（或者通过channel传递error），下面是一个例子： // 心跳函数 func Ping(ctx context.Context) error { ... code ... go func() { defer func() { if r := recover(); r != nil { log.Errorc(ctx, \"ping panic: %v, stack: %v\", r, string(debug.Stack())) } }() ... code ... }() ... code ... return nil } gRPC是什么？ 基于go的远程过程调用。RPC 框架的目标就是让远程服务调用更加简单、透明，RPC 框架负责屏蔽底层的传输方式（TCP 或者 UDP）、序列化方式（XML/Json/ 二进制）和通信细节。服务调用者可以像调用本地接口一样调用远程的服务提供者，而不需要关心底层通信细节和调用过程。 微服务了解吗？ 微服务是一种开发软件的架构和组织方法，其中软件由通过明确定义的 API 进行通信的小型独立服务组成。微服务架构使应用程序更易于扩展和更快地开发，从而加速创新并缩短新功能的上市时间。 服务发现是怎么做的？ 主要有两种服务发现机制：客户端发现和服务端发现。 客户端发现模式：当我们使用客户端发现的时候，客户端负责决定可用服务实例的网络地址并且在集群中对请求负载均衡, 客户端访问服务登记表，也就是一个可用服务的数据库，然后客户端使用一种负载均衡算法选择一个可用的服务实例然后发起请求。 服务端发现模式：客户端通过负载均衡器向某个服务提出请求，负载均衡器查询服务注册表，并将请求转发到可用的服务实例。如同客户端发现，服务实例在服务注册表中注册或注销。 ETCD用过吗？ etcd是一个高度一致的分布式键值存储，它提供了一种可靠的方式来存储需要由分布式系统或机器集群访问的数据。它可以优雅地处理网络分区期间的领导者选举，即使在领导者节点中也可以容忍机器故障。 etcd 是用Go语言编写的，它具有出色的跨平台支持，小的二进制文件和强大的社区。etcd机器之间的通信通过Raft共识算法处理。 你项目有优雅的启停吗？ 所谓「优雅」启停就是在启动退出服务时要满足以下几个条件： 不可以关闭现有连接（进程） 新的进程启动并「接管」旧进程 连接要随时响应用户请求，不可以出现拒绝请求的情况 停止的时候，必须处理完既有连接，并且停止接收新的连接。 为此我们必须引用信号来完成这些目的： 启动： 监听SIGHUP（在用户终端连接(正常或非正常)结束时发出）； 收到信号后将服务监听的文件描述符传递给新的子进程，此时新老进程同时接收请求； 退出： 监听SIGINT和SIGSTP和SIGQUIT等。 父进程停止接收新请求，等待旧请求完成（或超时）； 父进程退出。 实现：go1.8采用Http.Server内置的Shutdown方法支持优雅关机。 然后fvbock/endless可以实现优雅重启。 持久化怎么做的？ 所谓持久化就是将要保存的字符串写到硬盘等设备。 最简单的方式就是采用ioutil的WriteFile()方法将字符串写到磁盘上，这种方法面临格式化方面的问题。 更好的做法是将数据按照固定协议进行组织再进行读写，比如JSON，XML，Gob，csv等。 如果要考虑高并发和高可用，必须把数据放入到数据库中，比如MySQL，PostgreDB，MongoDB等。 对已经关闭的chan进行读写会怎么样？ 读已经关闭的chan能一直读到东西，但是读到的内容根据通道内关闭前是否有元素而不同。 如果chan关闭前，buffer内有元素还未读,会正确读到chan内的值，且返回的第二个bool值（是否读成功）为true。 如果chan关闭前，buffer内有元素已经被读完，chan内无值，接下来所有接收的值都会非阻塞直接成功，返回 channel 元素的零值，但是第二个bool值一直为false。 写已经关闭的chan会panic。 select的实现原理？ select源码位于src\\runtime\\select.go，最重要的scase 数据结构为： type scase struct { c *hchan // chan elem unsafe.Pointer // data element } scase.c为当前case语句所操作的channel指针，这也说明了一个case语句只能操作一个channel。 scase.elem表示缓冲区地址： caseRecv ： scase.elem表示读出channel的数据存放地址； caseSend ： scase.elem表示将要写入channel的数据存放地址； select的主要实现位于：select.go函数：其主要功能如下： 锁定scase语句中所有的channel 按照随机顺序检测scase中的channel是否ready 2.1 如果case可读，则读取channel中数据，解锁所有的channel，然后返回(case index, true) 2.2 如果case可写，则将数据写入channel，解锁所有的channel，然后返回(case index, false) 2.3 所有case都未ready，则解锁所有的channel，然后返回（default index, false） 所有case都未ready，且没有default语句 3.1 将当前协程加入到所有channel的等待队列 3.2 当将协程转入阻塞，等待被唤醒 唤醒后返回channel对应的case index 4.1 如果是读操作，解锁所有的channel，然后返回(case index, true) 4.2 如果是写操作，解锁所有的channel，然后返回(case index, false) 说说context包的作用？你用过哪些，原理知道吗？ context可以用来在goroutine之间传递上下文信息，相同的context可以传递给运行在不同goroutine中的函数，上下文对于多个goroutine同时使用是安全的，context包定义了上下文类型，可以使用background、TODO创建一个上下文，在函数调用链之间传播context，也可以使用WithDeadline、WithTimeout、WithCancel 或 WithValue 创建的修改副本替换它，听起来有点绕，其实总结起就是一句话：context的作用就是在不同的goroutine之间同步请求特定的数据、取消信号以及处理请求的截止日期。 © vishon all right reserved，powered by GitbookUpdated at 2024-01-17 20:14:36 "}}